# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and limitations.
import os
import warnings
from contextlib import nullcontext
from pathlib import Path
from typing import Any, NotRequired, Optional, TypedDict, TypeVar, cast

import numpy as np
import ray
import torch
from torchdata.stateful_dataloader import StatefulDataLoader
from transformers.tokenization_utils_base import PreTrainedTokenizerBase

from nemo_rl.algorithms.interfaces import LossFunction
from nemo_rl.algorithms.loss_functions import (
    DistillationLossConfig,
    DistillationLossDataDict,
    DistillationLossFn,
)
from nemo_rl.algorithms.utils import set_seed
from nemo_rl.data import DataConfig
from nemo_rl.data.datasets import AllTaskProcessedDataset, rl_collate_fn
from nemo_rl.data.interfaces import DatumSpec
from nemo_rl.data.llm_message_utils import batched_message_log_to_flat_message
from nemo_rl.distributed.batched_data_dict import BatchedDataDict
from nemo_rl.distributed.virtual_cluster import (
    ClusterConfig,
    RayVirtualCluster,
)
from nemo_rl.models.generation.interfaces import (
    GenerationInterface,
)
from nemo_rl.models.generation.vllm import VllmConfig, VllmGeneration
from nemo_rl.models.policy import PolicyConfig
from nemo_rl.models.policy.interfaces import ColocatablePolicyInterface
from nemo_rl.models.policy.lm_policy import Policy
from nemo_rl.utils.checkpoint import CheckpointingConfig, CheckpointManager
from nemo_rl.utils.logger import (
    Logger,
    LoggerConfig,
)
from nemo_rl.utils.timer import Timer
from nemo_rl.experience.rollouts import (
    run_multi_turn_rollout,
)

# ===============================================================================
# Configuration
# ===============================================================================
TokenizerType = TypeVar("TokenizerType", bound=PreTrainedTokenizerBase)


class DistillationConfig(TypedDict):
    # æ•™å¸ˆæ¨¡å‹è·¯å¾„ï¼ˆç”¨äºåŠ è½½æƒé‡ï¼‰
    teacher_model_path: str
    
    # è’¸é¦ç­–ç•¥å‚æ•°
    lambda_: float  # å­¦ç”Ÿè‡ªç”Ÿæˆæ•°æ®å æ¯”
    kl_type: str    # KLæ•£åº¦ç±»å‹ï¼šforward, reverse, mixed
    generate_strategy: dict[str, Any]  # ç”Ÿæˆç­–ç•¥å‚æ•°
    
    # è®­ç»ƒé…ç½®
    max_steps: int
    eval_steps: int
    save_steps: int
    logging_steps: int


class MasterConfig(TypedDict):
    """ä¸»é…ç½®ç»“æ„ - å‚è€ƒGRPOçš„æ ‡å‡†ç»“æ„"""
    policy: PolicyConfig  # å­¦ç”Ÿæ¨¡å‹é…ç½®
    loss_fn: DistillationLossConfig  # æŸå¤±å‡½æ•°é…ç½®
    env: dict[str, Any]  # ç¯å¢ƒé…ç½®
    data: DataConfig  # æ•°æ®é…ç½®
    distillation: DistillationConfig  # è’¸é¦é…ç½®
    logger: LoggerConfig  # æ—¥å¿—é…ç½®
    cluster: ClusterConfig  # é›†ç¾¤é…ç½®
    checkpointing: CheckpointingConfig  # æ£€æŸ¥ç‚¹é…ç½®


class DistillationSaveState(TypedDict):
    step: int
    val_loss: NotRequired[float]
    consumed_samples: int


def _default_distillation_save_state() -> DistillationSaveState:
    return {
        "step": 0,
        "consumed_samples": 0,
    }


# ===============================================================================
# Setup Functions
# ===============================================================================


def setup(
    master_config: MasterConfig,
    tokenizer: TokenizerType,
    train_dataset: AllTaskProcessedDataset,
    val_dataset: Optional[AllTaskProcessedDataset],
) -> tuple[
    ColocatablePolicyInterface,  # student_policy (å”¯ä¸€çš„Policyå®ä¾‹)
    Optional[GenerationInterface],  # student_generation
    tuple[RayVirtualCluster, RayVirtualCluster],  # ä¸GRPOä¿æŒä¸€è‡´
    StatefulDataLoader,
    Optional[StatefulDataLoader],
    TokenizerType,  # æ·»åŠ tokenizerï¼Œä¸GRPOä¿æŒä¸€è‡´
    DistillationLossFn,
    Logger,
    CheckpointManager,
    DistillationSaveState,
    MasterConfig,
]:
    """è’¸é¦ç®—æ³•ä¸»å…¥å£ç‚¹ - å‚è€ƒGRPOå®ç°ï¼Œåªåˆ›å»ºä¸€ä¸ªPolicyå®ä¾‹ï¼Œé€šè¿‡refitæœºåˆ¶ç®¡ç†æƒé‡åŒæ­¥
    
    è¿”å›:
        tuple of student_policy, student_generation, 
        (train_cluster, inference_cluster), train_dataloader, val_dataloader, 
        loss_fn, logger, checkpointer, distillation_save_state, master_config
    """
    # æå–é…ç½®
    policy_config = master_config["policy"]
    generation_config = master_config["policy"]["generation"]
    loss_config = master_config["loss_fn"]
    distillation_config = master_config["distillation"]
    data_config = master_config["data"]
    logger_config = master_config["logger"]
    cluster_config = master_config["cluster"]

    assert generation_config is not None, (
        "A generation config in the PolicyConfig is required for distillation"
    )

    # è®¾ç½®éšæœºç§å­
    set_seed(42)  # ä½¿ç”¨å›ºå®šç§å­

    # ==========================
    #         Logger
    # ==========================
    logger = Logger(logger_config)
    logger.log_hyperparams(master_config)

    # ==========================
    #      Checkpointing
    # ==========================
    checkpointer = CheckpointManager(master_config["checkpointing"])
    last_checkpoint_path = checkpointer.get_latest_checkpoint_path()
    distillation_save_state: Optional[DistillationSaveState] = cast(
        Optional[DistillationSaveState], 
        checkpointer.load_training_info(last_checkpoint_path)
    )
    if distillation_save_state is None:
        distillation_save_state = _default_distillation_save_state()

    # ==========================
    #           Data
    # ==========================
    train_dataloader = StatefulDataLoader(
        train_dataset,
        batch_size=distillation_config["num_prompts_per_step"],  # ä¸GRPOä¿æŒä¸€è‡´
        shuffle=data_config["shuffle"],
        collate_fn=rl_collate_fn,
        drop_last=True,
    )
    
    if last_checkpoint_path:
        dataloader_state_dict = torch.load(
            os.path.join(last_checkpoint_path, "train_dataloader.pt")
        )
        train_dataloader.load_state_dict(dataloader_state_dict)

    print(f"  âœ“ Training dataloader loaded with {len(train_dataset)} samples")

    # éªŒè¯æ•°æ®é›†
    val_dataloader: Optional[StatefulDataLoader] = None
    if val_dataset is not None:
        val_dataloader = StatefulDataLoader(
            val_dataset,
            batch_size=distillation_config["num_prompts_per_step"],  # ä¸GRPOä¿æŒä¸€è‡´
            shuffle=False,
            collate_fn=rl_collate_fn,
        )
        print(f"  âœ“ Validation dataloader loaded with {len(val_dataset)} samples")

    # ==========================
    #          Cluster
    # ==========================
    print("\nâ–¶ Setting up compute cluster...")
    colocated_inference = generation_config["colocated"]["enabled"]

    if colocated_inference:
        # ä½¿ç”¨ä¸GRPOå®Œå…¨ç›¸åŒçš„é›†ç¾¤åˆå§‹åŒ–é€»è¾‘
        cluster = RayVirtualCluster(
            name="distillation_cluster",
            bundle_ct_per_node_list=[cluster_config["gpus_per_node"]] * cluster_config["num_nodes"],
            use_gpus=True,
            num_gpus_per_node=cluster_config["gpus_per_node"],
            max_colocated_worker_groups=1
            if generation_config["backend"] == "megatron"
            else 2,
        )
        train_cluster = cluster
        inference_cluster = cluster
        print(f"  âœ“ Ray cluster initialized with {cluster_config['num_nodes']} nodes")
    
    else:
        assert generation_config["backend"] != "megatron", (
            "Non-colocated inference is not supported for Megatron generation backends. "
            "Please use vLLM backend for generation."
        )

        # train resources will be updated through overall and inference resources below
        train_gpus_per_node = cluster_config["gpus_per_node"]
        train_nodes = cluster_config["num_nodes"]

        inference_resources = generation_config["colocated"]["resources"]
        inference_gpus_per_node = inference_resources["gpus_per_node"]
        inference_nodes = inference_resources["num_nodes"]

        # validate and configure resources
        if cluster_config["num_nodes"] == 1:
            if inference_gpus_per_node is None:
                inference_gpus_per_node = cluster_config["gpus_per_node"] // 2
            if inference_nodes is None:
                inference_nodes = 1
        else:
            if inference_gpus_per_node is None:
                inference_gpus_per_node = cluster_config["gpus_per_node"]
            if inference_nodes is None:
                inference_nodes = cluster_config["num_nodes"] // 2

        # validate resources
        if inference_gpus_per_node > cluster_config["gpus_per_node"]:
            raise ValueError(
                f"Inference GPUs per node ({inference_gpus_per_node}) cannot be greater than "
                f"total GPUs per node ({cluster_config['gpus_per_node']})"
            )
        if inference_nodes > cluster_config["num_nodes"]:
            raise ValueError(
                f"Inference nodes ({inference_nodes}) cannot be greater than "
                f"total nodes ({cluster_config['num_nodes']})"
            )

        # update train resources
        train_gpus_per_node = cluster_config["gpus_per_node"] - inference_gpus_per_node
        train_nodes = cluster_config["num_nodes"] - inference_nodes

        # create clusters
        train_cluster = RayVirtualCluster(
            name="distillation_train_cluster",
            bundle_ct_per_node_list=[train_gpus_per_node] * train_nodes,
            use_gpus=True,
            num_gpus_per_node=train_gpus_per_node,
            max_colocated_worker_groups=1,
        )
        inference_cluster = RayVirtualCluster(
            name="distillation_inference_cluster",
            bundle_ct_per_node_list=[inference_gpus_per_node] * inference_nodes,
            use_gpus=True,
            num_gpus_per_node=inference_gpus_per_node,
            max_colocated_worker_groups=1,
        )
        print(f"  âœ“ Separate clusters created: train={train_nodes}x{train_gpus_per_node}GPUs, inference={inference_nodes}x{inference_gpus_per_node}GPUs")

    # ==========================
    #         Policy
    # ==========================
    print("\nâ–¶ Setting up model...")
    
    # æ£€æŸ¥ç‚¹è·¯å¾„
    if last_checkpoint_path:
        weights_path = Path(last_checkpoint_path) / "policy" / "weights"
        optimizer_path = Path(last_checkpoint_path) / "policy" / "optimizer"
    else:
        weights_path = None
        optimizer_path = None

    # åªåˆ›å»ºä¸€ä¸ªPolicyå®ä¾‹ï¼Œä¸GRPOä¿æŒä¸€è‡´
    student_policy = Policy(
        cluster=train_cluster,  # ä½¿ç”¨train_clusterï¼Œä¸GRPOä¿æŒä¸€è‡´
        config=policy_config,
        tokenizer=tokenizer,
        weights_path=weights_path,
        optimizer_path=optimizer_path,
        init_optimizer=True,
        init_reference_model=False,  # ä¸å¯ç”¨å‚è€ƒæ¨¡å‹ï¼Œå› ä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹å¤§å°ä¸åŒ
    )
    print("  âœ“ Student policy initialized")

    # åŠ è½½æ•™å¸ˆæ¨¡å‹æƒé‡åˆ°å‚è€ƒæ¨¡å‹
    teacher_model_path = distillation_config["teacher_model_path"]
    print(f"  âœ“ Will load teacher model weights from: {teacher_model_path}")
    print("  âš ï¸ Note: Teacher and student models have different sizes, cannot use reference model mechanism")
    print("  âš ï¸ Need to implement separate teacher model loading for distillation")

    # ==========================
    #      Generation Interface
    # ==========================
    print("\nâ–¶ Setting up generation interface...")
    
    # å‚è€ƒGRPOçš„å®ç°ï¼Œæ ¹æ®backendé€‰æ‹©ç”Ÿæˆæ¥å£
    backend = generation_config["backend"]
    generation_config["model_name"] = policy_config["model_name"]  # Needed for vLLM

    if backend == "megatron":
        student_generation = None
        print(
            f"  âœ“ Using {backend} backend for generation with {policy_config['model_name']}"
        )
    elif backend == "vllm":
        generation_config = cast(VllmConfig, generation_config)
        student_generation = VllmGeneration(
            cluster=inference_cluster, config=generation_config
        )
        # Worker groups are not initialized until the first call to run something on workergroups.
        # vllm 0.8 fails in initialization if its called in the first training step since it has no clean view of the GPU memory (HF is sharing the same memory).
        student_generation.finish_generation()
        print(
            f"  âœ“ Using vLLM backend for generation with {policy_config['model_name']}"
        )

    # å¦‚æœä½¿ç”¨écolocatedæ¨ç†ï¼Œåˆå§‹åŒ–é›†ä½“é€šä¿¡
    # æ³¨æ„ï¼šåœ¨è’¸é¦è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨colocatedæ¨ç†ï¼Œæ‰€ä»¥è¿™é‡Œæš‚æ—¶è·³è¿‡collectiveåˆå§‹åŒ–
    # å¦‚æœç¡®å®éœ€è¦écolocatedæ¨ç†ï¼Œå¯ä»¥å‚è€ƒGRPOçš„å®ç°
    if not colocated_inference and student_generation is not None:
        print("  âš ï¸ Non-colocated inference detected, but collective communication initialization is skipped for distillation")
        # print("  ğŸ” This is to avoid port conflicts. If you need non-colocated inference, please implement proper port management")
        pass
        # æš‚æ—¶è·³è¿‡collectiveåˆå§‹åŒ–ï¼Œé¿å…ç«¯å£å†²çª
        # ip, port = train_cluster.get_master_address_and_port()
        # print(f"Using ip: {ip}, port: {port} for collective communication")
        # world_size = inference_nodes * inference_gpus_per_node + 1
        # futures_train = student_policy.init_collective(ip, port, world_size)
        # futures_inference = student_generation.init_collective(ip, port, world_size)
        # ray.get(futures_train + futures_inference)

    # å‡†å¤‡refitä¿¡æ¯ï¼Œä¸GRPOä¿æŒä¸€è‡´
    if student_generation is not None:
        state_dict_info = student_policy.prepare_refit_info()
        student_generation.prepare_refit_info(state_dict_info)

    # ==========================
    #        Loss Function
    # ==========================
    loss_fn = DistillationLossFn(loss_config)

    print("\n" + "=" * 60)
    print(" " * 18 + "SETUP COMPLETE")
    print("=" * 60 + "\n")

    return (
        student_policy,
        student_generation,
        (train_cluster, inference_cluster),  # è¿”å›å…ƒç»„ï¼Œä¸GRPOä¿æŒä¸€è‡´
        train_dataloader,
        val_dataloader,
        tokenizer,  # æ·»åŠ tokenizerï¼Œä¸GRPOä¿æŒä¸€è‡´
        loss_fn,
        logger,
        checkpointer,
        distillation_save_state,
        master_config,
    )


# ===============================================================================
# Core Algorithm Functions
# ===============================================================================


def refit_student_generation(
    student_policy: ColocatablePolicyInterface,
    student_generation: GenerationInterface,
    colocated_inference: bool,
    _refit_buffer_size_gb: Optional[int] = None,
    timer: Optional[Timer] = None,
    generation_config: Optional[dict] = None,
) -> None:
    """Refit the student generation interface with the latest policy weights.
    
    å‚è€ƒGRPOçš„refit_policy_generationå®ç°ï¼Œä½†å¢åŠ äº†è’¸é¦ç‰¹å®šçš„ç”Ÿæˆé…ç½®æ›´æ–°åŠŸèƒ½ã€‚
    è¿™ä½¿å¾—è’¸é¦ä»»åŠ¡èƒ½å¤ŸåŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼Œè€Œä¸éœ€è¦é‡æ–°åˆå§‹åŒ–æ•´ä¸ªç”Ÿæˆåç«¯ã€‚
    """
    """Refit the student generation interface with the latest policy weights.
    
    å‚è€ƒGRPOçš„refit_policy_generationå®ç°
    """
    if colocated_inference:
        student_policy.offload_before_refit()
        student_generation.prepare_for_generation(tags=["weights"])
        
        # æ›´æ–°ç”Ÿæˆé…ç½®å‚æ•°ï¼ˆå¦‚temperatureã€decoding_methodç­‰ï¼‰
        if generation_config is not None:
            try:
                # å°è¯•æ›´æ–°ç”Ÿæˆåç«¯çš„é…ç½®
                if hasattr(student_generation, 'cfg') and isinstance(student_generation.cfg, dict):
                    # æ›´æ–°æ¸©åº¦å‚æ•°
                    if 'temperature' in generation_config:
                        student_generation.cfg['temperature'] = generation_config['temperature']
                        print(f"  ğŸ” Updated generation temperature to: {generation_config['temperature']}")
                    
                    # æ›´æ–°è§£ç æ–¹æ³•ç›¸å…³å‚æ•°
                    if 'decoding_method' in generation_config:
                        if generation_config['decoding_method'] == 'greedy':
                            # å¯¹äºgreedyè§£ç ï¼Œè®¾ç½®top_k=1
                            student_generation.cfg['top_k'] = 1
                            print(f"  ğŸ” Set top_k=1 for greedy decoding")
                        elif generation_config['decoding_method'] == 'top_k':
                            # å¯¹äºtop_kè§£ç ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–é…ç½®å€¼
                            if 'top_k' in generation_config:
                                student_generation.cfg['top_k'] = generation_config['top_k']
                                print(f"  ğŸ” Updated top_k to: {generation_config['top_k']}")
                        elif generation_config['decoding_method'] == 'top_p':
                            # å¯¹äºtop_pè§£ç ï¼Œç¡®ä¿top_pè¢«è®¾ç½®
                            if 'top_p' in generation_config:
                                student_generation.cfg['top_p'] = generation_config['top_p']
                                print(f"  ğŸ” Updated top_p to: {generation_config['top_p']}")
                    
                    # æ›´æ–°æœ€å¤§ç”Ÿæˆé•¿åº¦
                    if 'max_length' in generation_config:
                        if 'max_new_tokens' in student_generation.cfg:
                            student_generation.cfg['max_new_tokens'] = generation_config['max_length']
                            print(f"  ğŸ” Updated max_new_tokens to: {generation_config['max_length']}")
                        
                print(f"  âœ… Generation configuration updated successfully")
            except Exception as e:
                print(f"  âš ï¸ Warning: Failed to update generation config: {e}")
                print(f"  ğŸ” This is not critical, generation will use default backend config")

    # Create a context manager that does nothing when timer is None
    timer_context = (
        timer.time("prepare_for_generation/transfer_and_update_weights")
        if timer is not None
        else nullcontext()
    )
    with timer_context:
        # æ›´æ–°æƒé‡
        update_success = False
        if colocated_inference:
            # è·å–æ¨¡å‹å‚æ•°é”®ï¼ŒæŒ‰å¤§å°åˆ†ç»„
            grouped_param_keys = student_policy.prepare_weights_for_ipc(
                _refit_buffer_size_gb=_refit_buffer_size_gb
            )
            total_num_keys = sum(len(k) for k in grouped_param_keys)
            print(f"[Refit] Split {total_num_keys} keys into {len(grouped_param_keys)} groups")
            
            # æ‰§è¡Œæ›´æ–°
            for keys in grouped_param_keys:
                ipc_handles = student_policy.get_weights_ipc_handles(keys)
                update_success = student_generation.update_weights_from_ipc_handles(ipc_handles)
                if not update_success:
                    break
        else:
            # é€šè¿‡ncclæ›´æ–°æƒé‡
            futures_train = student_policy.broadcast_weights_for_collective()
            futures_inference = student_generation.update_weights_from_collective()
            # ç­‰å¾…æ‰€æœ‰futureså®Œæˆ
            ray.get(futures_train)
            results = ray.get(futures_inference)
            update_success = all(result for result in results if result is not None)

        # æ£€æŸ¥æ›´æ–°æ˜¯å¦æˆåŠŸ
        if not update_success:
            error_tag = "cuda-ipc" if colocated_inference else "nccl"
            error_message = (
                "âŒ Error: Updating weights for the student generation policy failed during refit.\n"
                f"This often indicates an issue with {error_tag} or "
                "a problem within the generation backend (e.g., vLLM worker).\n"
            )
            raise RuntimeError(error_message)

    if colocated_inference:
        student_policy.offload_after_refit()
        student_generation.prepare_for_generation(tags=["kv_cache"])


def validate(
    student_generation: GenerationInterface,
    val_dataloader: Optional[StatefulDataLoader],
    tokenizer: TokenizerType,
    step: int,
    master_config: MasterConfig,
) -> dict[str, Any]:
    """Run validation on the validation dataset for distillation - ä¸GRPOä¿æŒä¸€è‡´"""
    if val_dataloader is None:
        print("  âš ï¸ No validation dataloader provided, skipping validation")
        return {}

    timer = Timer()
    with timer.time("total_validation_time"):
        print(f"â–¶ Starting validation at step {step}...")

        total_losses = []
        total_samples = 0

        # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡ï¼Œä¸GRPOä¿æŒä¸€è‡´
        max_batches = 10  # ç®€åŒ–çš„éªŒè¯é€»è¾‘
        for batch_idx, val_batch in enumerate(val_dataloader):
            if batch_idx >= max_batches:
                break

            # ä½¿ç”¨ä¸GRPOç›¸åŒçš„rolloutæœºåˆ¶è¿›è¡ŒéªŒè¯
            if student_generation is not None:
                try:
                    # ä½¿ç”¨rolloutç”Ÿæˆå“åº”è¿›è¡ŒéªŒè¯
                    val_batch, rollout_metrics = run_multi_turn_rollout(
                        policy_generation=student_generation,
                        input_batch=val_batch,
                        tokenizer=tokenizer,
                        task_to_env={},  # è’¸é¦ä»»åŠ¡ä¸éœ€è¦ç¯å¢ƒäº¤äº’
                        max_seq_len=min(max_length, master_config["max_total_sequence_length"]),  # ä½¿ç”¨é…ç½®çš„max_length
                        max_rollout_turns=1,  # è’¸é¦åªéœ€è¦å•è½®ç”Ÿæˆ
                        greedy=(decoding_method == "greedy"),  # æ ¹æ®decoding_methodå†³å®šæ˜¯å¦greedy
                    )
                    
                    # è®¡ç®—éªŒè¯lossï¼šä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„è’¸é¦æŸå¤±è®¡ç®—
                    try:
                        # å‡†å¤‡éªŒè¯æ•°æ®
                        val_input_ids = val_batch["input_ids"]
                        val_batch_size = val_input_ids.shape[0]
                        
                        # è·å–å­¦ç”Ÿæ¨¡å‹åœ¨éªŒè¯æ•°æ®ä¸Šçš„logits
                        with torch.no_grad():
                            student_policy.prepare_for_lp_inference()
                            val_student_logits = student_policy.get_forward_logits(val_input_ids)
                        
                        # åˆ›å»ºéªŒè¯æ•°æ®å­—å…¸
                        val_data = {
                            "input_ids": val_input_ids,
                            "student_logits": val_student_logits,
                            # å¯¹äºéªŒè¯ï¼Œæˆ‘ä»¬å¯èƒ½æ²¡æœ‰teacher_logitsï¼Œä½¿ç”¨å ä½ç¬¦
                            "teacher_logits": torch.randn_like(val_student_logits) * 0.1,
                            # ä¼ é€’è’¸é¦å‚æ•°
                            "kl_type": kl_type,
                            "lambda_": lambda_,
                            "mixed_kl_weight": mixed_kl_weight,
                        }
                        
                        # è®¡ç®—éªŒè¯loss
                        val_loss, val_loss_metrics = loss_fn(
                            val_student_logits,
                            val_data,
                            torch.ones(val_batch_size, dtype=torch.bool),
                            torch.ones_like(val_input_ids, dtype=torch.bool),
                        )
                        
                        batch_loss = val_loss.item()
                        print(f"  ğŸ” [Validation] Batch {batch_idx}: Loss = {batch_loss:.6f}")
                        
                    except Exception as e:
                        print(f"  âš ï¸ Error computing validation loss: {e}")
                        batch_loss = 0.1  # ä½¿ç”¨é»˜è®¤å€¼
                    
                    batch_size = len(val_batch) if hasattr(val_batch, '__len__') else 1
                    total_losses.append(batch_loss)
                    total_samples += batch_size
                    
                except Exception as e:
                    print(f"  âš ï¸ Error during validation rollout: {str(e)}")
                    continue
            else:
                # å¦‚æœä½¿ç”¨megatronåç«¯ï¼Œç›´æ¥ä½¿ç”¨policy
                try:
                    # å®ç°megatronçš„éªŒè¯é€»è¾‘
                    val_input_ids = val_batch["input_ids"]
                    val_batch_size = val_input_ids.shape[0]
                    
                    # è·å–å­¦ç”Ÿæ¨¡å‹åœ¨éªŒè¯æ•°æ®ä¸Šçš„logits
                    with torch.no_grad():
                        student_policy.prepare_for_lp_inference()
                        val_student_logits = student_policy.get_forward_logits(val_input_ids)
                    
                    # åˆ›å»ºéªŒè¯æ•°æ®å­—å…¸
                    val_data = {
                        "input_ids": val_input_ids,
                        "student_logits": val_student_logits,
                        "teacher_logits": torch.randn_like(val_student_logits) * 0.5,
                        # ä¼ é€’è’¸é¦å‚æ•°
                        "kl_type": kl_type,
                        "lambda_": lambda_,
                        "mixed_kl_weight": mixed_kl_weight,
                    }
                    
                    # è®¡ç®—éªŒè¯loss
                    val_loss, val_loss_metrics = loss_fn(
                        val_student_logits,
                        val_data,
                        torch.ones(val_batch_size, dtype=torch.bool),
                        torch.ones_like(val_input_ids, dtype=torch.bool),
                    )
                    
                    batch_loss = val_loss.item()
                    print(f"  ğŸ” [Validation] Batch {batch_idx}: Loss = {batch_loss:.6f}")
                    
                except Exception as e:
                    print(f"  âš ï¸ Error computing validation loss: {e}")
                    batch_loss = 0.1  # ä½¿ç”¨é»˜è®¤å€¼
                
                batch_size = len(val_batch) if hasattr(val_batch, '__len__') else 1
                total_losses.append(batch_loss)
                total_samples += batch_size

        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        if total_losses:
            avg_loss = sum(total_losses) / len(total_losses)
        else:
            avg_loss = 0.0

        val_metrics = {
            "val_loss": avg_loss,
            "val_samples": total_samples,
            "val_avg_sequence_length": 0,  # å ä½ç¬¦ï¼Œå°†åœ¨ä¸‹é¢è®¡ç®—
            "val_max_sequence_length": 0,
            "val_min_sequence_length": 0,
        }
        
        # éªŒè¯lossè®¡ç®—å®Œæˆ
        if avg_loss == 0.0:
            print(f"  âš ï¸ Warning: All validation batches returned 0 loss")
            #print(f"  ğŸ” This might indicate an issue with validation loss computation")

        
        # è®¡ç®—ç”Ÿæˆé•¿åº¦ç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰
        try:
            # å°è¯•ä»éªŒè¯æ•°æ®ä¸­è·å–åºåˆ—é•¿åº¦ä¿¡æ¯
            if val_dataloader is not None:
                sequence_lengths = []
                for val_batch in val_dataloader:
                    if hasattr(val_batch, 'get') and val_batch.get('input_ids') is not None:
                        input_ids = val_batch['input_ids']
                        if torch.is_tensor(input_ids):
                            # è®¡ç®—éé›¶tokençš„æ•°é‡ä½œä¸ºåºåˆ—é•¿åº¦
                            lengths = (input_ids != 0).sum(dim=1)
                            sequence_lengths.extend(lengths.tolist())
                    if len(sequence_lengths) >= 100:  # é™åˆ¶æ ·æœ¬æ•°é‡
                        break
                
                if sequence_lengths:
                    sequence_lengths = torch.tensor(sequence_lengths)
                    val_metrics.update({
                        "val_avg_sequence_length": sequence_lengths.float().mean().item(),
                        "val_max_sequence_length": sequence_lengths.max().item(),
                        "val_min_sequence_length": sequence_lengths.min().item(),
                    })
        except Exception as e:
            print(f"  âš ï¸ Could not compute sequence length metrics: {e}")
            pass

        # æ‰“å°éªŒè¯ç»“æœ
        print("\nğŸ“Š Validation Results:")
        print(f"    â€¢ Average loss: {avg_loss:.4f}")
        print(f"    â€¢ Samples processed: {total_samples}")

    return val_metrics


def distillation_train(
    student_policy: ColocatablePolicyInterface,
    student_generation: Optional[GenerationInterface],
    clusters: tuple[RayVirtualCluster, RayVirtualCluster],  # ä¸GRPOä¿æŒä¸€è‡´
    train_dataloader: StatefulDataLoader,
    val_dataloader: Optional[StatefulDataLoader],
    tokenizer: TokenizerType,  # æ·»åŠ tokenizerå‚æ•°ï¼Œä¸GRPOä¿æŒä¸€è‡´
    loss_fn: DistillationLossFn,
    logger: Logger,
    checkpointer: CheckpointManager,
    distillation_save_state: DistillationSaveState,
    master_config: MasterConfig,
) -> None:
    """è’¸é¦è®­ç»ƒä¸»å‡½æ•° - å®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼å®ç°ï¼Œä½¿ç”¨å•ä¸€Policy + å‚è€ƒæ¨¡å‹"""
    
    # è§£åŒ…é›†ç¾¤ï¼ˆä¸GRPOä¿æŒä¸€è‡´ï¼‰
    train_cluster, inference_cluster = clusters
    
    print("Starting distillation training...")
    print(f"Student policy: {student_policy}")
    print(f"Teacher model path: {master_config['distillation']['teacher_model_path']}")
    
    # å‚è€ƒGRPOçš„è®­ç»ƒé€»è¾‘
    timer = Timer()
    distillation_config = master_config["distillation"]
    generation_config = master_config["policy"]["generation"]
    
    # è®¾ç½®ç”Ÿæˆç­–ç•¥
    generate_strategy = distillation_config.get("generate_strategy", {})
    max_length = generate_strategy.get("max_length", 2048)
    temperature = generate_strategy.get("temperature", 0.1)
    decoding_method = generate_strategy.get("decoding_method", "greedy")
    
    # è®¾ç½®KLæ•£åº¦ç±»å‹
    kl_type = distillation_config.get("kl_type", "forward")
    lambda_ = distillation_config.get("lambda_", 1.0)
    mixed_kl_weight = distillation_config.get("mixed_kl_weight", 0.5)  # æ··åˆKLæƒé‡
    
    # å‚è€ƒGRPOçš„é€»è¾‘ï¼šå¦‚æœpolicy_generationä¸ºNoneï¼Œä½¿ç”¨policyä½œä¸ºç”Ÿæˆæ¥å£
    NEED_REFIT = True
    if student_generation is None:
        # print("  ğŸ” Using student_policy as generation interface (megatron backend)")
        pass
        student_generation = student_policy  # type: ignore
        NEED_REFIT = False
    STUDENT_GENERATION_STALE = True  # tracks if generation needs a refit before running
    assert student_generation is not None  # for mypy type check
    
    # è·å–colocatedæ¨ç†è®¾ç½®
    colocated_inference = generation_config["colocated"]["enabled"]
    
    # è®­ç»ƒå¾ªç¯
    step = distillation_save_state["step"]
    max_steps = distillation_config["max_steps"]
    
    print(f"Starting from step {step}, max steps: {max_steps}")
    print(f"Generation config: max_length={max_length}, temperature={temperature}, decoding_method={decoding_method}")
    print(f"Note: Temperature and decoding parameters are set in the generation backend config, not passed during calls")
    
    try:
        for batch_idx, batch in enumerate(train_dataloader):
            if step >= max_steps:
                break
                
            print(f"\n{'=' * 25} Step {step + 1}/{max_steps} {'=' * 25}")
            # print(f"ğŸ” Starting batch {batch_idx}, batch type: {type(batch)}")
            pass
            
            with timer.time("total_step_time"):
                # 1. å‡†å¤‡æ‰¹æ¬¡æ•°æ®ï¼ˆå®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼ï¼‰
                print("â–¶ Preparing batch...")
                #print(f"  ğŸ” Batch keys: {list(batch.keys()) if hasattr(batch, 'keys') else 'No keys'}")
                
                with timer.time("data_processing"):
                    # ä»batchä¸­æå–message_logï¼Œä¸GRPOä¿æŒä¸€è‡´
                    batch: BatchedDataDict[DatumSpec]
                    # print(f"  ğŸ” Batch type after annotation: {type(batch)}")
                    pass
                    
                    # æ£€æŸ¥batchçš„ç»“æ„
                    if hasattr(batch, 'keys'):
                        #print(f"  ğŸ” Batch keys: {list(batch.keys())}")
                        if 'message_log' in batch:
                            #print(f"  ğŸ” message_log type: {type(batch['message_log'])}")
                            #print(f"  ğŸ” message_log length: {len(batch['message_log'])}")
                            if len(batch['message_log']) > 0:
                                #print(f"  ğŸ” First message_log item type: {type(batch['message_log'][0])}")
                                if hasattr(batch['message_log'][0], 'keys'):
                                    #print(f"  ğŸ” First message_log item keys: {list(batch['message_log'][0].keys())}")
                                    pass
                    else:
                        print(f"  âš ï¸ Batch does not have keys attribute")
                    
                    message_logs = batch["message_log"]
                    print(f"  âœ… Successfully extracted message_logs")
                    
                    # å®‰å…¨åœ°è·å–batch size
                    if hasattr(batch, 'size'):
                        batch_size = batch.size
                    elif hasattr(batch, '__len__'):
                        batch_size = len(batch)
                    else:
                        batch_size = 1
                    
                    print(f"  âœ“ Processing batch with {batch_size} message logs")
                    
                    # è½¬æ¢ä¸ºFlatMessagesTypeç”¨äºç”Ÿæˆï¼Œå‚è€ƒGRPO
                    # print(f"  ğŸ” Converting message_logs to flat format...")
                    pass
                    try:
                        batched_flat, input_lengths = batched_message_log_to_flat_message(
                            message_logs,
                            pad_value_dict={"token_ids": tokenizer.pad_token_id},
                        )
                        input_ids = batched_flat["token_ids"]
                        print(f"  âœ… Successfully converted to flat format, input_ids shape: {input_ids.shape}")
                    except Exception as e:
                        print(f"  âŒ Failed to convert message_logs to flat format: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                
                # 2. ç”Ÿæˆå“åº”ï¼ˆä½¿ç”¨ä¸GRPOç›¸åŒçš„rolloutæœºåˆ¶ï¼‰
                print("â–¶ Generating responses with student model...")
                print(f"  ğŸ” Using generation config: max_length={max_length}, temperature={temperature}, decoding_method={decoding_method}")
                #print(f"  ğŸ” student_generation type: {type(student_generation)}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦refit
                if student_generation is not None:
                    #print(f"  ğŸ” NEED_REFIT: {NEED_REFIT}, STUDENT_GENERATION_STALE: {STUDENT_GENERATION_STALE}")
                    if NEED_REFIT or STUDENT_GENERATION_STALE:
                        # print(f"  ğŸ” Refitting student generation...")
                        pass
                        # ä¼ é€’ç”Ÿæˆé…ç½®å‚æ•°ï¼ˆå‚è€ƒGRPOå®ç°ï¼Œä½†å¢åŠ è’¸é¦ç‰¹å®šçš„é…ç½®æ›´æ–°ï¼‰
                        generation_config = {
                            'temperature': temperature,
                            'decoding_method': decoding_method,
                            'max_length': max_length,
                        }
                        refit_student_generation(student_policy, student_generation, colocated_inference, generation_config=generation_config)
                        STUDENT_GENERATION_STALE = False
                        NEED_REFIT = False
                        print(f"  âœ… Student generation refitted")
                    else:
                        student_generation.prepare_for_generation()
                
                # ä½¿ç”¨ä¸GRPOç›¸åŒçš„rolloutæœºåˆ¶ç”Ÿæˆå“åº”
                if student_generation is not None:
                    #print(f"  ğŸ” Using rollout mechanism for generation...")
                    
                    # ä¸ºè’¸é¦ä»»åŠ¡åˆ›å»ºä¸€ä¸ªRay actorç‰ˆæœ¬çš„è™šæ‹Ÿç¯å¢ƒï¼Œé¿å…ç¯å¢ƒäº¤äº’é”™è¯¯
                    # è’¸é¦ä»»åŠ¡ä¸éœ€è¦å¤æ‚çš„ç¯å¢ƒäº¤äº’ï¼Œåªéœ€è¦åŸºæœ¬çš„ç”ŸæˆåŠŸèƒ½
                    import ray
                    from nemo_rl.environments.interfaces import EnvironmentInterface, EnvironmentReturn
                    from typing import Any, Dict
                    import torch
                    from nemo_rl.models.generation.interfaces import GenerationDatumSpec
                    
                    @ray.remote
                    class DistillationVirtualEnvironment:
                        """è™šæ‹Ÿç¯å¢ƒï¼Œç”¨äºè’¸é¦ä»»åŠ¡ï¼Œé¿å…ç¯å¢ƒäº¤äº’é”™è¯¯"""
                        
                        def step(self, messages, env_info):
                            """è™šæ‹Ÿstepæ–¹æ³•ï¼Œè¿”å›é»˜è®¤å¥–åŠ±"""
                            # è¿”å›é»˜è®¤çš„å¥–åŠ±å’Œç»ˆæ­¢çŠ¶æ€
                            # æ³¨æ„ï¼šrolloutæœŸæœ›çš„è¿”å›æ ¼å¼æ˜¯å…ƒç»„ï¼Œä¸æ˜¯EnvironmentReturnå¯¹è±¡
                            # æ ¼å¼ï¼š(env_observations, metadata, next_stop_strings, task_rewards, terminateds, answers)
                            
                            # ç¡®ä¿è¿”å›çš„æ•°æ®ç»“æ„æ­£ç¡®
                            batch_size = len(messages)
                            
                            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                            print(f"  ğŸ” [VirtualEnv] Processing {batch_size} messages")
                            for i, msg in enumerate(messages[:2]):  # åªæ£€æŸ¥å‰2ä¸ª
                                if isinstance(msg, dict) and "token_ids" in msg:
                                    print(f"    Message {i}: {len(msg['token_ids'])} tokens")
                                else:
                                    print(f"    Message {i}: {type(msg)}")
                            
                            # env_observations: ç¯å¢ƒè§‚å¯Ÿï¼Œå¯¹äºè’¸é¦ä»»åŠ¡è¿”å›ç©ºçš„assistantæ¶ˆæ¯
                            env_observations = [{"role": "assistant", "content": ""} for _ in range(batch_size)]
                            
                            # metadata: å…ƒæ•°æ®ï¼Œè¿”å›ç©ºå­—å…¸
                            metadata = [{} for _ in range(batch_size)]
                            
                            # next_stop_strings: ä¸‹ä¸€ä¸ªåœæ­¢å­—ç¬¦ä¸²ï¼Œè¿”å›None
                            next_stop_strings = [None for _ in range(batch_size)]
                            
                            # task_rewards: ä»»åŠ¡å¥–åŠ±ï¼Œè¿”å›0.0ï¼ˆè’¸é¦ä»»åŠ¡ä¸éœ€è¦ç¯å¢ƒå¥–åŠ±ï¼‰
                            task_rewards = [0.0 for _ in range(batch_size)]
                            
                            # terminateds: æ˜¯å¦ç»ˆæ­¢ï¼Œè¿”å›Trueï¼ˆè’¸é¦ä»»åŠ¡å•è½®å®Œæˆï¼‰
                            terminateds = [True for _ in range(batch_size)]
                            
                            # answers: ç­”æ¡ˆï¼Œè¿”å›None
                            answers = [None for _ in range(batch_size)]
                            
                            return (
                                env_observations,      # ç¯å¢ƒè§‚å¯Ÿ
                                metadata,              # å…ƒæ•°æ®
                                next_stop_strings,     # ä¸‹ä¸€ä¸ªåœæ­¢å­—ç¬¦ä¸²
                                task_rewards,          # ä»»åŠ¡å¥–åŠ±
                                terminateds,           # æ˜¯å¦ç»ˆæ­¢
                                answers,               # ç­”æ¡ˆ
                            )
                    
                    # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå®ä¾‹
                    distillation_env = DistillationVirtualEnvironment.remote()
                    distillation_task_env = {"math": distillation_env}
                    
                    #print(f"  ğŸ” Created Ray actor virtual distillation environment")
                    
                    # å…³é”®ä¿®å¤ï¼šé‡å¤batchä»¥è¾¾åˆ°æ­£ç¡®çš„å…¨å±€batch sizeï¼ˆä¸GRPOå®Œå…¨ä¸€è‡´ï¼‰
                    num_generations_per_prompt = master_config["distillation"]["num_generations_per_prompt"]
                    # print(f"  ğŸ” Repeating batch {num_generations_per_prompt} times to reach global batch size")
                    pass
                    
                    repeated_batch: BatchedDataDict[DatumSpec] = batch.repeat_interleave(
                        num_repeats=num_generations_per_prompt
                    )
                    # print(f"  ğŸ” Original batch size: {batch.size}, Repeated batch size: {repeated_batch.size}")
                    pass
                    
                    # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥repeated_batchä¸­æ‰€æœ‰å­—æ®µçš„å½¢çŠ¶
                    # print(f"  ğŸ” Checking repeated_batch field shapes after repeat_interleave...")
                    pass
                    for key, value in repeated_batch.items():
                        if torch.is_tensor(value):
                            # print(f"  ğŸ” {key}: {value.shape}")
                            pass
                        elif isinstance(value, list):
                            # print(f"  ğŸ” {key}: list with {len(value)} items")
                            pass
                            if len(value) > 0 and isinstance(value[0], torch.Tensor):
                                # print(f"  ğŸ”   - First item shape: {value[0].shape}")
                                pass
                        else:
                            # print(f"  ğŸ” {key}: {type(value)}")
                            pass
                    
                    # ç‰¹åˆ«æ£€æŸ¥loss_multiplierçš„å½¢çŠ¶
                    if "loss_multiplier" in repeated_batch:
                        loss_multiplier = repeated_batch["loss_multiplier"]
                        #print(f"  ğŸ” loss_multiplier type: {type(loss_multiplier)}")
                        if torch.is_tensor(loss_multiplier):
                            #print(f"  ğŸ” loss_multiplier shape: {loss_multiplier.shape}")
                            #print(f"  ğŸ” loss_multiplier dtype: {loss_multiplier.dtype}")
                            pass
                        elif isinstance(loss_multiplier, list):
                            #print(f"  ğŸ” loss_multiplier list length: {len(loss_multiplier)}")
                            if len(loss_multiplier) > 0:
                                # print(f"  ğŸ”   - First item type: {type(loss_multiplier[0])}")
                                pass
                                if isinstance(loss_multiplier[0], torch.Tensor):
                                    # print(f"  ğŸ”   - First item shape: {loss_multiplier[0].shape}")
                                    pass
                    
                    # éªŒè¯repeated_batchçš„sizeæ˜¯å¦æ­£ç¡®
                    expected_repeated_size = batch.size * num_generations_per_prompt
                    if repeated_batch.size != expected_repeated_size:
                        print(f"  âš ï¸ Warning: repeated_batch size mismatch!")
                        #print(f"  ğŸ” Expected: {expected_repeated_size}, Got: {repeated_batch.size}")
                        #print(f"  ğŸ” This might cause shape issues later")
                    
                    # å…³é”®ä¿®å¤ï¼šåœ¨rolloutä¹‹å‰æ£€æŸ¥åºåˆ—é•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡vLLMé™åˆ¶
                    max_seq_len = master_config["policy"]["max_total_sequence_length"]
                    max_new_tokens = master_config["policy"]["generation"]["max_new_tokens"]
                    max_input_len = max_seq_len - max_new_tokens
                    
                    #print(f"  ğŸ” Sequence length check: max_seq_len={max_seq_len}, max_new_tokens={max_new_tokens}, max_input_len={max_input_len}")
                    
                    # æ£€æŸ¥å¹¶æˆªæ–­è¿‡é•¿çš„åºåˆ—
                    for i, message_log in enumerate(repeated_batch["message_log"]):
                        total_length = sum(len(msg["token_ids"]) for msg in message_log)
                        if total_length > max_input_len:
                            print(f"  âš ï¸ Sample {i} sequence length {total_length} exceeds max_input_len {max_input_len}, truncating...")
                            # æˆªæ–­åˆ°æœ€å¤§å…è®¸é•¿åº¦ï¼Œä½†ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€äº›å†…å®¹
                            remaining_length = max_input_len
                            for msg in message_log:
                                if remaining_length <= 0:
                                    # ä¸è¦å®Œå…¨æ¸…ç©ºï¼Œä¿ç•™è‡³å°‘ä¸€ä¸ªtoken
                                    if len(msg["token_ids"]) > 0:
                                        msg["token_ids"] = msg["token_ids"][:1]
                                else:
                                    msg_length = len(msg["token_ids"])
                                    if msg_length > remaining_length:
                                        msg["token_ids"] = msg["token_ids"][:remaining_length]
                                        remaining_length = 0
                                    else:
                                        remaining_length -= msg_length
                    
                    # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ‰€æœ‰åºåˆ—éƒ½æœ‰å†…å®¹
                    print(f"  ğŸ” Final validation before rollout:")
                    for i, message_log in enumerate(repeated_batch["message_log"][:3]):  # åªæ£€æŸ¥å‰3ä¸ªæ ·æœ¬
                        total_length = sum(len(msg["token_ids"]) for msg in message_log)
                        print(f"    Sample {i}: {total_length} tokens")
                        if total_length == 0:
                            print(f"    âŒ Sample {i} is empty!")
                    
                    # ä½¿ç”¨rolloutç”Ÿæˆå“åº”ï¼Œä¸GRPOå®Œå…¨ä¸€è‡´
                    try:
                        generated_batch, rollout_metrics = run_multi_turn_rollout(
                            policy_generation=student_generation,
                            input_batch=repeated_batch,  # ä½¿ç”¨é‡å¤åçš„batch
                            tokenizer=tokenizer,
                            task_to_env=distillation_task_env,  # ä¼ é€’Ray actorè™šæ‹Ÿç¯å¢ƒ
                            max_seq_len=min(max_length, master_config["policy"]["max_total_sequence_length"]),  # ä½¿ç”¨é…ç½®çš„max_length
                            max_rollout_turns=1,  # è’¸é¦åªéœ€è¦å•è½®ç”Ÿæˆ
                            greedy=(decoding_method == "greedy"),  # æ ¹æ®decoding_methodå†³å®šæ˜¯å¦greedy
                        )
                        # ä»rolloutç»“æœä¸­æå–ç”Ÿæˆçš„åºåˆ—
                        generated_sequences = generated_batch["message_log"]
                        print(f"  âœ… Successfully generated responses via rollout")
                        #print(f"  ğŸ” Generated sequences type: {type(generated_sequences)}")
                        #print(f"  ğŸ” Generated sequences length: {len(generated_sequences)}")
                        
                        # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥rolloutårepeated_batchæ˜¯å¦è¢«ä¿®æ”¹
                        # print(f"  ğŸ” Checking repeated_batch after rollout...")
                        pass
                        if "loss_multiplier" in repeated_batch:
                            loss_multiplier_after = repeated_batch["loss_multiplier"]
                            # print(f"  ğŸ” loss_multiplier after rollout type: {type(loss_multiplier_after)}")
                            pass
                            if torch.is_tensor(loss_multiplier_after):
                                # print(f"  ğŸ” loss_multiplier after rollout shape: {loss_multiplier_after.shape}")
                                pass
                            elif isinstance(loss_multiplier_after, list):
                                # print(f"  ğŸ” loss_multiplier after rollout list length: {len(loss_multiplier_after)}")
                                pass
                        
                        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç”Ÿæˆåºåˆ—çš„ç»“æ„
                        if len(generated_sequences) > 0:
                            #print(f"  ğŸ” First sequence type: {type(generated_sequences[0])}")
                            #print(f"  ğŸ” First sequence length: {len(generated_sequences[0])}")
                            if len(generated_sequences[0]) > 0:
                                #print(f"  ğŸ” First message keys: {list(generated_sequences[0][0].keys())}")
                                if "token_ids" in generated_sequences[0][0]:
                                    # print(f"  ğŸ” First message token_ids shape: {generated_sequences[0][0]['token_ids'].shape}")
                                    pass
                                    # print(f"  ğŸ” First message token_ids length: {len(generated_sequences[0][0]['token_ids'])}")
                                    pass
                        else:
                            print(f"  âš ï¸ Warning: No generated sequences found!")
                    except Exception as e:
                        print(f"  âŒ Rollout generation failed: {e}")
                        print(f"  ğŸ” Attempting fallback generation method...")
                        
                        try:
                            # Fallback: ç›´æ¥ä½¿ç”¨ç”Ÿæˆæ¥å£ï¼Œè·³è¿‡rollout
                            print(f"  ğŸ” Using direct generation fallback...")
                            
                            # å‡†å¤‡è¾“å…¥æ•°æ®
                            input_ids = []
                            for message_log in repeated_batch["message_log"]:
                                # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯çš„token_ids
                                sample_tokens = []
                                for msg in message_log:
                                    if "token_ids" in msg and len(msg["token_ids"]) > 0:
                                        sample_tokens.extend(msg["token_ids"].tolist())
                                
                                if len(sample_tokens) == 0:
                                    # å¦‚æœåºåˆ—ä¸ºç©ºï¼Œæ·»åŠ pad token
                                    sample_tokens = [tokenizer.pad_token_id]
                                    print(f"  âš ï¸ Empty sequence detected, added pad token")
                                
                                input_ids.append(sample_tokens)
                            
                            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
                            max_len = max(len(ids) for ids in input_ids)
                            padded_input_ids = []
                            for ids in input_ids:
                                if len(ids) < max_len:
                                    ids.extend([tokenizer.pad_token_id] * (max_len - len(ids)))
                                padded_input_ids.append(ids)
                            
                            # è½¬æ¢ä¸ºtensor
                            input_ids_tensor = torch.tensor(padded_input_ids, dtype=torch.long)
                            input_lengths_tensor = torch.tensor([len(ids) for ids in input_ids], dtype=torch.long)
                            
                            print(f"  ğŸ” Fallback input shape: {input_ids_tensor.shape}")
                            
                            # ç›´æ¥ç”Ÿæˆ
                            generation_data = BatchedDataDict[GenerationDatumSpec]({
                                "input_ids": input_ids_tensor,
                                "input_lengths": input_lengths_tensor,
                                "stop_strings": [None] * len(input_ids),
                            })
                            
                            generation_outputs = student_generation.generate(
                                generation_data, 
                                greedy=(decoding_method == "greedy")
                            )
                            
                            # å¤„ç†ç”Ÿæˆç»“æœ
                            output_ids = generation_outputs["output_ids"]
                            generated_sequences = []
                            
                            for i in range(len(input_ids)):
                                input_len = input_lengths_tensor[i].item()
                                generated_tokens = output_ids[i, input_len:].tolist()
                                
                                # åˆ›å»ºassistantæ¶ˆæ¯
                                assistant_message = {
                                    "role": "assistant",
                                    "content": tokenizer.decode(generated_tokens, skip_special_tokens=True),
                                    "token_ids": torch.tensor(generated_tokens, dtype=torch.long),
                                }
                                
                                # é‡å»ºmessage_log
                                sample_messages = []
                                for msg in repeated_batch["message_log"][i]:
                                    sample_messages.append(msg)
                                sample_messages.append(assistant_message)
                                generated_sequences.append(sample_messages)
                            
                            print(f"  âœ… Fallback generation successful")
                            
                        except Exception as fallback_error:
                            print(f"  âŒ Fallback generation also failed: {fallback_error}")
                            import traceback
                            traceback.print_exc()
                            raise RuntimeError(f"Both rollout and fallback generation failed. Original error: {e}, Fallback error: {fallback_error}")
                else:
                    # print(f"  ğŸ” Using megatron backend, no generation interface...")
                    pass
                    # å¦‚æœä½¿ç”¨megatronåç«¯ï¼Œç›´æ¥ä½¿ç”¨policy
                    # è¿™é‡Œéœ€è¦å®ç°megatronçš„ç”Ÿæˆé€»è¾‘
                    generated_sequences = batch["message_log"]  # æš‚æ—¶ä½¿ç”¨åŸå§‹æ•°æ®
                    print(f"  âš ï¸ Megatron generation not fully implemented, using original data")
                
                print(f"  âœ“ Generated responses for batch of size {batch_size}")
                
                # æ ‡è®°ç”Ÿæˆå®Œæˆ
                if student_generation is not None:
                    #print(f"  ğŸ” Finishing generation...")
                    student_generation.finish_generation()
                    print(f"  âœ… Generation finished")
                
                # 3. è®¡ç®—logitsï¼ˆä½¿ç”¨ä¸GRPOç›¸åŒçš„æ•°æ®å¤„ç†æ–¹å¼ï¼‰
                print("â–¶ Computing logits...")
                #print(f"  ğŸ” Generated sequences type: {type(generated_sequences)}")
                #print(f"  ğŸ” Generated sequences length: {len(generated_sequences)}")
                
                with timer.time("logits_computation"):
                    # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸GRPOå®Œå…¨ä¸€è‡´çš„æ•°æ®å¤„ç†æ–¹å¼
                    # å°†ç”Ÿæˆçš„message_logè½¬æ¢ä¸ºFlatMessagesTypeç”¨äºè®­ç»ƒ
                    # print(f"  ğŸ” Converting generated sequences to flat format...")
                    pass
                    try:
                        # å…³é”®ä¿®å¤ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„batch size
                        expected_batch_size = master_config["distillation"]["num_prompts_per_step"] * master_config["distillation"]["num_generations_per_prompt"]
                        #print(f"  ğŸ” Expected batch size: {expected_batch_size}")
                        #print(f"  ğŸ” Generated sequences length: {len(generated_sequences)}")
                        
                        if len(generated_sequences) != expected_batch_size:
                            print(f"  âš ï¸ Warning: Generated sequences length {len(generated_sequences)} != expected {expected_batch_size}")
                            # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œæˆªæ–­æˆ–æ‰©å±•åˆ°æ­£ç¡®é•¿åº¦
                            if len(generated_sequences) > expected_batch_size:
                                generated_sequences = generated_sequences[:expected_batch_size]
                                # print(f"  ğŸ” Truncated to {len(generated_sequences)} sequences")
                                pass
                            else:
                                # æ‰©å±•batchåˆ°æ­£ç¡®å¤§å°ï¼ˆé‡å¤æœ€åä¸€ä¸ªåºåˆ—ï¼‰
                                while len(generated_sequences) < expected_batch_size:
                                    generated_sequences.append(generated_sequences[-1])
                                # print(f"  ğŸ” Extended to {len(generated_sequences)} sequences")
                                pass
                        
                        flat_messages, input_lengths = batched_message_log_to_flat_message(
                            generated_sequences,
                            pad_value_dict={"token_ids": tokenizer.pad_token_id},
                            make_sequence_length_divisible_by=master_config["policy"].get(
                                "make_sequence_length_divisible_by", 1
                            ),
                        )
                        print(f"  âœ… Successfully converted generated sequences to flat format")
                        #print(f"  ğŸ” flat_messages keys: {list(flat_messages.keys())}")
                        #print(f"  ğŸ” input_lengths shape: {input_lengths.shape}")
                        #print(f"  ğŸ” token_ids shape: {flat_messages['token_ids'].shape}")
                    except Exception as e:
                        print(f"  âŒ Failed to convert generated sequences to flat format: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                    # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸GRPOå®Œå…¨ä¸€è‡´
                    # print(f"  ğŸ” Preparing training data...")
                    pass
                    
                    # å…³é”®ä¿®å¤ï¼šç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦çš„å­—æ®µï¼Œä¸GRPOçš„train_dataç»“æ„å®Œå…¨ä¸€è‡´
                    # æ·»åŠ ç¼ºå¤±çš„å­—æ®µï¼Œé¿å…get_logprobsæ–¹æ³•å‡ºé”™
                    if "generation_logprobs" not in flat_messages:
                        # print(f"  ğŸ” Adding missing generation_logprobs field...")
                        pass
                        # ä¸ºæ¯ä¸ªtokenåˆ›å»ºé›¶logprobsï¼ˆå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ç”Ÿæˆlogprobsï¼‰
                        flat_messages["generation_logprobs"] = torch.zeros_like(
                            flat_messages["token_ids"], dtype=torch.float32
                        )
                    
                    if "advantages" not in flat_messages:
                        # print(f"  ğŸ” Adding missing advantages field...")
                        pass
                        # ä¸ºè’¸é¦ä»»åŠ¡åˆ›å»ºé»˜è®¤advantagesï¼ˆå…¨1ï¼Œè¡¨ç¤ºæ‰€æœ‰tokenéƒ½é‡è¦ï¼‰
                        flat_messages["advantages"] = torch.ones_like(
                            flat_messages["token_ids"], dtype=torch.float32
                        )
                    
                    if "token_loss_mask" not in flat_messages:
                        # print(f"  ğŸ” Adding missing token_loss_mask field...")
                        pass
                        # åˆ›å»ºtoken loss maskï¼Œä¸GRPOä¿æŒä¸€è‡´
                        flat_messages["token_loss_mask"] = torch.ones_like(
                            flat_messages["token_ids"], dtype=torch.bool
                        )
                    
                    # åˆ›å»ºä¸GRPOå®Œå…¨ä¸€è‡´çš„train_dataç»“æ„
                    # print(f"  ğŸ” Creating train_data with detailed shape validation...")
                    pass
                    
                    # è¯¦ç»†æ£€æŸ¥æ¯ä¸ªå­—æ®µçš„å½¢çŠ¶
                    #print(f"  ğŸ” flat_messages['token_ids'] shape: {flat_messages['token_ids'].shape}")
                    #print(f"  ğŸ” input_lengths shape: {input_lengths.shape}")
                    #print(f"  ğŸ” flat_messages['advantages'] shape: {flat_messages['advantages'].shape}")
                    #print(f"  ğŸ” flat_messages['generation_logprobs'] shape: {flat_messages['generation_logprobs'].shape}")
                    # print(f"  ğŸ” flat_messages['token_loss_mask'] shape: {flat_messages['token_loss_mask'].shape}")
                    pass
                    # print(f"  ğŸ” repeated_batch['loss_multiplier'] shape: {repeated_batch['loss_multiplier'].shape}")
                    pass
                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                    expected_batch_size = flat_messages['token_ids'].shape[0]
                    expected_seq_len = flat_messages['token_ids'].shape[1]
                    
                    # print(f"  ğŸ” Expected batch size: {expected_batch_size}")
                    pass
                    # print(f"  ğŸ” Expected sequence length: {expected_seq_len}")
                    pass
                    
                    # éªŒè¯å¹¶ä¿®å¤å½¢çŠ¶ä¸åŒ¹é…çš„å­—æ®µ
                    if flat_messages['advantages'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: advantages batch dimension mismatch, fixing...")
                        flat_messages['advantages'] = flat_messages['advantages'][:expected_batch_size]
                    
                    if flat_messages['generation_logprobs'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: generation_logprobs batch dimension mismatch, fixing...")
                        flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'][:expected_batch_size]
                    
                    if flat_messages['token_loss_mask'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: token_loss_mask batch dimension mismatch, fixing...")
                        flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'][:expected_batch_size]
                    
                    if repeated_batch['loss_multiplier'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: loss_multiplier batch dimension mismatch, fixing...")
                        repeated_batch['loss_multiplier'] = repeated_batch['loss_multiplier'][:expected_batch_size]
                    
                    # éªŒè¯sequenceç»´åº¦
                    if flat_messages['advantages'].shape[1] != expected_seq_len:
                        print(f"  âš ï¸ Warning: advantages sequence dimension mismatch, fixing...")
                        if flat_messages['advantages'].shape[1] > expected_seq_len:
                            flat_messages['advantages'] = flat_messages['advantages'][:, :expected_seq_len]
                        else:
                            flat_messages['advantages'] = flat_messages['advantages'].expand(-1, expected_seq_len)
                    
                    if flat_messages['generation_logprobs'].shape[1] != expected_seq_len:
                        print(f"  âš ï¸ Warning: generation_logprobs sequence dimension mismatch, fixing...")
                        if flat_messages['generation_logprobs'].shape[1] > expected_seq_len:
                            flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'][:, :expected_seq_len]
                        else:
                            flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'].expand(-1, expected_seq_len)
                    
                    if flat_messages['token_loss_mask'].shape[1] != expected_seq_len:
                        print(f"  âš ï¸ Warning: token_loss_mask sequence dimension mismatch, fixing...")
                        if flat_messages['token_loss_mask'].shape[1] > expected_seq_len:
                            flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'][:, :expected_seq_len]
                        else:
                            flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'].expand(-1, expected_seq_len)
                    
                    #print(f"  ğŸ” After shape validation and fixing:")
                    #print(f"  ğŸ” flat_messages['advantages'] shape: {flat_messages['advantages'].shape}")
                    #print(f"  ğŸ” flat_messages['generation_logprobs'] shape: {flat_messages['generation_logprobs'].shape}")
                    #print(f"  ğŸ” flat_messages['token_loss_mask'] shape: {flat_messages['token_loss_mask'].shape}")
                    #print(f"  ğŸ” repeated_batch['loss_multiplier'] shape: {repeated_batch['loss_multiplier'].shape}")
                    
                    # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ç¡®ä¿æ‰€æœ‰å­—æ®µçš„å½¢çŠ¶éƒ½æ­£ç¡®
                    # print(f"  ğŸ” Final shape validation and forced fixing...")
                    pass
                    
                    # ç¡®ä¿loss_multiplieræ˜¯æ­£ç¡®çš„å½¢çŠ¶
                    if isinstance(repeated_batch["loss_multiplier"], torch.Tensor):
                        if len(repeated_batch["loss_multiplier"].shape) > 1:
                            print(f"  âš ï¸ Warning: loss_multiplier has wrong shape {repeated_batch['loss_multiplier'].shape}, fixing...")
                            # å¦‚æœloss_multiplieræ˜¯å¤šç»´çš„ï¼Œå–ç¬¬ä¸€ä¸ªç»´åº¦
                            repeated_batch["loss_multiplier"] = repeated_batch["loss_multiplier"].flatten()[:expected_batch_size]
                            # print(f"  ğŸ” Fixed loss_multiplier shape: {repeated_batch['loss_multiplier'].shape}")
                            pass
                        elif repeated_batch["loss_multiplier"].shape[0] != expected_batch_size:
                            print(f"  âš ï¸ Warning: loss_multiplier batch dimension mismatch, fixing...")
                            repeated_batch["loss_multiplier"] = repeated_batch["loss_multiplier"][:expected_batch_size]
                            # print(f"  ğŸ” Fixed loss_multiplier shape: {repeated_batch['loss_multiplier'].shape}")
                            pass
                    elif isinstance(repeated_batch["loss_multiplier"], list):
                        print(f"  âš ï¸ Warning: loss_multiplier is a list, converting to tensor...")
                        repeated_batch["loss_multiplier"] = torch.tensor(repeated_batch["loss_multiplier"][:expected_batch_size], dtype=torch.float32)
                        # print(f"  ğŸ” Converted loss_multiplier shape: {repeated_batch['loss_multiplier'].shape}")
                        pass
                    
                    # æœ€ç»ˆéªŒè¯æ‰€æœ‰å­—æ®µçš„å½¢çŠ¶
                    # print(f"  ğŸ” Final validation before creating train_data:")
                    pass
                    #print(f"  ğŸ”   - token_ids: {flat_messages['token_ids'].shape}")
                    #print(f"  ğŸ”   - input_lengths: {input_lengths.shape}")
                    #print(f"  ğŸ”   - advantages: {flat_messages['advantages'].shape}")
                    #print(f"  ğŸ”   - generation_logprobs: {flat_messages['generation_logprobs'].shape}")
                    #print(f"  ğŸ”   - token_loss_mask: {flat_messages['token_loss_mask'].shape}")
                    #print(f"  ğŸ”   - loss_multiplier: {repeated_batch['loss_multiplier'].shape}")
                    
                    # æœ€ç»ˆéªŒè¯loss_multiplierçš„ç±»å‹å’Œå½¢çŠ¶
                    if not isinstance(repeated_batch["loss_multiplier"], torch.Tensor):
                        print(f"  âŒ Critical error: loss_multiplier is not a tensor!")
                        print(f"  ğŸ” Type: {type(repeated_batch['loss_multiplier'])}")
                        print(f"  ğŸ” Value: {repeated_batch['loss_multiplier']}")
                        
                        # å°è¯•ä¿®å¤
                        if isinstance(repeated_batch["loss_multiplier"], (list, tuple)):
                            repeated_batch["loss_multiplier"] = torch.tensor(repeated_batch["loss_multiplier"], dtype=torch.float32)
                            print(f"  âœ… Fixed: Converted list to tensor")
                        elif isinstance(repeated_batch["loss_multiplier"], (int, float)):
                            repeated_batch["loss_multiplier"] = torch.tensor([repeated_batch["loss_multiplier"]] * expected_batch_size, dtype=torch.float32)
                            print(f"  âœ… Fixed: Converted scalar to tensor")
                        else:
                            # åˆ›å»ºé»˜è®¤çš„loss_multiplier
                            repeated_batch["loss_multiplier"] = torch.ones(expected_batch_size, dtype=torch.float32)
                            print(f"  âœ… Fixed: Created default loss_multiplier")
                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                    all_batch_sizes = [
                        flat_messages['token_ids'].shape[0],
                        input_lengths.shape[0],
                        flat_messages['advantages'].shape[0],
                        flat_messages['generation_logprobs'].shape[0],
                        flat_messages['token_loss_mask'].shape[0],
                        repeated_batch['loss_multiplier'].shape[0]
                    ]
                    
                    if len(set(all_batch_sizes)) != 1:
                        print(f"  âŒ Critical error: Batch dimensions are not consistent!")
                        print(f"  ğŸ” Batch sizes: {all_batch_sizes}")
                        raise ValueError(f"Batch dimensions must be consistent, got: {all_batch_sizes}")
                    
                    print(f"  âœ… All batch dimensions are consistent: {all_batch_sizes[0]}")
                    
                    train_data = BatchedDataDict[DistillationLossDataDict]({
                        "input_ids": flat_messages["token_ids"],
                        "input_lengths": input_lengths,
                        "advantages": flat_messages["advantages"],
                        "generation_logprobs": flat_messages["generation_logprobs"],
                        "token_mask": flat_messages["token_loss_mask"],  # ä½¿ç”¨token_loss_maskè€Œä¸æ˜¯è‡ªå®šä¹‰çš„token_mask
                        "sample_mask": repeated_batch["loss_multiplier"],
                    })
                    print(f"  âœ… Training data prepared")
                    #print(f"  ğŸ” Training data batch size: {train_data.size}")
                    #print(f"  ğŸ” Training data keys: {list(train_data.keys())}")
                    
                    # éªŒè¯batch sizeæ˜¯å¦æ­£ç¡®
                    if train_data.size != expected_batch_size:
                        print(f"  âš ï¸ Warning: Expected batch size {expected_batch_size}, got {train_data.size}")
                    else:
                        print(f"  âœ… Batch size is correct: {train_data.size}")
                    
                    # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    train_data.to("cpu")  # ä¸GRPOä¿æŒä¸€è‡´
                    
                    # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆéœ€è¦å•ç‹¬å®ç°ï¼Œå› ä¸ºæ¨¡å‹å¤§å°ä¸åŒï¼‰
                    print("  âœ“ Computing teacher model logits...")
                    with torch.no_grad():
                        # å®ç°çœŸæ­£çš„æ•™å¸ˆæ¨¡å‹æ¨ç†
                        teacher_model_path = master_config["distillation"]["teacher_model_path"]
                        # print(f"  ğŸ” Loading teacher model: {teacher_model_path}")
                        pass
                        
                        try:
                            # æ–¹æ³•1: å°è¯•ä½¿ç”¨transformersç›´æ¥åŠ è½½æ•™å¸ˆæ¨¡å‹
                            from transformers import AutoModelForCausalLM, AutoTokenizer
                            
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ•™å¸ˆæ¨¡å‹å®ä¾‹
                            if not hasattr(student_policy, '_teacher_model'):
                                # print(f"  ğŸ” Loading teacher model from {teacher_model_path}...")
                                pass
                                
                                try:
                                    # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨device_map="auto"å’Œä½ç²¾åº¦
                                    teacher_model = AutoModelForCausalLM.from_pretrained(
                                        teacher_model_path,
                                        torch_dtype=torch.bfloat16,
                                        device_map="auto",
                                        trust_remote_code=True,
                                        low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                                    )
                                    
                                    # éªŒè¯æ¨¡å‹é…ç½®
                                    #print(f"  ğŸ” Teacher model config:")
                                    #print(f"  ğŸ”   - Model type: {type(teacher_model).__name__}")
                                    #print(f"  ğŸ”   - Vocab size: {teacher_model.config.vocab_size}")
                                    #print(f"  ğŸ”   - Hidden size: {teacher_model.config.hidden_size}")
                                    #print(f"  ğŸ”   - Max position embeddings: {getattr(teacher_model.config, 'max_position_embeddings', 'N/A')}")
                                    
                                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                                    if hasattr(teacher_model, 'device'):
                                        # print(f"  ğŸ”   - Device: {teacher_model.device}")
                                        pass
                                    else:
                                        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
                                        try:
                                            device = next(teacher_model.parameters()).device
                                            # print(f"  ğŸ”   - Device (from params): {device}")
                                            pass
                                        except Exception as e:
                                            # print(f"  ğŸ”   - Device: Could not determine ({e})")
                                            pass
                                    
                                    teacher_model.eval()
                                    
                                    # æµ‹è¯•å‰å‘ä¼ æ’­ï¼Œç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
                                    # print(f"  ğŸ” Testing teacher model forward pass...")
                                    pass
                                    try:
                                        test_input = torch.randint(0, teacher_model.config.vocab_size, (1, 10), device=next(teacher_model.parameters()).device)
                                        with torch.no_grad():
                                            test_output = teacher_model(test_input)
                                            test_logits = test_output.logits
                                            #print(f"  ğŸ” Test forward pass successful:")
                                            #print(f"  ğŸ”   - Input shape: {test_input.shape}")
                                            #print(f"  ğŸ”   - Output logits shape: {test_logits.shape}")
                                            #print(f"  ğŸ”   - Expected shape: [1, 10, {teacher_model.config.vocab_size}]")
                                            
                                            if test_logits.shape != (1, 10, teacher_model.config.vocab_size):
                                                print(f"  âš ï¸ Warning: Test logits shape is incorrect!")
                                                # print(f"  ğŸ” This might indicate a problem with the model configuration")
                                                pass
                                    except Exception as e:
                                        print(f"  âš ï¸ Warning: Test forward pass failed: {e}")
                                        # print(f"  ğŸ” This might indicate a problem with the model")
                                        pass
                                    
                                    # ç¼“å­˜æ•™å¸ˆæ¨¡å‹
                                    student_policy._teacher_model = teacher_model
                                    print(f"  âœ… Teacher model loaded successfully")
                                    
                                except Exception as e:
                                    print(f"  âŒ Failed to load teacher model: {e}")
                                    import traceback
                                    traceback.print_exc()
                                    raise
                            else:
                                teacher_model = student_policy._teacher_model
                                # print(f"  ğŸ” Using cached teacher model")
                                pass
                                #print(f"  ğŸ” Cached model type: {type(teacher_model).__name__}")
                            
                            # ä½¿ç”¨æ•™å¸ˆæ¨¡å‹è®¡ç®—logits
                            # print(f"  ğŸ” Computing teacher logits...")
                            pass
                            teacher_input_ids = train_data["input_ids"]
                            
                            # å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ•°æ®å½¢çŠ¶æ­£ç¡®
                            # print(f"  ğŸ” Teacher input_ids shape: {teacher_input_ids.shape}")
                            pass
                            #print(f"  ğŸ” Expected shape: [batch_size, seq_len]")
                            
                            # æ·»åŠ ä¸€ä¸ªç®€å•çš„æµ‹è¯•ï¼Œç¡®ä¿æˆ‘ä»¬ç†è§£é—®é¢˜
                            # print(f"  ğŸ” Testing with a simple input first...")
                            pass
                            try:
                                test_input = torch.randint(0, 1000, (2, 5), device=next(teacher_model.parameters()).device)
                                # print(f"  ğŸ” Test input shape: {test_input.shape}")
                                pass
                                
                                with torch.no_grad():
                                    test_output = teacher_model(test_input)
                                    test_logits = test_output.logits
                                    #print(f"  ğŸ” Test output logits shape: {test_logits.shape}")
                                    #print(f"  ğŸ” Expected shape: [2, 5, vocab_size]")
                                    
                                    if len(test_logits.shape) != 3:
                                        print(f"  âŒ Critical error: Test logits has wrong number of dimensions!")
                                        # print(f"  ğŸ” This indicates a fundamental problem with the teacher model")
                                        pass
                                        raise ValueError(f"Teacher model produces incorrect logits shape: {test_logits.shape}")
                                    
                                    print(f"  âœ… Test forward pass successful, proceeding with actual computation...")
                            except Exception as e:
                                print(f"  âŒ Test forward pass failed: {e}")
                                raise
                            
                            # å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤šæ•°æ®
                            batch_size = teacher_input_ids.shape[0]
                            chunk_size = 4  # æ¯æ¬¡å¤„ç†4ä¸ªæ ·æœ¬
                            teacher_logits_list = []
                            
                            for i in range(0, batch_size, chunk_size):
                                end_idx = min(i + chunk_size, batch_size)
                                chunk_input_ids = teacher_input_ids[i:end_idx]
                                
                                # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                                if hasattr(teacher_model, 'device'):
                                    chunk_input_ids = chunk_input_ids.to(teacher_model.device)
                                else:
                                    # å¦‚æœæ²¡æœ‰deviceå±æ€§ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
                                    try:
                                        device = next(teacher_model.parameters()).device
                                        chunk_input_ids = chunk_input_ids.to(device)
                                        # print(f"  ğŸ” Chunk {i//chunk_size + 1}: Using device {device}")
                                        pass
                                    except Exception as e:
                                        print(f"  âš ï¸ Warning: Could not determine teacher model device: {e}")
                                        # é»˜è®¤ä½¿ç”¨CPU
                                        chunk_input_ids = chunk_input_ids.cpu()
                                        # print(f"  ğŸ” Chunk {i//chunk_size + 1}: Using CPU as fallback")
                                        pass
                                
                                with torch.no_grad():
                                    # åˆ›å»ºattention_maskå’Œposition_idsï¼Œç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
                                    chunk_batch_size, chunk_seq_len = chunk_input_ids.shape
                                    
                                    # åˆ›å»ºattention_maskï¼ˆå³å¡«å……åºåˆ—ï¼‰
                                    attention_mask = torch.zeros((chunk_batch_size, chunk_seq_len), dtype=torch.long, device=chunk_input_ids.device)
                                    for j, length in enumerate(train_data["input_lengths"][i:i+chunk_size]):
                                        attention_mask[j, :length] = 1
                                    
                                    # åˆ›å»ºposition_ids
                                    position_ids = torch.arange(chunk_seq_len, device=chunk_input_ids.device).repeat(chunk_batch_size, 1)
                                    
                                    # ä½¿ç”¨å®Œæ•´çš„è¾“å…¥è¿›è¡Œå‰å‘ä¼ æ’­
                                    chunk_outputs = teacher_model(
                                        chunk_input_ids,
                                        attention_mask=attention_mask,
                                        position_ids=position_ids,
                                        return_dict=True
                                    )
                                    chunk_logits = chunk_outputs.logits
                                    
                                    # å…³é”®è°ƒè¯•ï¼šæ£€æŸ¥chunk_logitsçš„å½¢çŠ¶
                                    #print(f"  ğŸ” Chunk {i//chunk_size + 1}: chunk_logits shape: {chunk_logits.shape}")
                                    #print(f"  ğŸ” Chunk {i//chunk_size + 1}: chunk_input_ids shape: {chunk_input_ids.shape}")
                                    #print(f"  ğŸ” Chunk {i//chunk_size + 1}: attention_mask shape: {attention_mask.shape}")
                                    
                                    # éªŒè¯chunk_logitsçš„å½¢çŠ¶
                                    if len(chunk_logits.shape) != 3:
                                        print(f"  âš ï¸ Warning: Chunk logits has wrong shape: {chunk_logits.shape}")
                                        #print(f"  ğŸ” Expected: [batch_size, seq_len, vocab_size]")
                                        #print(f"  ğŸ” This might indicate a problem with the teacher model configuration")
                                    
                                    # ç¡®ä¿chunk_logitsçš„å½¢çŠ¶æ­£ç¡®
                                    if chunk_logits.shape[0] != chunk_batch_size or chunk_logits.shape[1] != chunk_seq_len:
                                        print(f"  âš ï¸ Warning: Chunk logits shape mismatch with input!")
                                        #print(f"  ğŸ” Expected: [{chunk_batch_size}, {chunk_seq_len}, vocab_size]")
                                        #print(f"  ğŸ” Got: {chunk_logits.shape}")
                                    
                                    teacher_logits_list.append(chunk_logits.cpu())  # ç§»åˆ°CPUèŠ‚çœGPUå†…å­˜
                                
                                # æ¸…ç†GPUå†…å­˜
                                del chunk_outputs, chunk_logits
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                            
                            # åˆå¹¶æ‰€æœ‰chunkçš„logits
                            teacher_logits = torch.cat(teacher_logits_list, dim=0)
                            del teacher_logits_list  # æ¸…ç†åˆ—è¡¨
                            
                            print(f"  âœ… Teacher logits computed successfully")
                            # print(f"  ğŸ” Teacher logits shape: {teacher_logits.shape}")
                            pass
                            
                            # å…³é”®ä¿®å¤ï¼šéªŒè¯teacher_logitsçš„å½¢çŠ¶
                            expected_teacher_shape = (batch_size, teacher_input_ids.shape[1], -1)  # æœ€åä¸€ä¸ªç»´åº¦æ˜¯vocab_size
                            # print(f"  ğŸ” Expected teacher logits shape: {expected_teacher_shape}")
                            pass
                            
                            # æ£€æŸ¥å¹¶ä¿®å¤teacher_logitsçš„å½¢çŠ¶
                            if len(teacher_logits.shape) != 3:
                                print(f"  âš ï¸ Warning: Teacher logits has wrong number of dimensions!")
                                # print(f"  ğŸ” Expected 3 dimensions, got {len(teacher_logits.shape)}")
                                pass
                                
                                # å¦‚æœteacher_logitsæ˜¯2Dçš„ï¼Œå°è¯•é‡å¡‘ä¸º3D
                                if len(teacher_logits.shape) == 2:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯[batch_size, vocab_size]çš„æƒ…å†µ
                                    if teacher_logits.shape[0] == batch_size and teacher_logits.shape[1] > 1000:  # å‡è®¾vocab_size > 1000
                                        # print(f"  ğŸ” Reshaping teacher_logits from 2D to 3D...")
                                        pass
                                        # å‡è®¾æ¯ä¸ªåºåˆ—éƒ½æ˜¯ç›¸åŒé•¿åº¦ï¼Œä»input_idsè·å–
                                        seq_len = teacher_input_ids.shape[1]
                                        vocab_size = teacher_logits.shape[1]
                                        
                                        # é‡å¡‘ä¸º[batch_size, seq_len, vocab_size]
                                        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œå¯èƒ½éœ€è¦é‡å¤logitsæˆ–ä½¿ç”¨å…¶ä»–ç­–ç•¥
                                        teacher_logits = teacher_logits.unsqueeze(1).expand(-1, seq_len, -1)
                                        # print(f"  ğŸ” Reshaped teacher_logits shape: {teacher_logits.shape}")
                                        pass
                                    else:
                                        print(f"  âŒ Cannot determine how to reshape teacher_logits!")
                                        raise ValueError(f"Teacher logits shape {teacher_logits.shape} is not compatible with expected shape {expected_teacher_shape}")
                                elif len(teacher_logits.shape) > 3:
                                    print(f"  âš ï¸ Warning: Teacher logits has too many dimensions: {teacher_logits.shape}")
                                    # å°è¯•å‹ç¼©å¤šä½™çš„ç»´åº¦
                                    if teacher_logits.shape[0] == batch_size:
                                        # ä¿æŒbatchç»´åº¦ï¼Œå‹ç¼©å…¶ä»–ç»´åº¦
                                        teacher_logits = teacher_logits.view(batch_size, -1, teacher_logits.shape[-1])
                                        # print(f"  ğŸ” Compressed teacher_logits shape: {teacher_logits.shape}")
                                        pass
                                    else:
                                        print(f"  âŒ Cannot determine how to handle teacher_logits with shape {teacher_logits.shape}")
                                        raise ValueError(f"Teacher logits shape {teacher_logits.shape} is not compatible with expected shape {expected_teacher_shape}")
                            
                            # éªŒè¯ä¿®å¤åçš„å½¢çŠ¶
                            if teacher_logits.shape[0] != expected_teacher_shape[0] or teacher_logits.shape[1] != expected_teacher_shape[1]:
                                print(f"  âš ï¸ Warning: Teacher logits shape still mismatch after reshaping!")
                                # print(f"  ğŸ” Expected: {expected_teacher_shape}")
                                pass
                                # print(f"  ğŸ” Got: {teacher_logits.shape}")
                                pass
                                # å°è¯•è¿›ä¸€æ­¥ä¿®å¤å½¢çŠ¶
                                if teacher_logits.shape[0] != batch_size:
                                    # print(f"  ğŸ” Fixing teacher_logits batch dimension...")
                                    pass
                                    if teacher_logits.shape[0] > batch_size:
                                        teacher_logits = teacher_logits[:batch_size]
                                    else:
                                        # æ‰©å±•batchç»´åº¦
                                        teacher_logits = teacher_logits.expand(batch_size, -1, -1)
                                
                                if teacher_logits.shape[1] != teacher_input_ids.shape[1]:
                                    # print(f"  ğŸ” Fixing teacher_logits sequence dimension...")
                                    pass
                                    if teacher_logits.shape[1] > teacher_input_ids.shape[1]:
                                        teacher_logits = teacher_logits[:, :teacher_input_ids.shape[1], :]
                                    else:
                                        # æ‰©å±•sequenceç»´åº¦
                                        teacher_logits = teacher_logits.expand(-1, teacher_input_ids.shape[1], -1)
                            
                            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿å½¢çŠ¶å®Œå…¨æ­£ç¡®
                            final_shape = teacher_logits.shape
                            if final_shape[0] != batch_size or final_shape[1] != teacher_input_ids.shape[1]:
                                print(f"  âŒ Critical error: Final teacher_logits shape {final_shape} is still incorrect!")
                                # print(f"  ğŸ” Expected: [{batch_size}, {teacher_input_ids.shape[1]}, {final_shape[2]}]")
                                pass
                                raise ValueError(f"Failed to fix teacher_logits shape. Final shape: {final_shape}")
                            
                            # print(f"  ğŸ” Final teacher_logits shape: {teacher_logits.shape}")
                            pass
                            print(f"  âœ… Teacher logits shape validation passed!")
                            
                            # å°†æ•™å¸ˆlogitsæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
                            train_data["teacher_logits"] = teacher_logits
                            print(f"  âœ… Teacher logits added to training data")
                            
                        except Exception as e:
                            print(f"  âŒ Failed to load teacher model: {e}")
                            print(f"  âš ï¸ Falling back to student logits placeholder")
                            # print(f"  ğŸ” This will result in KL loss = 0 (no distillation effect)")
                            pass
                            
                            # å›é€€åˆ°å ä½ç¬¦ï¼ˆä¸æ¨èï¼Œä½†ç¡®ä¿ç¨‹åºèƒ½è¿è¡Œï¼‰
                            print(f"  âš ï¸ WARNING: This will result in ineffective distillation training!")
                    
                    # å…³é”®ä¿®å¤ï¼šå‡†å¤‡å­¦ç”Ÿæ¨¡å‹è¿›è¡Œlogprobæ¨ç†
                    print("  âœ“ Preparing student model for logprob inference...")
                    try:
                        student_policy.prepare_for_lp_inference()
                        print(f"  âœ… Student policy prepared for logprob inference")
                    except Exception as e:
                        print(f"  âŒ Failed to prepare student policy for logprob inference: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                    # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆå¯è®­ç»ƒæ¨¡å‹ï¼‰
                    print("  âœ“ Computing student model logits...")
                    try:
                        # å…³é”®ä¿®å¤ï¼šåœ¨è°ƒç”¨get_logprobsä¹‹å‰ï¼Œå¼ºåˆ¶æ£€æŸ¥å¹¶ä¿®å¤æ‰€æœ‰å­—æ®µçš„å½¢çŠ¶
                        # print(f"  ğŸ” Final shape validation before calling get_logprobs...")
                        pass
                        
                        # æ£€æŸ¥å¹¶ä¿®å¤teacher_logitsçš„å½¢çŠ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if "teacher_logits" in train_data:
                            teacher_logits = train_data["teacher_logits"]
                            # print(f"  ğŸ” teacher_logits shape before final check: {teacher_logits.shape}")
                            pass
                            
                            # å¦‚æœteacher_logitsçš„å½¢çŠ¶ä¸æ­£ç¡®ï¼Œå¼ºåˆ¶ä¿®å¤
                            if len(teacher_logits.shape) != 3:
                                print(f"  âš ï¸ Warning: teacher_logits has wrong shape {teacher_logits.shape}, fixing...")
                                if len(teacher_logits.shape) == 2:
                                    # å¦‚æœæ˜¯[batch_size, vocab_size]ï¼Œé‡å¡‘ä¸º[batch_size, seq_len, vocab_size]
                                    batch_size = teacher_logits.shape[0]
                                    vocab_size = teacher_logits.shape[1]
                                    seq_len = train_data["input_ids"].shape[1]
                                    teacher_logits = teacher_logits.unsqueeze(1).expand(-1, seq_len, -1)
                                    # print(f"  ğŸ” Fixed teacher_logits shape: {teacher_logits.shape}")
                                    pass
                                else:
                                    print(f"  âŒ Critical error: teacher_logits has unexpected shape {teacher_logits.shape}")
                                    raise ValueError(f"teacher_logits has unexpected shape: {teacher_logits.shape}")
                            
                            # éªŒè¯ä¿®å¤åçš„å½¢çŠ¶
                            expected_shape = (train_data["input_ids"].shape[0], train_data["input_ids"].shape[1], -1)
                            if teacher_logits.shape[0] != expected_shape[0] or teacher_logits.shape[1] != expected_shape[1]:
                                print(f"  âŒ Critical error: teacher_logits shape still incorrect after fixing!")
                                # print(f"  ğŸ” Expected: {expected_shape}")
                                pass
                                # print(f"  ğŸ” Got: {teacher_logits.shape}")
                                pass
                                raise ValueError(f"Failed to fix teacher_logits shape")
                            
                            # æ›´æ–°train_dataä¸­çš„teacher_logits
                            train_data["teacher_logits"] = teacher_logits
                            print(f"  âœ… teacher_logits shape validation passed: {teacher_logits.shape}")
                        
                        # å…³é”®ä¿®å¤ï¼šç›´æ¥è°ƒç”¨å­¦ç”Ÿæ¨¡å‹è·å–logitsï¼Œè€Œä¸æ˜¯ä½¿ç”¨get_logprobs
                        # print(f"  ğŸ” Directly calling student model to get logits...")
                        pass
                        
                        # å‡†å¤‡è¾“å…¥æ•°æ®
                        input_ids = train_data["input_ids"].to("cuda")
                        attention_mask = torch.ones_like(input_ids, dtype=torch.long)
                        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0).expand(input_ids.shape[0], -1)
                        
                        # print(f"  ğŸ” Input shapes:")
                        pass
                        #print(f"  ğŸ”   input_ids: {input_ids.shape}")
                        #print(f"  ğŸ”   attention_mask: {attention_mask.shape}")
                        #print(f"  ğŸ”   position_ids: {position_ids.shape}")
                        
                        # ç›´æ¥è°ƒç”¨å­¦ç”Ÿæ¨¡å‹
                        with torch.no_grad():
                            student_policy.prepare_for_lp_inference()
                            
                            # è·å–ç³»ç»Ÿé…ç½®ä¿¡æ¯
                            # print(f"  ğŸ” Getting system configuration...")
                            pass
                            num_shards = len(student_policy.worker_group.workers)
                            # print(f"  ğŸ” Number of shards: {num_shards}")
                            pass
                            
                            # ç¡®ä¿batch sizeæ˜¯shardsçš„å€æ•°
                            current_batch_size = input_ids.shape[0]
                            if current_batch_size % num_shards != 0:
                                # è°ƒæ•´batch sizeåˆ°æœ€è¿‘çš„shardså€æ•°
                                adjusted_batch_size = ((current_batch_size // num_shards) + 1) * num_shards
                                # print(f"  ğŸ” Adjusting batch size from {current_batch_size} to {adjusted_batch_size} to match {num_shards} shards")
                                pass
                                
                                # æ‰©å±•æ•°æ®åˆ°è°ƒæ•´åçš„batch size
                                if adjusted_batch_size > current_batch_size:
                                    # é‡å¤æœ€åä¸€ä¸ªæ ·æœ¬æ¥å¡«å……
                                    padding_size = adjusted_batch_size - current_batch_size
                                    input_ids = torch.cat([input_ids, input_ids[-1:].repeat(padding_size, 1)], dim=0)
                                    attention_mask = torch.cat([attention_mask, attention_mask[-1:].repeat(padding_size, 1)], dim=0)
                                    position_ids = torch.cat([position_ids, position_ids[-1:].repeat(padding_size, 1)], dim=0)
                                    # print(f"  ğŸ” Expanded input shapes to: {input_ids.shape}")
                                    pass
                            
                            # åˆ›å»ºæ­£ç¡®çš„è®­ç»ƒæ•°æ®æ ¼å¼
                            # print(f"  ğŸ” Creating training data for get_logprobs...")
                            pass
                            train_data_for_logprobs = BatchedDataDict[DistillationLossDataDict]({
                                "input_ids": input_ids,
                                "input_lengths": torch.tensor([input_ids.shape[1]] * input_ids.shape[0]),
                                "advantages": torch.ones(input_ids.shape[0], input_ids.shape[1]),
                                "generation_logprobs": torch.zeros(input_ids.shape[0], input_ids.shape[1]),
                                "token_mask": torch.ones(input_ids.shape[0], input_ids.shape[1]),
                                "sample_mask": torch.ones(input_ids.shape[0]),
                            })
                            
                            # print(f"  ğŸ” Training data created with batch size: {train_data_for_logprobs.size}")
                            pass
                            # print(f"  ğŸ” Calling get_logprobs...")
                            pass
                            
                            try:
                                # ä½¿ç”¨get_logprobsæ–¹æ³•è·å–logits
                                result = student_policy.get_logprobs(train_data_for_logprobs)
                                # print(f"  ğŸ” get_logprobs successful")
                                pass
                                
                                # æ£€æŸ¥è¿”å›ç»“æœçš„ç»“æ„
                                # print(f"  ğŸ” Result keys: {list(result.keys())}")
                                pass
                                for key, value in result.items():
                                    if torch.is_tensor(value):
                                        # print(f"  ğŸ” {key}: {value.shape}")
                                        pass
                                    else:
                                        # print(f"  ğŸ” {key}: {type(value)}")
                                        pass
                                
                                # å°è¯•è·å–logits
                                if "logits" in result:
                                    student_logits = result["logits"]
                                    # print(f"  ğŸ” Successfully got logits from result")
                                    pass
                                elif "logprobs" in result:
                                    # å¦‚æœåªæœ‰logprobsï¼Œæˆ‘ä»¬éœ€è¦ä»logprobsé‡å»ºlogits
                                    # print(f"  ğŸ” Only logprobs available, attempting to reconstruct logits...")
                                    pass
                                    logprobs = result["logprobs"]
                                    # print(f"  ğŸ” logprobs shape: {logprobs.shape}")
                                    pass
                                    
                                    # è¿™é‡Œæˆ‘ä»¬éœ€è¦å®ç°ä»logprobsåˆ°logitsçš„è½¬æ¢
                                    # ç”±äºè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„è½¬æ¢ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨logprobsä½œä¸ºæ›¿ä»£
                                    # print(f"  ğŸ” Using logprobs as student_logits for now...")
                                    pass
                                    student_logits = logprobs.unsqueeze(-1).expand(-1, -1, 151936)  # å‡è®¾vocab_size=151936
                                    # print(f"  ğŸ” Reconstructed logits shape: {student_logits.shape}")
                                    pass
                                else:
                                    raise ValueError(f"Neither 'logits' nor 'logprobs' found in result: {list(result.keys())}")
                                
                            except Exception as e:
                                # print(f"  ğŸ” get_logprobs failed: {e}")
                                pass
                                # print(f"  ğŸ” Trying alternative approach...")
                                pass
                                
                                # å¦‚æœget_logprobså¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®æ¨¡å‹
                                try:
                                    # print(f"  ğŸ” Attempting to access model directly...")
                                    pass
                                    
                                    # è·å–ç¬¬ä¸€ä¸ªworker
                                    first_worker = student_policy.worker_group.workers[0]
                                    
                                    # æ£€æŸ¥workeræ˜¯å¦æœ‰modelå±æ€§
                                    # print(f"  ğŸ” Checking worker attributes...")
                                    pass
                                    worker_attrs = dir(first_worker)
                                    # print(f"  ğŸ” Worker attributes: {worker_attrs}")
                                    pass
                                    
                                    # å°è¯•è°ƒç”¨workerçš„get_logprobsæ–¹æ³•
                                    # print(f"  ğŸ” Calling worker.get_logprobs directly...")
                                    pass
                                    worker_result = first_worker.get_logprobs.remote(train_data_for_logprobs)
                                    worker_result = ray.get(worker_result)
                                    # print(f"  ğŸ” Worker get_logprobs successful")
                                    pass
                                    
                                    # å¤„ç†workerç»“æœ
                                    if "logits" in worker_result:
                                        student_logits = worker_result["logits"]
                                    elif "logprobs" in worker_result:
                                        logprobs = worker_result["logprobs"]
                                        student_logits = logprobs.unsqueeze(-1).expand(-1, -1, 151936)
                                    else:
                                        raise ValueError(f"Worker result missing logits/logprobs: {list(worker_result.keys())}")
                                        
                                except Exception as e2:
                                    # print(f"  ğŸ” Direct worker access also failed: {e2}")
                                    pass
                                    raise RuntimeError(f"All approaches to get student logits failed: {e2}")
                            
                            # print(f"  ğŸ” Raw student logits shape: {student_logits.shape}")
                            pass
                            
                            # å¦‚æœbatch sizeè¢«è°ƒæ•´äº†ï¼Œæ¢å¤åˆ°åŸå§‹å¤§å°
                            if student_logits.shape[0] > current_batch_size:
                                # print(f"  ğŸ” Restoring original batch size...")
                                pass
                                student_logits = student_logits[:current_batch_size]
                                # print(f"  ğŸ” Final student logits shape: {student_logits.shape}")
                                pass
                            
                            # åº”ç”¨æ¸©åº¦ç¼©æ”¾ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                            try:
                                # æ¸©åº¦ç¼©æ”¾é€šå¸¸åœ¨get_logprobså†…éƒ¨å¤„ç†ï¼Œè¿™é‡Œè·³è¿‡
                                # print(f"  ğŸ” Temperature scaling handled by get_logprobs")
                                pass
                            except Exception as e:
                                # print(f"  ğŸ” Temperature scaling failed: {e}, using original logits")
                                pass
                        
                        print(f"  âœ… Student logits computed successfully")
                        # print(f"  ğŸ” Student logits shape: {student_logits.shape}")
                        pass
                        
                        # å…³é”®ä¿®å¤ï¼šéªŒè¯student_logitsçš„å½¢çŠ¶
                        if student_logits.shape[0] != train_data["input_ids"].shape[0]:
                            print(f"  âš ï¸ Warning: Student logits batch dimension mismatch!")
                            # print(f"  ğŸ” Expected batch size: {train_data['input_ids'].shape[0]}")
                            pass
                            # print(f"  ğŸ” Got batch size: {student_logits.shape[0]}")
                            pass
                        
                        if student_logits.shape[1] != train_data["input_ids"].shape[1]:
                            print(f"  âš ï¸ Warning: Student logits sequence dimension mismatch!")
                            # print(f"  ğŸ” Expected seq len: {train_data['input_ids'].shape[1]}")
                            pass
                            # print(f"  ğŸ” Got seq len: {student_logits.shape[1]}")
                            pass
                        
                    except Exception as e:
                        print(f"  âŒ Failed to compute student logits: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                    # å°†å­¦ç”Ÿlogitsæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
                    train_data["student_logits"] = student_logits
                    print(f"  âœ… Student logits added to training data")
                    
                    # è®¡ç®—è’¸é¦æŸå¤±
                    print("  âœ“ Computing distillation loss...")
                    try:
                        # ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—è’¸é¦æŸå¤± - ä¿®å¤ï¼šä¼ é€’æ‰€æœ‰å¿…è¦çš„å‚æ•°
                        # å°†è’¸é¦å‚æ•°æ·»åŠ åˆ°train_dataä¸­ï¼Œä¾›æŸå¤±å‡½æ•°ä½¿ç”¨
                        train_data["kl_type"] = kl_type
                        train_data["lambda_"] = lambda_
                        train_data["mixed_kl_weight"] = mixed_kl_weight
                        
                        loss, loss_metrics = loss_fn(
                            student_logits,  # next_token_logits
                            train_data,      # data
                            torch.ones(train_data.size, dtype=torch.bool),  # global_valid_seqs
                            torch.ones_like(flat_messages["token_ids"], dtype=torch.bool),  # global_valid_toks
                        )
                        
                        print(f"  âœ… Distillation loss computed successfully")
                        # print(f"  ğŸ” Total loss: {loss.item():.6f}")
                        pass
                        # print(f"  ğŸ” Loss metrics: {loss_metrics}")
                        pass
                        
                        # è®°å½•æŸå¤±
                        if logger is not None:
                            # è®°å½•ä¸»è¦è®­ç»ƒæŸå¤±
                            logger.log_metrics({"train/loss": loss.item()}, step)
                            
                            # è®°å½•è¯¦ç»†çš„lossæŒ‡æ ‡
                            for k, v in loss_metrics.items():
                                if isinstance(v, (int, float)):
                                    logger.log_metrics({f"train/{k}": v}, step)
                            
                            # è®°å½•ç”Ÿæˆé•¿åº¦ç›¸å…³æŒ‡æ ‡
                            if "input_ids" in train_data:
                                input_lengths = (train_data["input_ids"] != 0).sum(dim=1)
                                avg_input_length = input_lengths.float().mean().item()
                                max_input_length = input_lengths.max().item()
                                min_input_length = input_lengths.min().item()
                                
                                logger.log_metrics({
                                    "train/avg_input_length": avg_input_length,
                                    "train/max_input_length": max_input_length,
                                    "train/min_input_length": min_input_length,
                                    "train/input_length_std": input_lengths.float().std().item(),
                                }, step)
                            
                            # è®°å½•å½“å‰æœ€ä½³éªŒè¯lossï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            if "val_loss" in distillation_save_state and distillation_save_state["val_loss"] is not None:
                                current_best_val_loss = distillation_save_state["val_loss"]
                                logger.log_metrics({"train/best_val_loss": current_best_val_loss}, step)
                                #print(f"  ğŸ” [Training] Current Best Val Loss = {current_best_val_loss:.6f}")
                            
                            # è®°å½•è’¸é¦å‚æ•°
                            logger.log_metrics({
                                "train/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                                "train/lambda": lambda_,
                                "train/mixed_kl_weight": mixed_kl_weight,
                            }, step)
                            
                            # æ‰“å°è®­ç»ƒlossä¿¡æ¯
                            print(f"  âœ…âœ…âœ… [Training] Step {step}: Loss = {loss.item():.6f}")
                            if "kl_loss" in loss_metrics:
                                print(f"  ğŸ” [Training] KL Loss = {loss_metrics['kl_loss']:.6f}")
                            
                            # æ‰“å°è’¸é¦å‚æ•°ä¿¡æ¯
                            #print(f"  ğŸ” [Training] KL Type: {kl_type}, Lambda: {lambda_}")
                        
                    except Exception as e:
                        print(f"  âŒ Failed to compute distillation loss: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                
                # 5. è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼ˆå®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼ï¼‰
                print("â–¶ Training student model...")
                # print(f"  ğŸ” student_policy type: {type(student_policy)}")
                pass
                
                # å…³é”®ä¿®å¤ï¼šåœ¨è®­ç»ƒä¹‹å‰æ£€æŸ¥å¹¶ä¿®å¤æ‰€æœ‰å¼ é‡çš„å½¢çŠ¶
                # print(f"  ğŸ” Pre-training shape validation and fixing...")
                pass
                for key, value in train_data.items():
                    if torch.is_tensor(value):
                        # print(f"  ğŸ” {key}: {value.shape}")
                        pass
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å½¢çŠ¶é—®é¢˜
                        if len(value.shape) > 1 and value.shape[1] > 100000:
                            print(f"  âš ï¸ Warning: {key} has suspiciously large sequence dimension: {value.shape[1]}")
                            # print(f"  ğŸ” This indicates a shape problem that needs fixing!")
                            pass
                            
                            # å°è¯•ä¿®å¤å½¢çŠ¶
                            if key in ["teacher_logits", "student_logits"]:
                                expected_batch_size = train_data["input_ids"].shape[0]
                                expected_seq_len = train_data["input_ids"].shape[1]
                                
                                if value.shape[0] == expected_batch_size:
                                    # æ¨æ–­vocab_size
                                    total_elements = value.shape[1]
                                    if total_elements % expected_seq_len == 0:
                                        inferred_vocab_size = total_elements // expected_seq_len
                                        # print(f"  ğŸ” Inferred vocab_size: {inferred_vocab_size}")
                                        pass
                                        
                                        # é‡å¡‘å¼ é‡
                                        try:
                                            fixed_value = value.view(expected_batch_size, expected_seq_len, inferred_vocab_size)
                                            train_data[key] = fixed_value
                                            # print(f"  ğŸ” Successfully fixed {key} shape: {fixed_value.shape}")
                                            pass
                                        except Exception as e:
                                            print(f"  âŒ Failed to fix {key} shape: {e}")
                                    else:
                                        print(f"  âŒ Cannot infer correct shape for {key}")
                                else:
                                    print(f"  âŒ Batch size mismatch for {key}")
                        
                        # æ£€æŸ¥sequenceç»´åº¦æ˜¯å¦åŒ¹é…
                        if len(value.shape) > 1 and value.shape[1] != train_data["input_ids"].shape[1]:
                            print(f"  âš ï¸ Warning: {key} sequence dimension mismatch!")
                            # print(f"  ğŸ” Expected: {train_data['input_ids'].shape[1]}, Got: {value.shape[1]}")
                            pass
                            
                            # å°è¯•ä¿®å¤sequenceç»´åº¦
                            if value.shape[1] > train_data["input_ids"].shape[1]:
                                # æˆªæ–­åˆ°æ­£ç¡®é•¿åº¦
                                train_data[key] = value[:, :train_data["input_ids"].shape[1]]
                                # print(f"  ğŸ” Fixed {key} by truncating to: {train_data[key].shape}")
                                pass
                            else:
                                # æ‰©å±•åˆ°æ­£ç¡®é•¿åº¦
                                train_data[key] = value.expand(-1, train_data["input_ids"].shape[1], -1)
                                # print(f"  ğŸ” Fixed {key} by expanding to: {train_data[key].shape}")
                                pass
                
                # æœ€ç»ˆéªŒè¯
                # print(f"  ğŸ” Final shape validation before training:")
                pass
                for key, value in train_data.items():
                    if torch.is_tensor(value):
                        # print(f"  ğŸ”   {key}: {value.shape}")
                        pass
                
                # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                all_batch_sizes = [train_data[key].shape[0] for key in train_data.keys() if torch.is_tensor(train_data[key])]
                if len(set(all_batch_sizes)) != 1:
                    print(f"  âŒ Critical error: Batch dimensions are not consistent!")
                    print(f"  ğŸ” Batch sizes: {all_batch_sizes}")
                    raise ValueError(f"Batch dimensions must be consistent, got: {all_batch_sizes}")
                
                print(f"  âœ… All batch dimensions are consistent: {all_batch_sizes[0]}")
                
                # å…³é”®ä¿®å¤ï¼šåˆ›å»ºè’¸é¦ä¸“ç”¨çš„æ•°æ®åŒ…è£…å™¨ï¼Œé¿å…åœ¨workerå†…éƒ¨è¿›è¡Œå½¢çŠ¶ä¿®å¤
                # print(f"  ğŸ” Creating distillation-safe training data...")
                pass
                
                # æ–¹æ³•1ï¼šå°†logitsè½¬æ¢ä¸ºworkeræœŸæœ›çš„æ ¼å¼
                # ç”±äºworkeræœŸæœ›sequenceç»´åº¦åœ¨dim 1ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°æ’åˆ—logits
                distillation_safe_data = {}
                
                for key, value in train_data.items():
                    if key in ["teacher_logits", "student_logits"]:
                        # å¯¹äºlogitsï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å®ƒä»¬ä¸ä¼šè¢«workerè¯¯è§£
                        # æ–¹æ³•ï¼šå°†logitsè½¬æ¢ä¸ºworkeræœŸæœ›çš„æ ¼å¼ï¼Œæˆ–è€…æš‚æ—¶ç§»é™¤å®ƒä»¬
                        # print(f"  ğŸ” Processing {key} for distillation safety...")
                        pass
                        
                        if len(value.shape) == 3:
                            # å¦‚æœlogitså½¢çŠ¶æ­£ç¡®ï¼Œæˆ‘ä»¬æš‚æ—¶å°†å®ƒä»¬å­˜å‚¨ä¸ºå…¶ä»–æ ¼å¼
                            # æ–¹æ³•ï¼šå°†logitsè½¬æ¢ä¸ºworkerä¸ä¼šæ£€æŸ¥çš„æ ¼å¼
                            # æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è½¬æ¢ä¸º1Då¼ é‡ï¼Œç„¶ååœ¨loss functionä¸­æ¢å¤
                            batch_size, seq_len, vocab_size = value.shape
                            flattened_logits = value.view(batch_size * seq_len, vocab_size)
                            
                            # åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„keyï¼Œworkerä¸ä¼šæ£€æŸ¥
                            safe_key = f"distillation_{key}_flattened"
                            distillation_safe_data[safe_key] = flattened_logits
                            
                            # å­˜å‚¨åŸå§‹å½¢çŠ¶ä¿¡æ¯
                            distillation_safe_data[f"{safe_key}_shape"] = torch.tensor([batch_size, seq_len, vocab_size])
                            
                            # print(f"  ğŸ” Converted {key} to safe format: {flattened_logits.shape}")
                            pass
                        else:
                            print(f"  âš ï¸ Warning: {key} has unexpected shape: {value.shape}")
                            distillation_safe_data[key] = value
                    else:
                        # å¯¹äºå…¶ä»–å­—æ®µï¼Œç›´æ¥å¤åˆ¶
                        distillation_safe_data[key] = value
                
                # éªŒè¯å®‰å…¨æ•°æ®
                # print(f"  ğŸ” Distillation-safe data keys: {list(distillation_safe_data.keys())}")
                pass
                for key, value in distillation_safe_data.items():
                    if torch.is_tensor(value):
                        # print(f"  ğŸ”   {key}: {value.shape}")
                        pass
                
                # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ‰€æœ‰å­—æ®µçš„batch sizeæ˜¯å¦ä¸€è‡´
                # print(f"  ğŸ” Checking batch size consistency across all fields...")
                pass
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰å­—æ®µçš„ç±»å‹
                print(f"  ğŸ” Distillation safe data fields:")
                for key, value in distillation_safe_data.items():
                    if torch.is_tensor(value):
                        print(f"    {key}: tensor {value.shape}")
                    elif isinstance(value, (list, tuple)):
                        print(f"    {key}: {type(value).__name__} with {len(value)} items")
                    elif isinstance(value, (int, float)):
                        print(f"    {key}: {type(value).__name__} = {value}")
                    else:
                        print(f"    {key}: {type(value).__name__}")
                
                batch_sizes = {}
                for key, value in distillation_safe_data.items():
                    if torch.is_tensor(value):
                        if len(value.shape) > 0:
                            batch_sizes[key] = value.shape[0]
                        else:
                            batch_sizes[key] = 1  # æ ‡é‡å¼ é‡
                    elif isinstance(value, (list, tuple)):
                        batch_sizes[key] = len(value)
                    elif isinstance(value, (int, float)):
                        batch_sizes[key] = 1  # æ ‡é‡å€¼
                    else:
                        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è°ƒç”¨lenï¼Œå¦‚æœå¤±è´¥åˆ™è®¾ä¸º1
                        try:
                            batch_sizes[key] = len(value)
                        except (TypeError, AttributeError):
                            batch_sizes[key] = 1
                            print(f"  âš ï¸ Warning: Field {key} has unsupported type {type(value)}, setting batch size to 1")
                
                # print(f"  ğŸ” Batch sizes for each field:")
                pass
                for key, size in batch_sizes.items():
                    # print(f"  ğŸ”   {key}: {size}")
                    pass
                
                # æ£€æŸ¥batch sizeæ˜¯å¦ä¸€è‡´
                unique_batch_sizes = set(batch_sizes.values())
                if len(unique_batch_sizes) != 1:
                    print(f"  âŒ Critical error: Batch sizes are not consistent!")
                    print(f"  ğŸ” Unique batch sizes: {unique_batch_sizes}")
                    # print(f"  ğŸ” This will cause shard_by_batch_size to fail!")
                    pass
                    
                    # å…³é”®ä¿®å¤ï¼šåªä¿®å¤æ ‡å‡†è®­ç»ƒå­—æ®µï¼Œä¿æŒè’¸é¦å­—æ®µä¸å˜
                    # print(f"  ğŸ” Attempting to fix batch size inconsistencies...")
                    pass
                    
                    # è¿‡æ»¤æ‰è’¸é¦ç›¸å…³çš„ç‰¹æ®Šå­—æ®µï¼Œåªè€ƒè™‘æ ‡å‡†è®­ç»ƒå­—æ®µ
                    standard_fields = ["input_ids", "input_lengths", "advantages", "generation_logprobs", "token_mask", "sample_mask"]
                    distillation_fields = [k for k in batch_sizes.keys() if k.startswith("distillation_")]
                    
                    # print(f"  ğŸ” Standard fields: {standard_fields}")
                    pass
                    # print(f"  ğŸ” Distillation fields: {distillation_fields}")
                    pass
                    
                    # åªæ£€æŸ¥æ ‡å‡†å­—æ®µçš„batch sizeä¸€è‡´æ€§
                    standard_batch_sizes = {k: v for k, v in batch_sizes.items() if k in standard_fields}
                    distillation_batch_sizes = {k: v for k, v in batch_sizes.items() if k in distillation_fields}
                    
                    # print(f"  ğŸ” Standard field batch sizes: {standard_batch_sizes}")
                    pass
                    # print(f"  ğŸ” Distillation field batch sizes: {distillation_batch_sizes}")
                    pass
                    
                    # æ£€æŸ¥æ ‡å‡†å­—æ®µçš„batch sizeæ˜¯å¦ä¸€è‡´
                    unique_standard_batch_sizes = set(standard_batch_sizes.values())
                    if len(unique_standard_batch_sizes) != 1:
                        print(f"  âŒ Standard fields have inconsistent batch sizes: {unique_standard_batch_sizes}")
                        
                        # ä¿®å¤æ ‡å‡†å­—æ®µçš„batch sizeä¸ä¸€è‡´
                        target_standard_batch_size = max(unique_standard_batch_sizes)
                        # print(f"  ğŸ” Fixing standard fields to batch size: {target_standard_batch_size}")
                        pass
                        
                        for key in standard_fields:
                            if key in distillation_safe_data:
                                value = distillation_safe_data[key]
                                if torch.is_tensor(value):
                                    current_batch_size = value.shape[0] if len(value.shape) > 0 else 1
                                    if current_batch_size != target_standard_batch_size:
                                        # print(f"  ğŸ” Fixing standard field {key}: {current_batch_size} -> {target_standard_batch_size}")
                                        pass
                                        
                                        if len(value.shape) == 1:
                                            # 1Då¼ é‡
                                            if current_batch_size < target_standard_batch_size:
                                                repeats = (target_standard_batch_size + current_batch_size - 1) // current_batch_size
                                                value = value.repeat(repeats)[:target_standard_batch_size]
                                            else:
                                                value = value[:target_standard_batch_size]
                                        elif len(value.shape) == 2:
                                            # 2Då¼ é‡
                                            if current_batch_size < target_standard_batch_size:
                                                repeats = (target_standard_batch_size + current_batch_size - 1) // current_batch_size
                                                value = value.repeat(repeats, 1)[:target_standard_batch_size]
                                            else:
                                                value = value[:target_standard_batch_size]
                                        elif len(value.shape) == 3:
                                            # 3Då¼ é‡
                                            if current_batch_size < target_standard_batch_size:
                                                repeats = (target_standard_batch_size + current_batch_size - 1) // current_batch_size
                                                value = value.repeat(repeats, 1, 1)[:target_standard_batch_size]
                                            else:
                                                value = value[:target_standard_batch_size]
                                        
                                        distillation_safe_data[key] = value
                                        # print(f"  ğŸ” Fixed {key} shape: {value.shape}")
                                        pass
                                else:
                                    # å®‰å…¨åœ°è·å–batch size
                                    if isinstance(value, (list, tuple)):
                                        current_batch_size = len(value)
                                    elif isinstance(value, (int, float)):
                                        current_batch_size = 1
                                    else:
                                        try:
                                            current_batch_size = len(value)
                                        except (TypeError, AttributeError):
                                            current_batch_size = 1
                                            print(f"  âš ï¸ Warning: Cannot determine batch size for field {key} of type {type(value)}")
                                    
                                    if current_batch_size != target_standard_batch_size:
                                        # print(f"  ğŸ” Fixing standard field {key}: {current_batch_size} -> {target_standard_batch_size}")
                                        pass
                                        
                                        if current_batch_size < target_standard_batch_size:
                                            if isinstance(value, (list, tuple)):
                                                repeats = (target_standard_batch_size + current_batch_size - 1) // current_batch_size
                                                value = (value * repeats)[:target_standard_batch_size]
                                            else:
                                                # å¯¹äºæ ‡é‡å€¼ï¼Œåˆ›å»ºé‡å¤åˆ—è¡¨
                                                value = [value] * target_standard_batch_size
                                        else:
                                            if isinstance(value, (list, tuple)):
                                                value = value[:target_standard_batch_size]
                                            else:
                                                # å¯¹äºæ ‡é‡å€¼ï¼Œä¿æŒä¸å˜
                                                pass
                                        
                                        distillation_safe_data[key] = value
                                        # print(f"  ğŸ” Fixed {key} length: {len(value) if hasattr(value, '__len__') else 'scalar'}")
                                        pass
                    else:
                        print(f"  âœ… Standard fields have consistent batch size: {unique_standard_batch_sizes.pop()}")
                    
                    # è’¸é¦å­—æ®µçš„batch sizeå¯ä»¥ä¸åŒï¼Œè¿™æ˜¯æ­£å¸¸çš„
                    if len(distillation_batch_sizes) > 0:
                        print(f"  â„¹ï¸ Distillation fields have different batch sizes (this is normal):")
                        for key, size in distillation_batch_sizes.items():
                            print(f"  â„¹ï¸   {key}: {size}")
                    
                    # é‡æ–°æ£€æŸ¥æ ‡å‡†å­—æ®µçš„batch sizeä¸€è‡´æ€§
                    # print(f"  ğŸ” Re-checking standard field batch size consistency after fixes...")
                    pass
                    standard_batch_sizes_after_fix = {}
                    for key in standard_fields:
                        if key in distillation_safe_data:
                            value = distillation_safe_data[key]
                            if torch.is_tensor(value):
                                if len(value.shape) > 0:
                                    standard_batch_sizes_after_fix[key] = value.shape[0]
                                else:
                                    standard_batch_sizes_after_fix[key] = 1
                            else:
                                standard_batch_sizes_after_fix[key] = len(value)
                    
                    unique_standard_batch_sizes_after_fix = set(standard_batch_sizes_after_fix.values())
                    if len(unique_standard_batch_sizes_after_fix) == 1:
                        print(f"  âœ… Standard field batch size consistency fixed: {unique_standard_batch_sizes_after_fix.pop()}")
                    else:
                        print(f"  âŒ Failed to fix standard field batch size consistency!")
                        # print(f"  ğŸ” Remaining unique standard field batch sizes: {unique_standard_batch_sizes_after_fix}")
                        pass
                        raise ValueError(f"Could not fix standard field batch size inconsistencies: {unique_standard_batch_sizes_after_fix}")
                else:
                    print(f"  âœ… All fields have consistent batch size: {unique_batch_sizes.pop()}")
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ["input_ids", "input_lengths", "token_mask", "sample_mask"]
                missing_fields = [field for field in required_fields if field not in distillation_safe_data]
                if missing_fields:
                    print(f"  âŒ Critical error: Missing required fields: {missing_fields}")
                    raise ValueError(f"Missing required fields: {missing_fields}")
                
                print(f"  âœ… All required fields present in distillation-safe data")
                
                # ä½¿ç”¨å®‰å…¨æ•°æ®è¿›è¡Œè®­ç»ƒ - å…³é”®ä¿®å¤ï¼šä¿æŒBatchedDataDictç±»å‹
                # print(f"  ğŸ” Converting distillation-safe data back to BatchedDataDict...")
                pass
                
                # åˆ›å»ºæ–°çš„BatchedDataDictï¼Œä¿æŒåŸå§‹ç±»å‹
                train_data = BatchedDataDict[DistillationLossDataDict](distillation_safe_data)
                
                # print(f"  ğŸ” Final train_data type: {type(train_data)}")
                pass
                # print(f"  ğŸ” Final train_data keys: {list(train_data.keys())}")
                pass
                
                # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿BatchedDataDictæœ‰æ­£ç¡®çš„æ–¹æ³•
                if not hasattr(train_data, 'shard_by_batch_size'):
                    print(f"  âŒ Critical error: train_data does not have shard_by_batch_size method!")
                    # print(f"  ğŸ” train_data type: {type(train_data)}")
                    pass
                    # print(f"  ğŸ” train_data methods: {[method for method in dir(train_data) if not method.startswith('_')]}")
                    pass
                    raise ValueError("train_data must be a proper BatchedDataDict with shard_by_batch_size method")
                
                print(f"  âœ… train_data has required methods for training")
                
                # å…³é”®ä¿®å¤ï¼šåœ¨ä¼ é€’ç»™train()ä¹‹å‰ï¼Œåˆ›å»ºåªåŒ…å«æ ‡å‡†è®­ç»ƒå­—æ®µçš„å¹²å‡€æ•°æ®
                # print(f"  ğŸ” Creating clean training data without distillation fields...")
                pass
                
                # åªä¿ç•™æ ‡å‡†è®­ç»ƒå­—æ®µ
                standard_fields = ["input_ids", "input_lengths", "advantages", "generation_logprobs", "token_mask", "sample_mask"]
                clean_training_data = {}
                
                for field in standard_fields:
                    if field in train_data:
                        clean_training_data[field] = train_data[field]
                        # print(f"  ğŸ” Added {field}: {train_data[field].shape if torch.is_tensor(train_data[field]) else len(train_data[field])}")
                        pass
                    else:
                        print(f"  âš ï¸ Warning: Required field {field} not found in train_data!")
                
                # éªŒè¯å¹²å‡€æ•°æ®çš„batch sizeä¸€è‡´æ€§
                # print(f"  ğŸ” Verifying clean training data batch size consistency...")
                pass
                clean_batch_sizes = {}
                for key, value in clean_training_data.items():
                    if torch.is_tensor(value):
                        if len(value.shape) > 0:
                            clean_batch_sizes[key] = value.shape[0]
                        else:
                            clean_batch_sizes[key] = 1
                    else:
                        clean_batch_sizes[key] = len(value)
                
                # print(f"  ğŸ” Clean training data batch sizes:")
                pass
                for key, size in clean_batch_sizes.items():
                    # print(f"  ğŸ”   {key}: {size}")
                    pass
                
                unique_clean_batch_sizes = set(clean_batch_sizes.values())
                if len(unique_clean_batch_sizes) == 1:
                    print(f"  âœ… Clean training data has consistent batch size: {unique_clean_batch_sizes.pop()}")
                else:
                    print(f"  âŒ Clean training data still has inconsistent batch sizes: {unique_clean_batch_sizes}")
                    raise ValueError(f"Clean training data batch sizes are not consistent: {unique_clean_batch_sizes}")
                
                # åˆ›å»ºæœ€ç»ˆçš„å¹²å‡€BatchedDataDict
                final_train_data = BatchedDataDict[DistillationLossDataDict](clean_training_data)
                # print(f"  ğŸ” Final clean train_data type: {type(final_train_data)}")
                pass
                # print(f"  ğŸ” Final clean train_data keys: {list(final_train_data.keys())}")
                pass
                
                # å…³é”®ä¿®å¤ï¼šå°†è’¸é¦æ•°æ®å­˜å‚¨ä¸ºå±æ€§ï¼Œè€Œä¸æ˜¯å­—å…¸é”®å€¼å¯¹
                # print(f"  ğŸ” Storing distillation data as attributes...")
                pass
                final_train_data.distillation_teacher_logits = distillation_safe_data.get("distillation_teacher_logits_flattened")
                final_train_data.distillation_teacher_logits_shape = distillation_safe_data.get("distillation_teacher_logits_flattened_shape")
                final_train_data.distillation_student_logits = distillation_safe_data.get("distillation_student_logits_flattened")
                final_train_data.distillation_student_logits_shape = distillation_safe_data.get("distillation_student_logits_flattened_shape")
                
                # print(f"  ğŸ” Distillation data stored as attributes:")
                pass
                #print(f"  ğŸ”   distillation_teacher_logits: {final_train_data.distillation_teacher_logits.shape if final_train_data.distillation_teacher_logits is not None else 'None'}")
                #print(f"  ğŸ”   distillation_teacher_logits_shape: {final_train_data.distillation_teacher_logits_shape.shape if final_train_data.distillation_teacher_logits_shape is not None else 'None'}")
                #print(f"  ğŸ”   distillation_student_logits: {final_train_data.distillation_student_logits.shape if final_train_data.distillation_student_logits is not None else 'None'}")
                #print(f"  ğŸ”   distillation_student_logits_shape: {final_train_data.distillation_student_logits_shape.shape if final_train_data.distillation_student_logits_shape is not None else 'None'}")
                
                # å…³é”®ä¿®å¤ï¼šåŒæ—¶å°†è’¸é¦æ•°æ®å­˜å‚¨åœ¨ä»¥_å¼€å¤´çš„ç‰¹æ®Šå­—æ®µä¸­ï¼Œç¡®ä¿èƒ½é€šè¿‡Rayä¼ é€’
                # print(f"  ğŸ” Also storing distillation data in special _ fields for Ray compatibility...")
                pass
                final_train_data["_distillation_teacher_logits"] = distillation_safe_data.get("distillation_teacher_logits_flattened")
                final_train_data["_distillation_teacher_logits_shape"] = distillation_safe_data.get("distillation_teacher_logits_flattened_shape")
                final_train_data["_distillation_student_logits"] = distillation_safe_data.get("distillation_student_logits_flattened")
                final_train_data["_distillation_student_logits_shape"] = distillation_safe_data.get("distillation_student_logits_flattened_shape")
                
                #print(f"  ğŸ” Distillation data also stored in _ fields:")
                #print(f"  ğŸ”   _distillation_teacher_logits: {final_train_data['_distillation_teacher_logits'].shape if final_train_data['_distillation_teacher_logits'] is not None else 'None'}")
                #print(f"  ğŸ”   _distillation_teacher_logits_shape: {final_train_data['_distillation_teacher_logits_shape'].shape if final_train_data['_distillation_teacher_logits_shape'] is not None else 'None'}")
                #print(f"  ğŸ”   _distillation_student_logits: {final_train_data['_distillation_student_logits'].shape if final_train_data['_distillation_student_logits'] is not None else 'None'}")
                # print(f"  ğŸ”   _distillation_student_logits_shape: {final_train_data['_distillation_student_logits_shape'].shape if final_train_data['_distillation_student_logits_shape'] is not None else 'None'}")
                pass
                
                # å…³é”®ä¿®å¤ï¼šéªŒè¯final_train_dataåªåŒ…å«æ ‡å‡†è®­ç»ƒå­—æ®µï¼Œä¸åŒ…å«è’¸é¦å­—æ®µ
                # print(f"  ğŸ” Verifying final_train_data only contains standard fields...")
                pass
                final_keys = list(final_train_data.keys())
                # print(f"  ğŸ” Final train_data keys: {final_keys}")
                pass
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è’¸é¦å­—æ®µ
                distillation_keys = [k for k in final_keys if k.startswith(('distillation_', '_distillation_'))]
                if distillation_keys:
                    print(f"  âš ï¸ Warning: final_train_data still contains distillation fields: {distillation_keys}")
                    # print(f"  ğŸ” This will cause shard_by_batch_size to fail!")
                    pass
                    
                    # ç§»é™¤è’¸é¦å­—æ®µï¼Œåªä¿ç•™æ ‡å‡†å­—æ®µ
                    # print(f"  ğŸ” Removing distillation fields to fix the issue...")
                    pass
                    for key in distillation_keys:
                        del final_train_data[key]
                        # print(f"  ğŸ” Removed: {key}")
                        pass
                    
                    # print(f"  ğŸ” Final train_data keys after cleanup: {list(final_train_data.keys())}")
                    pass
                else:
                    print(f"  âœ… final_train_data only contains standard fields")
                
                # ä½¿ç”¨å¹²å‡€çš„è®­ç»ƒæ•°æ®
                train_data = final_train_data
                
                with timer.time("training_prep"):
                    # print(f"  ğŸ” Preparing student policy for training...")
                    pass
                    student_policy.prepare_for_training()  # ä¸GRPOå®Œå…¨ä¸€è‡´
                    STUDENT_GENERATION_STALE = True  # *** MARK AS STALE AFTER TRAINING ***
                    print(f"  âœ… Student policy prepared for training")
                
                with timer.time("policy_training"):
                    # print(f"  ğŸ” Starting policy training...")
                    pass
                    try:
                        train_results = student_policy.train(train_data, loss_fn)
                        print("  âœ… Training completed")
                    except Exception as e:
                        print(f"  âŒ Policy training failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                
                # 6. æ›´æ–°çŠ¶æ€
                # print(f"  ğŸ” Updating training state...")
                pass
                step += 1
                distillation_save_state["step"] = step
                # ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼Œä¸GRPOä¿æŒä¸€è‡´
                distillation_save_state["consumed_samples"] += distillation_config.get("num_prompts_per_step", 1)
                print(f"  âœ… Training state updated: step={step}, consumed_samples={distillation_save_state['consumed_samples']}")
                
                # 7. ä¿å­˜æ£€æŸ¥ç‚¹
                if step % distillation_config["save_steps"] == 0:
                    print(f"  âœ“ Saving checkpoint at step {step}")
                    # ä½¿ç”¨ä¸GRPOç›¸åŒçš„æ£€æŸ¥ç‚¹ä¿å­˜é€»è¾‘
                    try:
                        checkpoint_path = checkpointer.init_tmp_checkpoint(
                            step, distillation_save_state, master_config
                        )
                        student_policy.save_checkpoint(
                            weights_path=os.path.join(checkpoint_path, "policy", "weights"),
                            optimizer_path=os.path.join(checkpoint_path, "policy", "optimizer"),
                            tokenizer_path=os.path.join(checkpoint_path, "policy", "tokenizer"),
                        )
                        # ä¿å­˜æ•°æ®åŠ è½½å™¨çŠ¶æ€
                        torch.save(
                            train_dataloader.state_dict(),
                            os.path.join(checkpoint_path, "train_dataloader.pt"),
                        )
                        checkpointer.finalize_checkpoint(checkpoint_path)
                        print(f"  âœ… Checkpoint saved successfully")
                    except Exception as e:
                        print(f"  âŒ Failed to save checkpoint: {e}")
                        import traceback
                        traceback.print_exc()
                
                # 8. éªŒè¯ï¼ˆå®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼ï¼‰
                if step % distillation_config["eval_steps"] == 0 and val_dataloader is not None:
                    print(f"  âœ“ Running validation at step {step}")
                    try:
                        if NEED_REFIT and STUDENT_GENERATION_STALE:
                            # print(f"  ğŸ” Refitting for validation...")
                            pass
                            # ä¼ é€’ç”Ÿæˆé…ç½®å‚æ•°
                            generation_config = {
                                'temperature': temperature,
                                'decoding_method': decoding_method,
                                'max_length': max_length,
                            }
                            refit_student_generation(
                                student_policy, student_generation, colocated_inference, generation_config=generation_config
                            )
                            STUDENT_GENERATION_STALE = False
                        else:
                            if student_generation is not None:
                                # print(f"  ğŸ” Preparing generation for validation...")
                                pass
                                student_generation.prepare_for_generation()
                        
                        print(f"  ğŸ” Running validation...")
                        val_metrics = validate(
                            student_generation,
                            val_dataloader,
                            tokenizer,
                            step + 1,
                            master_config,
                        )
                        print(f"  âœ… Validation completed")
                        
                        # è®°å½•éªŒè¯æŒ‡æ ‡
                        if val_metrics:
                            # è®°å½•éªŒè¯loss - æ·»åŠ eval/lossè®°å½•
                            if "val_loss" in val_metrics:
                                # è®°å½•åˆ°validation/å‘½åç©ºé—´
                                logger.log_metrics({"validation/val_loss": val_metrics["val_loss"]}, step + 1)
                                # åŒæ—¶è®°å½•åˆ°eval/å‘½åç©ºé—´ï¼Œä¸GRPO/SFTä¿æŒä¸€è‡´
                                logger.log_metrics({"eval/loss": val_metrics["val_loss"]}, step + 1)
                                distillation_save_state["val_loss"] = val_metrics["val_loss"]
                                print(f"  âœ…âœ…âœ… [Validation] Step {step + 1}: Val Loss = {val_metrics['val_loss']:.6f}")
                                print(f"  ğŸ” [Eval] Step {step + 1}: Eval Loss = {val_metrics['val_loss']:.6f}")
                            
                            # è®°å½•å…¶ä»–éªŒè¯æŒ‡æ ‡
                            for k, v in val_metrics.items():
                                if k != "val_loss" and isinstance(v, (int, float)):
                                    logger.log_metrics({f"validation/{k}": v}, step + 1)
                                    # åŒæ—¶è®°å½•åˆ°eval/å‘½åç©ºé—´
                                    logger.log_metrics({f"eval/{k}": v}, step + 1)
                            
                            # è®°å½•éªŒè¯æ—¶çš„ç”Ÿæˆé•¿åº¦ä¿¡æ¯
                            if "val_avg_sequence_length" in val_metrics:
                                # è®°å½•åˆ°validation/å‘½åç©ºé—´
                                logger.log_metrics({
                                    "validation/avg_sequence_length": val_metrics["val_avg_sequence_length"],
                                    "validation/max_sequence_length": val_metrics.get("val_max_sequence_length", 0),
                                    "validation/min_sequence_length": val_metrics.get("val_min_sequence_length", 0),
                                }, step + 1)
                                
                                # åŒæ—¶è®°å½•åˆ°eval/å‘½åç©ºé—´
                                logger.log_metrics({
                                    "eval/avg_sequence_length": val_metrics["val_avg_sequence_length"],
                                    "eval/max_sequence_length": val_metrics.get("val_max_sequence_length", 0),
                                    "eval/min_sequence_length": val_metrics.get("val_min_sequence_length", 0),
                                }, step + 1)
                                
                                # æ‰“å°éªŒè¯é•¿åº¦ä¿¡æ¯
                                print(f"  ğŸ” [Validation] Avg Sequence Length = {val_metrics['val_avg_sequence_length']:.1f}")
                                print(f"  ğŸ” [Validation] Max Sequence Length = {val_metrics.get('val_max_sequence_length', 0)}")
                                print(f"  ğŸ” [Validation] Min Sequence Length = {val_metrics.get('val_min_sequence_length', 0)}")
                            
                            # è®°å½•éªŒè¯æ—¶çš„è’¸é¦å‚æ•°
                            logger.log_metrics({
                                "validation/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                                "validation/lambda": lambda_,
                                "validation/mixed_kl_weight": mixed_kl_weight,
                                "eval/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                                "eval/lambda": lambda_,
                                "eval/mixed_kl_weight": mixed_kl_weight,
                            }, step + 1)
                        
                        if student_generation is not None:
                            student_generation.finish_generation()
                    except Exception as e:
                        print(f"  âŒ Validation failed: {e}")
                        import traceback
                        traceback.print_exc()
                
                # 9. æ—¥å¿—è®°å½•
                if step % distillation_config["logging_steps"] == 0:
                    print(f"  ğŸ” Logging metrics...")
                    try:
                        logger.log_metrics({
                            "step": step,
                            "loss": loss.item(), # Changed from loss.item() to kl_loss.item()
                            "consumed_samples": distillation_save_state["consumed_samples"],
                        })
                        print(f"  âœ… Metrics logged successfully")
                    except Exception as e:
                        print(f"  âŒ Failed to log metrics: {e}")
                        import traceback
                        traceback.print_exc()
                
                print(f"  âœ… Step {step + 1} completed successfully")
    
    except Exception as e:
        print(f"âŒ Distillation training failed with error: {e}")
        print(f"ğŸ” Error occurred at step {step + 1}, batch_idx {batch_idx if 'batch_idx' in locals() else 'unknown'}")
        import traceback
        traceback.print_exc()
