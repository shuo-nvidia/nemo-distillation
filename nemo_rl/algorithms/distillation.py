# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and limitations.
import os
import warnings
from contextlib import nullcontext
from pathlib import Path
from typing import Any, NotRequired, Optional, TypedDict, TypeVar, cast

import numpy as np
import ray
import torch
from torchdata.stateful_dataloader import StatefulDataLoader
from transformers.tokenization_utils_base import PreTrainedTokenizerBase
from nemo_rl.algorithms.loss_functions import (
    DistillationLossConfig,
    DistillationLossDataDict,
    DistillationLossFn,
)
from nemo_rl.algorithms.utils import set_seed
from nemo_rl.data import DataConfig
from nemo_rl.data.datasets import AllTaskProcessedDataset, rl_collate_fn
from nemo_rl.data.interfaces import DatumSpec
from nemo_rl.data.llm_message_utils import batched_message_log_to_flat_message
from nemo_rl.distributed.batched_data_dict import BatchedDataDict
from nemo_rl.distributed.virtual_cluster import (
    ClusterConfig,
    RayVirtualCluster,
)
from nemo_rl.models.generation.interfaces import (
    GenerationInterface,
)
from nemo_rl.models.generation.vllm import VllmConfig, VllmGeneration
from nemo_rl.models.policy import PolicyConfig
from nemo_rl.models.policy.interfaces import ColocatablePolicyInterface
from nemo_rl.models.policy.lm_policy import Policy
from nemo_rl.utils.checkpoint import CheckpointingConfig, CheckpointManager
from nemo_rl.utils.logger import (
    Logger,
    LoggerConfig,
)
from nemo_rl.utils.timer import Timer
from nemo_rl.experience.rollouts import (
    run_multi_turn_rollout,
)

# ===============================================================================
# Configuration
# ===============================================================================
TokenizerType = TypeVar("TokenizerType", bound=PreTrainedTokenizerBase)


class DistillationConfig(TypedDict):
    # Teacher model path (for loading weights)
    teacher_model_path: str
    
    # Distillation strategy parameters
    kl_type: str    # KL divergence type: forward, reverse, mixed
    generate_strategy: dict[str, Any]  # Generation strategy parameters
    
    # Training configuration
    max_steps: int
    eval_steps: int
    save_steps: int
    logging_steps: int


class MasterConfig(TypedDict):
    """Main configuration structure"""
    policy: PolicyConfig  # Student model configuration
    loss_fn: DistillationLossConfig  # Loss function configuration
    env: dict[str, Any]  # Environment configuration
    data: DataConfig  # Data configuration
    distillation: DistillationConfig  # Distillation configuration
    logger: LoggerConfig  # Logger configuration
    cluster: ClusterConfig  # Cluster configuration
    checkpointing: CheckpointingConfig  # Checkpointing configuration


class DistillationSaveState(TypedDict):
    step: int
    val_loss: NotRequired[float]
    consumed_samples: int


def _default_distillation_save_state() -> DistillationSaveState:
    return {
        "step": 0,
        "consumed_samples": 0,
    }


# ===============================================================================
# Setup Functions
# ===============================================================================


def setup(
    master_config: MasterConfig,
    tokenizer: TokenizerType,
    train_dataset: AllTaskProcessedDataset,
    val_dataset: Optional[AllTaskProcessedDataset],
) -> tuple[
    ColocatablePolicyInterface,  # student_policy (single Policy instance)
    Optional[GenerationInterface],  # student_generation
    StatefulDataLoader,
    Optional[StatefulDataLoader],
    TokenizerType,  # tokenizer
    DistillationLossFn,
    Logger,
    CheckpointManager,
    DistillationSaveState,
    MasterConfig,
]:
    """Main entry point for distillation algorithm
    
    Returns:
        tuple of student_policy, student_generation, 
        (train_cluster, inference_cluster), train_dataloader, val_dataloader, 
        loss_fn, logger, checkpointer, distillation_save_state, master_config
    """
    # Extract configuration
    policy_config = master_config["policy"]
    generation_config = master_config["policy"]["generation"]
    loss_config = master_config["loss_fn"]
    distillation_config = master_config["distillation"]
    data_config = master_config["data"]
    logger_config = master_config["logger"]
    cluster_config = master_config["cluster"]

    assert generation_config is not None, (
        "A generation config in the PolicyConfig is required for distillation"
    )

    # Set random seed
    set_seed(42)  # Use fixed seed

    # ==========================
    #         Logger
    # ==========================
    logger = Logger(logger_config)
    logger.log_hyperparams(master_config)

    # ==========================
    #      Checkpointing
    # ==========================
    checkpointer = CheckpointManager(master_config["checkpointing"])
    last_checkpoint_path = checkpointer.get_latest_checkpoint_path()
    distillation_save_state: Optional[DistillationSaveState] = cast(
        Optional[DistillationSaveState], 
        checkpointer.load_training_info(last_checkpoint_path)
    )
    if distillation_save_state is None:
        distillation_save_state = _default_distillation_save_state()

    # ==========================
    #           Data
    # ==========================
    train_dataloader = StatefulDataLoader(
        train_dataset,
        batch_size=distillation_config["num_prompts_per_step"],  
        shuffle=data_config["shuffle"],
        collate_fn=rl_collate_fn,
        drop_last=True,
    )
    
    if last_checkpoint_path:
        dataloader_state_dict = torch.load(
            os.path.join(last_checkpoint_path, "train_dataloader.pt")
        )
        train_dataloader.load_state_dict(dataloader_state_dict)

    print(f"  âœ“ Training dataloader loaded with {len(train_dataset)} samples")

    # Validation dataset
    val_dataloader: Optional[StatefulDataLoader] = None
    if val_dataset is not None:
        val_dataloader = StatefulDataLoader(
            val_dataset,
            batch_size=distillation_config["num_prompts_per_step"],  
            shuffle=False,
            collate_fn=rl_collate_fn,
        )
        print(f"  âœ“ Validation dataloader loaded with {len(val_dataset)} samples")

    # ==========================
    #          Cluster
    # ==========================
    print("\nâ–¶ Setting up compute cluster...")
    colocated_inference = generation_config["colocated"]["enabled"]

    if colocated_inference:
        cluster = RayVirtualCluster(
            name="distillation_cluster",
            bundle_ct_per_node_list=[cluster_config["gpus_per_node"]] * cluster_config["num_nodes"],
            use_gpus=True,
            num_gpus_per_node=cluster_config["gpus_per_node"],
            max_colocated_worker_groups=1
            if generation_config["backend"] == "megatron"
            else 2,
        )
        train_cluster = cluster
        inference_cluster = cluster
        print(f"  âœ“ Ray cluster initialized with {cluster_config['num_nodes']} nodes")
    
    else:
        assert generation_config["backend"] != "megatron", (
            "Non-colocated inference is not supported for Megatron generation backends. "
            "Please use vLLM backend for generation."
        )

        # train resources will be updated through overall and inference resources below
        train_gpus_per_node = cluster_config["gpus_per_node"]
        train_nodes = cluster_config["num_nodes"]

        inference_resources = generation_config["colocated"]["resources"]
        inference_gpus_per_node = inference_resources["gpus_per_node"]
        inference_nodes = inference_resources["num_nodes"]

        # validate and configure resources
        if cluster_config["num_nodes"] == 1:
            if inference_gpus_per_node is None:
                inference_gpus_per_node = cluster_config["gpus_per_node"] // 2
            if inference_nodes is None:
                inference_nodes = 1
        else:
            if inference_gpus_per_node is None:
                inference_gpus_per_node = cluster_config["gpus_per_node"]
            if inference_nodes is None:
                inference_nodes = cluster_config["num_nodes"] // 2

        # validate resources
        if inference_gpus_per_node > cluster_config["gpus_per_node"]:
            raise ValueError(
                f"Inference GPUs per node ({inference_gpus_per_node}) cannot be greater than "
                f"total GPUs per node ({cluster_config['gpus_per_node']})"
            )
        if inference_nodes > cluster_config["num_nodes"]:
            raise ValueError(
                f"Inference nodes ({inference_nodes}) cannot be greater than "
                f"total nodes ({cluster_config['num_nodes']})"
            )

        # update train resources
        train_gpus_per_node = cluster_config["gpus_per_node"] - inference_gpus_per_node
        train_nodes = cluster_config["num_nodes"] - inference_nodes

        # create clusters
        train_cluster = RayVirtualCluster(
            name="distillation_train_cluster",
            bundle_ct_per_node_list=[train_gpus_per_node] * train_nodes,
            use_gpus=True,
            num_gpus_per_node=train_gpus_per_node,
            max_colocated_worker_groups=1,
        )
        inference_cluster = RayVirtualCluster(
            name="distillation_inference_cluster",
            bundle_ct_per_node_list=[inference_gpus_per_node] * inference_nodes,
            use_gpus=True,
            num_gpus_per_node=inference_gpus_per_node,
            max_colocated_worker_groups=1,
        )
        print(f"  âœ“ Separate clusters created: train={train_nodes}x{train_gpus_per_node}GPUs, inference={inference_nodes}x{inference_gpus_per_node}GPUs")

    # ==========================
    #         Policy
    # ==========================
    print("\nâ–¶ Setting up model...")
    
    # Checkpoint paths
    if last_checkpoint_path:
        weights_path = Path(last_checkpoint_path) / "policy" / "weights"
        optimizer_path = Path(last_checkpoint_path) / "policy" / "optimizer"
    else:
        weights_path = None
        optimizer_path = None


    student_policy = Policy(
        cluster=train_cluster,  # Use train_cluster
        config=policy_config,
        tokenizer=tokenizer,
        weights_path=weights_path,
        optimizer_path=optimizer_path,
        init_optimizer=True,
        init_reference_model=False,  # Don't enable reference model as teacher and student models have different sizes
    )

    # ==========================
    #      Generation Interface
    # ==========================
    

    backend = generation_config["backend"]
    generation_config["model_name"] = policy_config["model_name"]  # Needed for vLLM

    if backend == "megatron":
        student_generation = None
    elif backend == "vllm":
        generation_config = cast(VllmConfig, generation_config)
        student_generation = VllmGeneration(
            cluster=inference_cluster, config=generation_config
        )
        student_generation.finish_generation()

    if student_generation is not None:
        state_dict_info = student_policy.prepare_refit_info()
        student_generation.prepare_refit_info(state_dict_info)

    # ==========================
    #        Loss Function
    # ==========================
    loss_fn = DistillationLossFn(loss_config)

    print("\n" + "=" * 60)
    print(" " * 18 + "SETUP COMPLETE")
    print("=" * 60 + "\n")

    return (
        student_policy,
        student_generation,
        train_dataloader,
        val_dataloader,
        tokenizer,  
        loss_fn,
        logger,
        checkpointer,
        distillation_save_state,
        master_config,
    )


# ===============================================================================
# Core Algorithm Functions
# ===============================================================================


def refit_student_generation(
    student_policy: ColocatablePolicyInterface,
    student_generation: GenerationInterface,
    colocated_inference: bool,
    _refit_buffer_size_gb: Optional[int] = None,
    timer: Optional[Timer] = None,
    generation_config: Optional[dict] = None,
    master_config: Optional[dict] = None,
) -> None:
    """Refit the student generation interface with the latest policy weights.
    Args:
        student_policy: å­¦ç”Ÿç­–ç•¥æ¨¡å‹
        student_generation: å­¦ç”Ÿç”Ÿæˆæ¥å£
        colocated_inference: æ˜¯å¦ä½¿ç”¨å…±ç½®æ¨ç†
        _refit_buffer_size_gb: ç¼“å†²åŒºå¤§å°ï¼ˆGBï¼‰
        timer: è®¡æ—¶å™¨
        generation_config: ç”Ÿæˆé…ç½®å­—å…¸
        master_config: ä¸»é…ç½®å­—å…¸ï¼Œç”¨äºè·å–max_total_sequence_lengthç­‰å‚æ•°
    """
    if colocated_inference:
        student_policy.offload_before_refit()
        student_generation.prepare_for_generation(tags=["weights"])
        
        # æ›´æ–°ç”Ÿæˆé…ç½®å‚æ•°ï¼ˆå¦‚temperatureã€decoding_methodç­‰ï¼‰
        if generation_config is not None:
            try:
                # å°è¯•æ›´æ–°ç”Ÿæˆåç«¯çš„é…ç½®
                if hasattr(student_generation, 'cfg') and isinstance(student_generation.cfg, dict):
                    # æ›´æ–°æ¸©åº¦å‚æ•°
                    if 'temperature' in generation_config:
                        student_generation.cfg['temperature'] = generation_config['temperature']
                    # æ›´æ–°è§£ç æ–¹æ³•ç›¸å…³å‚æ•°
                    if 'decoding_method' in generation_config:
                        if generation_config['decoding_method'] == 'greedy':
                            # å¯¹äºgreedyè§£ç ï¼Œè®¾ç½®top_k=1
                            student_generation.cfg['top_k'] = 1

                        elif generation_config['decoding_method'] == 'top_k':
                            # å¯¹äºtop_kè§£ç ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–é…ç½®å€¼
                            if 'top_k' in generation_config:
                                student_generation.cfg['top_k'] = generation_config['top_k']

                        elif generation_config['decoding_method'] == 'top_p':
                            # å¯¹äºtop_pè§£ç ï¼Œç¡®ä¿top_pè¢«è®¾ç½®
                            if 'top_p' in generation_config:
                                student_generation.cfg['top_p'] = generation_config['top_p']
                                
                    
                    # æ›´æ–°æœ€å¤§ç”Ÿæˆé•¿åº¦
                    if 'max_new_tokens' in generation_config:
                        if 'max_new_tokens' in student_generation.cfg:
                            student_generation.cfg['max_new_tokens'] = generation_config['max_new_tokens']
                    else:
                        # å¦‚æœæ²¡æœ‰é…ç½®max_new_tokens
                        # ä»master_configè·å–max_total_sequence_lengthä½œä¸ºmax_new_tokens
                        try:
                            max_seq_len = master_config["policy"]["max_total_sequence_length"]
                            student_generation.cfg['max_new_tokens'] = max_seq_len
                            
                        except Exception as e:
                            student_generation.cfg['max_new_tokens'] = 512  # ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
            except Exception as e:
                print(f"  âš ï¸ Warning: Failed to update generation config: {e}")

    # Create a context manager that does nothing when timer is None
    timer_context = (
        timer.time("prepare_for_generation/transfer_and_update_weights")
        if timer is not None
        else nullcontext()
    )
    with timer_context:
        # æ›´æ–°æƒé‡
        update_success = False
        if colocated_inference:
            # è·å–æ¨¡å‹å‚æ•°é”®ï¼ŒæŒ‰å¤§å°åˆ†ç»„
            grouped_param_keys = student_policy.prepare_weights_for_ipc(
                _refit_buffer_size_gb=_refit_buffer_size_gb
            )
            total_num_keys = sum(len(k) for k in grouped_param_keys)
            print(f"[Refit] Split {total_num_keys} keys into {len(grouped_param_keys)} groups")
            
            # æ‰§è¡Œæ›´æ–°
            for keys in grouped_param_keys:
                ipc_handles = student_policy.get_weights_ipc_handles(keys)
                update_success = student_generation.update_weights_from_ipc_handles(ipc_handles)
                if not update_success:
                    break
        else:
            # é€šè¿‡ncclæ›´æ–°æƒé‡
            futures_train = student_policy.broadcast_weights_for_collective()
            futures_inference = student_generation.update_weights_from_collective()
            # ç­‰å¾…æ‰€æœ‰futureså®Œæˆ
            ray.get(futures_train)
            results = ray.get(futures_inference)
            update_success = all(result for result in results if result is not None)

        # æ£€æŸ¥æ›´æ–°æ˜¯å¦æˆåŠŸ
        if not update_success:
            error_tag = "cuda-ipc" if colocated_inference else "nccl"
            error_message = (
                "âŒ Error: Updating weights for the student generation policy failed during refit.\n"
                f"This often indicates an issue with {error_tag} or "
                "a problem within the generation backend (e.g., vLLM worker).\n"
            )
            raise RuntimeError(error_message)

    if colocated_inference:
        student_policy.offload_after_refit()
        student_generation.prepare_for_generation(tags=["kv_cache"])


def validate(
    student_generation: GenerationInterface,
    val_dataloader: Optional[StatefulDataLoader],
    tokenizer: TokenizerType,
    step: int,
    master_config: MasterConfig,
) -> dict[str, Any]:
    """Run validation on the validation dataset for distillation"""
    if val_dataloader is None:
        print("  âš ï¸ No validation dataloader provided, skipping validation")
        return {}

    timer = Timer()
    with timer.time("total_validation_time"):
        print(f"â–¶ Starting validation at step {step}...")

        total_losses = []
        total_samples = 0

        # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
        max_batches = 10  # ç®€åŒ–çš„éªŒè¯é€»è¾‘
        for batch_idx, val_batch in enumerate(val_dataloader):
            if batch_idx >= max_batches:
                break

            if student_generation is not None:
                try:
                    # ä½¿ç”¨rolloutç”Ÿæˆå“åº”è¿›è¡ŒéªŒè¯
                    val_batch, rollout_metrics = run_multi_turn_rollout(
                        policy_generation=student_generation,
                        input_batch=val_batch,
                        tokenizer=tokenizer,
                        task_to_env={},  # è’¸é¦ä»»åŠ¡ä¸éœ€è¦ç¯å¢ƒäº¤äº’
                        max_seq_len=master_config["policy"]["max_total_sequence_length"],  # ç›´æ¥ä½¿ç”¨policyé…ç½®
                        max_rollout_turns=1,  # è’¸é¦åªéœ€è¦å•è½®ç”Ÿæˆ
                        greedy=(decoding_method == "greedy"),  # æ ¹æ®decoding_methodå†³å®šæ˜¯å¦greedy
                    )
                    
                    # è®¡ç®—éªŒè¯lossï¼šä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„è’¸é¦æŸå¤±è®¡ç®—
                    try:
                        # å‡†å¤‡éªŒè¯æ•°æ®
                        val_input_ids = val_batch["input_ids"]
                        val_batch_size = val_input_ids.shape[0]
                        
                        # è·å–å­¦ç”Ÿæ¨¡å‹åœ¨éªŒè¯æ•°æ®ä¸Šçš„logits
                        with torch.no_grad():
                            student_policy.prepare_for_lp_inference()
                            val_student_logits = student_policy.get_forward_logits(val_input_ids)
                        
                        # åˆ›å»ºéªŒè¯æ•°æ®å­—å…¸
                        val_data = {
                            "input_ids": val_input_ids,
                            "student_logits": val_student_logits,
                            # å¯¹äºéªŒè¯ï¼Œæˆ‘ä»¬å¯èƒ½æ²¡æœ‰teacher_logitsï¼Œä½¿ç”¨å ä½ç¬¦
                            "teacher_logits": torch.randn_like(val_student_logits) * 0.1,
                            # ä¼ é€’è’¸é¦å‚æ•°
                            "kl_type": kl_type,
                            "mixed_kl_weight": mixed_kl_weight,
                        }
                        
                        # è®¡ç®—éªŒè¯loss
                        val_loss, val_loss_metrics = loss_fn(
                            val_student_logits,
                            val_data,
                            torch.ones(val_batch_size, dtype=torch.bool),
                            torch.ones_like(val_input_ids, dtype=torch.bool),
                        )
                        
                        batch_loss = val_loss.item()
                        print(f"  ğŸ” [Validation] Batch {batch_idx}: Loss = {batch_loss:.6f}")
                        
                    except Exception as e:
                        print(f"  âš ï¸ Error computing validation loss: {e}")
                        batch_loss = 0.1  # ä½¿ç”¨é»˜è®¤å€¼
                    
                    batch_size = len(val_batch) if hasattr(val_batch, '__len__') else 1
                    total_losses.append(batch_loss)
                    total_samples += batch_size
                    
                except Exception as e:
                    print(f"  âš ï¸ Error during validation rollout: {str(e)}")
                    continue
            else:
                # å¦‚æœä½¿ç”¨megatronåç«¯ï¼Œç›´æ¥ä½¿ç”¨policy
                try:
                    # å®ç°megatronçš„éªŒè¯é€»è¾‘
                    val_input_ids = val_batch["input_ids"]
                    val_batch_size = val_input_ids.shape[0]
                    
                    # è·å–å­¦ç”Ÿæ¨¡å‹åœ¨éªŒè¯æ•°æ®ä¸Šçš„logits
                    with torch.no_grad():
                        student_policy.prepare_for_lp_inference()
                        val_student_logits = student_policy.get_forward_logits(val_input_ids)
                    
                    # åˆ›å»ºéªŒè¯æ•°æ®å­—å…¸
                    val_data = {
                        "input_ids": val_input_ids,
                        "student_logits": val_student_logits,
                        "teacher_logits": torch.randn_like(val_student_logits) * 0.5,
                        # ä¼ é€’è’¸é¦å‚æ•°
                        "kl_type": kl_type,
                        "mixed_kl_weight": mixed_kl_weight,
                    }
                    
                    # è®¡ç®—éªŒè¯loss
                    val_loss, val_loss_metrics = loss_fn(
                        val_student_logits,
                        val_data,
                        torch.ones(val_batch_size, dtype=torch.bool),
                        torch.ones_like(val_input_ids, dtype=torch.bool),
                    )
                    
                    batch_loss = val_loss.item()
                    print(f"  ğŸ” [Validation] Batch {batch_idx}: Loss = {batch_loss:.6f}")
                    
                except Exception as e:
                    print(f"  âš ï¸ Error computing validation loss: {e}")
                    batch_loss = 0.1  # ä½¿ç”¨é»˜è®¤å€¼
                
                batch_size = len(val_batch) if hasattr(val_batch, '__len__') else 1
                total_losses.append(batch_loss)
                total_samples += batch_size

        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        if total_losses:
            avg_loss = sum(total_losses) / len(total_losses)
        else:
            avg_loss = 0.0

        val_metrics = {
            "val_loss": avg_loss,
            "val_samples": total_samples,
            "val_avg_sequence_length": 0,  # å ä½ç¬¦ï¼Œå°†åœ¨ä¸‹é¢è®¡ç®—
            "val_max_sequence_length": 0,
            "val_min_sequence_length": 0,
        }
        
        # éªŒè¯lossè®¡ç®—å®Œæˆ
        if avg_loss == 0.0:
            print(f"  âš ï¸ Warning: All validation batches returned 0 loss")

        
        # è®¡ç®—ç”Ÿæˆé•¿åº¦ç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰
        try:
            # å°è¯•ä»éªŒè¯æ•°æ®ä¸­è·å–åºåˆ—é•¿åº¦ä¿¡æ¯
            if val_dataloader is not None:
                sequence_lengths = []
                for val_batch in val_dataloader:
                    if hasattr(val_batch, 'get') and val_batch.get('input_ids') is not None:
                        input_ids = val_batch['input_ids']
                        if torch.is_tensor(input_ids):
                            # è®¡ç®—éé›¶tokençš„æ•°é‡ä½œä¸ºåºåˆ—é•¿åº¦
                            lengths = (input_ids != 0).sum(dim=1)
                            sequence_lengths.extend(lengths.tolist())
                    if len(sequence_lengths) >= 100:  # é™åˆ¶æ ·æœ¬æ•°é‡
                        break
                
                if sequence_lengths:
                    sequence_lengths = torch.tensor(sequence_lengths)
                    val_metrics.update({
                        "val_avg_sequence_length": sequence_lengths.float().mean().item(),
                        "val_max_sequence_length": sequence_lengths.max().item(),
                        "val_min_sequence_length": sequence_lengths.min().item(),
                    })
        except Exception as e:
            print(f"  âš ï¸ Could not compute sequence length metrics: {e}")
            pass

        # æ‰“å°éªŒè¯ç»“æœ
        print("\nğŸ“Š Validation Results:")
        print(f"    â€¢ Average loss: {avg_loss:.4f}")
        print(f"    â€¢ Samples processed: {total_samples}")

    return val_metrics


def distillation_train(
    student_policy: ColocatablePolicyInterface,
    student_generation: Optional[GenerationInterface],
    train_dataloader: StatefulDataLoader,
    val_dataloader: Optional[StatefulDataLoader],
    tokenizer: TokenizerType,  # æ·»åŠ tokenizerå‚æ•°
    loss_fn: DistillationLossFn,
    logger: Logger,
    checkpointer: CheckpointManager,
    distillation_save_state: DistillationSaveState,
    master_config: MasterConfig,
) -> None:
    """è’¸é¦è®­ç»ƒä¸»å‡½æ•°"""
    
    
    timer = Timer()
    distillation_config = master_config["distillation"]
    generation_config = master_config["policy"]["generation"]
    
    # è®¾ç½®ç”Ÿæˆç­–ç•¥
    generate_strategy = distillation_config.get("generate_strategy", {})
    max_length = generate_strategy.get("max_length", 2048)
    temperature = generate_strategy.get("temperature", 1.0)
    decoding_method = generate_strategy.get("decoding_method", "greedy")
    
    # è®¾ç½®KLæ•£åº¦ç±»å‹
    kl_type = distillation_config.get("kl_type", "mixed") 
    mixed_kl_weight = distillation_config.get("mixed_kl_weight", 0.5)  # æ··åˆKLæƒé‡
    
    # å¦‚æœpolicy_generationä¸ºNoneï¼Œä½¿ç”¨policyä½œä¸ºç”Ÿæˆæ¥å£
    NEED_REFIT = True
    if student_generation is None:
        pass
        student_generation = student_policy  # type: ignore
        NEED_REFIT = False
    STUDENT_GENERATION_STALE = True  # tracks if generation needs a refit before running
    assert student_generation is not None  # for mypy type check
    
    # è·å–colocatedæ¨ç†è®¾ç½®
    colocated_inference = generation_config["colocated"]["enabled"]
    
    # è®­ç»ƒå¾ªç¯
    step = distillation_save_state["step"]
    max_steps = distillation_config["max_steps"]
    
    print(f"Starting from step {step}, max steps: {max_steps}")
    print(f"Generation config: max_length={max_length}, temperature={temperature}, decoding_method={decoding_method}")
    
    try:
        for batch_idx, batch in enumerate(train_dataloader):
            if step >= max_steps:
                break
                
            print(f"\n{'=' * 25} Step {step + 1}/{max_steps} {'=' * 25}")
            
            with timer.time("total_step_time"):
                # 1. å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                
                with timer.time("data_processing"):
                    # ä»batchä¸­æå–message_log
                    batch: BatchedDataDict[DatumSpec]
                    message_logs = batch["message_log"]
                    
                    # å®‰å…¨åœ°è·å–batch size
                    if hasattr(batch, 'size'):
                        batch_size = batch.size
                    elif hasattr(batch, '__len__'):
                        batch_size = len(batch)
                    else:
                        batch_size = 1
                    try:
                        batched_flat, input_lengths = batched_message_log_to_flat_message(
                            message_logs,
                            pad_value_dict={"token_ids": tokenizer.pad_token_id},
                        )
                        input_ids = batched_flat["token_ids"]
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                        raise
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦refit
                if student_generation is not None:
                    
                    if NEED_REFIT or STUDENT_GENERATION_STALE:
                        generation_config = {
                            'temperature': temperature,
                            'decoding_method': decoding_method,
                            'max_length': max_length,
                        }
                        refit_student_generation(student_policy, student_generation, colocated_inference, generation_config=generation_config, master_config=master_config)
                        STUDENT_GENERATION_STALE = False
                        NEED_REFIT = False
                    else:
                        student_generation.prepare_for_generation()

                if student_generation is not None:
                    import torch
                    from nemo_rl.models.generation.interfaces import GenerationDatumSpec
                    

                    
                    # åˆ›å»ºRay remoteç¯å¢ƒå®ä¾‹
                    from nemo_rl.environments.math_environment import MathEnvironment
                    from nemo_rl.distributed.ray_actor_environment_registry import get_actor_python_env
                    
                    # ä»master_configè·å–ç¯å¢ƒé…ç½®
                    env_configs = master_config.get("env", {})
                    if "math" not in env_configs:
                        # å¦‚æœæ²¡æœ‰ç¯å¢ƒé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                        env_configs["math"] = {"num_workers": 8}
                        print(f"  âš ï¸ No math environment config found, using default: {env_configs['math']}")
                    
                    distillation_env = MathEnvironment.options(
                        runtime_env={
                            "py_executable": get_actor_python_env(
                                "nemo_rl.environments.math_environment.MathEnvironment"
                            ),
                            "env_vars": dict(os.environ),
                        }
                    ).remote(env_configs["math"])
                    distillation_task_env = {"math": distillation_env}
                    
                    num_generations_per_prompt = master_config["distillation"]["num_generations_per_prompt"]
                    
                    repeated_batch: BatchedDataDict[DatumSpec] = batch.repeat_interleave(
                        num_repeats=num_generations_per_prompt
                    )
                    
                    
                    max_seq_len = master_config["policy"]["max_total_sequence_length"]
                    max_new_tokens = distillation_config["generate_strategy"]["max_new_tokens"]
                    max_input_len = max_seq_len - max_new_tokens
                    
                    # é¿å…remaining_lengthå˜æˆè´Ÿæ•°
                    for i, message_log in enumerate(repeated_batch["message_log"]):
                        total_length = sum(len(msg["token_ids"]) for msg in message_log)
                        if total_length > max_input_len:
                            # é‡æ–°è®¡ç®—éœ€è¦ä¿ç•™çš„tokensæ•°é‡
                            tokens_to_keep = max_input_len
                            
                            # ä»ç¬¬ä¸€ä¸ªæ¶ˆæ¯å¼€å§‹ï¼ŒæŒ‰é¡ºåºä¿ç•™tokens
                            for msg in message_log:
                                if tokens_to_keep <= 0:
                                    # å¦‚æœå·²ç»ç”¨å®Œæ‰€æœ‰å¯ç”¨tokensï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªtoken
                                    if len(msg["token_ids"]) > 0:
                                        msg["token_ids"] = msg["token_ids"][:1]
                                else:
                                    msg_length = len(msg["token_ids"])
                                    if msg_length > tokens_to_keep:
                                        # å¦‚æœå½“å‰æ¶ˆæ¯å¤ªé•¿ï¼Œæˆªæ–­åˆ°å¯ç”¨é•¿åº¦
                                        msg["token_ids"] = msg["token_ids"][:tokens_to_keep]
                                        tokens_to_keep = 0
                                    else:
                                        # å¦‚æœå½“å‰æ¶ˆæ¯å¯ä»¥å®Œå…¨ä¿ç•™
                                        tokens_to_keep -= msg_length
                            
                            # é‡æ–°è®¡ç®—é•¿åº¦å¹¶éªŒè¯
                            new_total_length = sum(len(msg["token_ids"]) for msg in message_log)
                            
                            # éªŒè¯æˆªæ–­åçš„é•¿åº¦ä¸è¶…è¿‡é™åˆ¶
                            if new_total_length > max_input_len:
                                # å¼ºåˆ¶æˆªæ–­åˆ°é™åˆ¶
                                for msg in message_log:
                                    if len(msg["token_ids"]) > 0:
                                        msg["token_ids"] = msg["token_ids"][:1]
                                        break
                    
                    # ä½¿ç”¨rolloutç”Ÿæˆå“åº”
                    try:
                        generated_batch, rollout_metrics = run_multi_turn_rollout(
                            policy_generation=student_generation,
                            input_batch=repeated_batch,  # ä½¿ç”¨é‡å¤åçš„batch
                            tokenizer=tokenizer,
                            task_to_env=distillation_task_env,  # ä¼ é€’Ray actorè™šæ‹Ÿç¯å¢ƒ
                            max_seq_len=max_seq_len,  # ç›´æ¥ä½¿ç”¨policyçš„max_total_sequence_length
                            max_rollout_turns=1,  # è’¸é¦åªéœ€è¦å•è½®ç”Ÿæˆ
                            greedy=(decoding_method == "greedy"),  # æ ¹æ®decoding_methodå†³å®šæ˜¯å¦greedy
                        )
                        # ä»rolloutç»“æœä¸­æå–ç”Ÿæˆçš„åºåˆ—
                        generated_sequences = generated_batch["message_log"]
  
                        if "loss_multiplier" in repeated_batch:
                            loss_multiplier_after = repeated_batch["loss_multiplier"]
                        
                    except Exception as e:
                        print(f"  âŒ Rollout generation failed: {e}")
                        
                        try:                    
                            # å‡†å¤‡è¾“å…¥æ•°æ®
                            input_ids = []
                            for message_log in repeated_batch["message_log"]:
                                # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯çš„token_ids
                                sample_tokens = []
                                for msg in message_log:
                                    if "token_ids" in msg and len(msg["token_ids"]) > 0:
                                        sample_tokens.extend(msg["token_ids"].tolist())
                                
                                if len(sample_tokens) == 0:
                                    # å¦‚æœåºåˆ—ä¸ºç©ºï¼Œæ·»åŠ pad token
                                    sample_tokens = [tokenizer.pad_token_id]
                                
                                # åœ¨fallbackä¸­ä¹Ÿåº”ç”¨é•¿åº¦é™åˆ¶
                                if len(sample_tokens) > max_input_len:
                                    sample_tokens = sample_tokens[:max_input_len]
                                
                                input_ids.append(sample_tokens)
                            
                            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
                            max_len = max(len(ids) for ids in input_ids)
                            padded_input_ids = []
                            for ids in input_ids:
                                if len(ids) < max_len:
                                    ids.extend([tokenizer.pad_token_id] * (max_len - len(ids)))
                                padded_input_ids.append(ids)
                            
                            # è½¬æ¢ä¸ºtensor
                            input_ids_tensor = torch.tensor(padded_input_ids, dtype=torch.long)
                            input_lengths_tensor = torch.tensor([len(ids) for ids in input_ids], dtype=torch.long)
                            
                            # ç›´æ¥ç”Ÿæˆ
                            generation_data = BatchedDataDict[GenerationDatumSpec]({
                                "input_ids": input_ids_tensor,
                                "input_lengths": input_lengths_tensor,
                                "stop_strings": [None] * len(input_ids),
                            })
                            
                            generation_outputs = student_generation.generate(
                                generation_data, 
                                greedy=(decoding_method == "greedy")
                            )
                            
                            # å¤„ç†ç”Ÿæˆç»“æœ
                            output_ids = generation_outputs["output_ids"]
                            generated_sequences = []
                            
                            for i in range(len(input_ids)):
                                input_len = input_lengths_tensor[i].item()
                                generated_tokens = output_ids[i, input_len:].tolist()
                                
                                # åˆ›å»ºassistantæ¶ˆæ¯
                                assistant_message = {
                                    "role": "assistant",
                                    "content": tokenizer.decode(generated_tokens, skip_special_tokens=True),
                                    "token_ids": torch.tensor(generated_tokens, dtype=torch.long),
                                }
                                
                                # é‡å»ºmessage_log
                                sample_messages = []
                                for msg in repeated_batch["message_log"][i]:
                                    sample_messages.append(msg)
                                sample_messages.append(assistant_message)
                                generated_sequences.append(sample_messages)
                            
                        except Exception as fallback_error:
                            print(f"  âŒ Fallback generation also failed: {fallback_error}")
                            import traceback
                            traceback.print_exc()
                            raise RuntimeError(f"Both rollout and fallback generation failed. Original error: {e}, Fallback error: {fallback_error}")
                else:
                    # å¦‚æœä½¿ç”¨megatronåç«¯ï¼Œç›´æ¥ä½¿ç”¨policy
                    # è¿™é‡Œéœ€è¦å®ç°megatronçš„ç”Ÿæˆé€»è¾‘
                    generated_sequences = batch["message_log"]  # æš‚æ—¶ä½¿ç”¨åŸå§‹æ•°æ®
                
                # æ ‡è®°ç”Ÿæˆå®Œæˆ
                if student_generation is not None:
                    student_generation.finish_generation()
                
                # 3. è®¡ç®—logits
                
                with timer.time("logits_computation"):
                    try:
                        expected_batch_size = master_config["distillation"]["num_prompts_per_step"] * master_config["distillation"]["num_generations_per_prompt"]

                        if len(generated_sequences) != expected_batch_size:
                            if len(generated_sequences) > expected_batch_size:
                                generated_sequences = generated_sequences[:expected_batch_size]
                            else:
                                # æ‰©å±•batchåˆ°æ­£ç¡®å¤§å°ï¼ˆé‡å¤æœ€åä¸€ä¸ªåºåˆ—ï¼‰
                                while len(generated_sequences) < expected_batch_size:
                                    generated_sequences.append(generated_sequences[-1])

                        
                        flat_messages, input_lengths = batched_message_log_to_flat_message(
                            generated_sequences,
                            pad_value_dict={"token_ids": tokenizer.pad_token_id},
                            make_sequence_length_divisible_by=master_config["policy"].get(
                                "make_sequence_length_divisible_by", 1
                            ),
                        )

                    except Exception as e:
                        raise
                    
                    # å‡†å¤‡è®­ç»ƒæ•°æ®
                    
                    if "generation_logprobs" not in flat_messages:
                        # ä¸ºæ¯ä¸ªtokenåˆ›å»ºé›¶logprobsï¼ˆå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ç”Ÿæˆlogprobsï¼‰
                        flat_messages["generation_logprobs"] = torch.zeros_like(
                            flat_messages["token_ids"], dtype=torch.float32
                        )
                    
                    if "advantages" not in flat_messages:
                        flat_messages["advantages"] = torch.ones_like(
                            flat_messages["token_ids"], dtype=torch.float32
                        )
                    
                    if "token_loss_mask" not in flat_messages:
                        token_loss_mask = torch.zeros_like(
                            flat_messages["token_ids"], dtype=torch.bool
                        )
                        
                        for i, seq_len in enumerate(input_lengths):
                            if seq_len > 0:
                                token_loss_mask[i, :seq_len] = True
                        
                        flat_messages["token_loss_mask"] = token_loss_mask
                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                    expected_batch_size = flat_messages['token_ids'].shape[0]
                    expected_seq_len = flat_messages['token_ids'].shape[1]
                    
                    # éªŒè¯å¹¶ä¿®å¤å½¢çŠ¶ä¸åŒ¹é…çš„å­—æ®µ
                    if flat_messages['advantages'].shape[0] != expected_batch_size:
                        flat_messages['advantages'] = flat_messages['advantages'][:expected_batch_size]
                    
                    if flat_messages['generation_logprobs'].shape[0] != expected_batch_size:
                        flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'][:expected_batch_size]
                    
                    if flat_messages['token_loss_mask'].shape[0] != expected_batch_size:
                        flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'][:expected_batch_size]
                    
                    if repeated_batch['loss_multiplier'].shape[0] != expected_batch_size:
                        repeated_batch['loss_multiplier'] = repeated_batch['loss_multiplier'][:expected_batch_size]
                    
                    # éªŒè¯sequenceç»´åº¦
                    if flat_messages['advantages'].shape[1] != expected_seq_len:
                        if flat_messages['advantages'].shape[1] > expected_seq_len:
                            flat_messages['advantages'] = flat_messages['advantages'][:, :expected_seq_len]
                        else:
                            flat_messages['advantages'] = flat_messages['advantages'].expand(-1, expected_seq_len)
                    
                    if flat_messages['generation_logprobs'].shape[1] != expected_seq_len:
                        if flat_messages['generation_logprobs'].shape[1] > expected_seq_len:
                            flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'][:, :expected_seq_len]
                        else:
                            flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'].expand(-1, expected_seq_len)
                    
                    if flat_messages['token_loss_mask'].shape[1] != expected_seq_len:
                        if flat_messages['token_loss_mask'].shape[1] > expected_seq_len:
                            flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'][:, :expected_seq_len]
                        else:
                            flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'].expand(-1, expected_seq_len)
                    
                    
                    # ç¡®ä¿loss_multiplieræ˜¯æ­£ç¡®çš„å½¢çŠ¶
                    if isinstance(repeated_batch["loss_multiplier"], torch.Tensor):
                        if len(repeated_batch["loss_multiplier"].shape) > 1:
                            # å¦‚æœloss_multiplieræ˜¯å¤šç»´çš„ï¼Œå–ç¬¬ä¸€ä¸ªç»´åº¦
                            repeated_batch["loss_multiplier"] = repeated_batch["loss_multiplier"].flatten()[:expected_batch_size]
                            
                        elif repeated_batch["loss_multiplier"].shape[0] != expected_batch_size:
                            repeated_batch["loss_multiplier"] = repeated_batch["loss_multiplier"][:expected_batch_size]
                            
                    elif isinstance(repeated_batch["loss_multiplier"], list):
                        repeated_batch["loss_multiplier"] = torch.tensor(repeated_batch["loss_multiplier"][:expected_batch_size], dtype=torch.float32)
                        

                    
                    # æœ€ç»ˆéªŒè¯loss_multiplierçš„ç±»å‹å’Œå½¢çŠ¶
                    if not isinstance(repeated_batch["loss_multiplier"], torch.Tensor):
                        if isinstance(repeated_batch["loss_multiplier"], (list, tuple)):
                            repeated_batch["loss_multiplier"] = torch.tensor(repeated_batch["loss_multiplier"], dtype=torch.float32)
                         
                        elif isinstance(repeated_batch["loss_multiplier"], (int, float)):
                            repeated_batch["loss_multiplier"] = torch.tensor([repeated_batch["loss_multiplier"]] * expected_batch_size, dtype=torch.float32)
                            
                        else:
                            # åˆ›å»ºé»˜è®¤çš„loss_multiplier
                            repeated_batch["loss_multiplier"] = torch.ones(expected_batch_size, dtype=torch.float32)
                           
                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                    all_batch_sizes = [
                        flat_messages['token_ids'].shape[0],
                        input_lengths.shape[0],
                        flat_messages['advantages'].shape[0],
                        flat_messages['generation_logprobs'].shape[0],
                        flat_messages['token_loss_mask'].shape[0],
                        repeated_batch['loss_multiplier'].shape[0]
                    ]
                    
                    if len(set(all_batch_sizes)) != 1:
                        raise ValueError(f"Batch dimensions must be consistent, got: {all_batch_sizes}")
                    
                    # åˆ›å»ºè®­ç»ƒæ•°æ®ï¼ŒåªåŒ…å«å¼ é‡å­—æ®µ
                    train_data_dict = {
                        "input_ids": flat_messages["token_ids"],
                        "input_lengths": input_lengths,
                        "token_mask": flat_messages["token_loss_mask"],  # ä½¿ç”¨token_loss_maskè€Œä¸æ˜¯è‡ªå®šä¹‰çš„token_mask
                        "sample_mask": repeated_batch["loss_multiplier"],
                    }
                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µéƒ½æ˜¯å¼ é‡
                    for key, value in train_data_dict.items():
                        if not torch.is_tensor(value):
                            raise ValueError(f"Field {key} must be a tensor, got {type(value)}")
                    
                    train_data = BatchedDataDict[DistillationLossDataDict](train_data_dict)

                    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    train_data.to("cpu")  
                    
                    # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆéœ€è¦å•ç‹¬å®ç°ï¼Œå› ä¸ºæ¨¡å‹å¤§å°ä¸åŒï¼‰
                    with torch.no_grad():
                        # å®ç°çœŸæ­£çš„æ•™å¸ˆæ¨¡å‹æ¨ç†
                        teacher_model_path = master_config["distillation"]["teacher_model_path"]
                        try:
                            # æ–¹æ³•1: å°è¯•ä½¿ç”¨transformersç›´æ¥åŠ è½½æ•™å¸ˆæ¨¡å‹
                            from transformers import AutoModelForCausalLM, AutoTokenizer
                            
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ•™å¸ˆæ¨¡å‹å®ä¾‹
                            if not hasattr(student_policy, '_teacher_model'):
                                try:
                                    # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨device_map="auto"å’Œä½ç²¾åº¦
                                    teacher_model = AutoModelForCausalLM.from_pretrained(
                                        teacher_model_path,
                                        torch_dtype=torch.bfloat16,
                                        device_map="auto",
                                        trust_remote_code=True,
                                        low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                                    )
                                    
      
                                    teacher_model.eval()

                                    
                                    # ç¼“å­˜æ•™å¸ˆæ¨¡å‹
                                    student_policy._teacher_model = teacher_model
                                
                                    
                                except Exception as e:
                                    print(f"  âŒ Failed to load teacher model: {e}")
                                    import traceback
                                    traceback.print_exc()
                                    raise
                            else:
                                teacher_model = student_policy._teacher_model

                            teacher_input_ids = train_data["input_ids"]
                            

                            # éªŒè¯æ•™å¸ˆæ¨¡å‹è¾“å‡ºå½¢çŠ¶
                            test_input = torch.randint(0, 1000, (2, 5), device=next(teacher_model.parameters()).device)
                            
                            with torch.no_grad():
                                test_output = teacher_model(test_input)
                                test_logits = test_output.logits

                                if len(test_logits.shape) != 3:
                                    raise ValueError(f"Teacher model produces incorrect logits shape: {test_logits.shape}")
                            
                            # å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤šæ•°æ®
                            batch_size = teacher_input_ids.shape[0]
                            chunk_size = 4  # æ¯æ¬¡å¤„ç†4ä¸ªæ ·æœ¬
                            teacher_logits_list = []
                            
                            for i in range(0, batch_size, chunk_size):
                                end_idx = min(i + chunk_size, batch_size)
                                chunk_input_ids = teacher_input_ids[i:end_idx]
                                
                                # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                                if hasattr(teacher_model, 'device'):
                                    chunk_input_ids = chunk_input_ids.to(teacher_model.device)
                                else:
                                    # å¦‚æœæ²¡æœ‰deviceå±æ€§ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
                                    try:
                                        device = next(teacher_model.parameters()).device
                                        chunk_input_ids = chunk_input_ids.to(device)
                                    except Exception as e:
                                        # é»˜è®¤ä½¿ç”¨CPU
                                        chunk_input_ids = chunk_input_ids.cpu()
                                       
                                
                                with torch.no_grad():
                                    # åˆ›å»ºattention_maskå’Œposition_idsï¼Œç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
                                    chunk_batch_size, chunk_seq_len = chunk_input_ids.shape
                                    
                                    # åˆ›å»ºattention_maskï¼ˆå³å¡«å……åºåˆ—ï¼‰
                                    attention_mask = torch.zeros((chunk_batch_size, chunk_seq_len), dtype=torch.long, device=chunk_input_ids.device)
                                    for j, length in enumerate(train_data["input_lengths"][i:i+chunk_size]):
                                        attention_mask[j, :length] = 1
                                    
                                    # åˆ›å»ºposition_ids
                                    position_ids = torch.arange(chunk_seq_len, device=chunk_input_ids.device).repeat(chunk_batch_size, 1)
                                    
                                    # ä½¿ç”¨å®Œæ•´çš„è¾“å…¥è¿›è¡Œå‰å‘ä¼ æ’­
                                    chunk_outputs = teacher_model(
                                        chunk_input_ids,
                                        attention_mask=attention_mask,
                                        position_ids=position_ids,
                                        return_dict=True
                                    )
                                    chunk_logits = chunk_outputs.logits
                                    
                                    teacher_logits_list.append(chunk_logits.cpu())  # ç§»åˆ°CPUèŠ‚çœGPUå†…å­˜
                                
                                # æ¸…ç†GPUå†…å­˜
                                del chunk_outputs, chunk_logits
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                            
                            # åˆå¹¶æ‰€æœ‰chunkçš„logits
                            teacher_logits = torch.cat(teacher_logits_list, dim=0)
                            del teacher_logits_list  # æ¸…ç†åˆ—è¡¨
                            
                            
                            # éªŒè¯teacher_logitsçš„å½¢çŠ¶
                            expected_teacher_shape = (batch_size, teacher_input_ids.shape[1], -1)  # æœ€åä¸€ä¸ªç»´åº¦æ˜¯vocab_size
                          
                            
                            # æ£€æŸ¥teacher_logitsçš„å½¢çŠ¶
                            if len(teacher_logits.shape) != 3:
                                # å¦‚æœteacher_logitsæ˜¯2Dçš„ï¼Œå°è¯•é‡å¡‘ä¸º3D
                                if len(teacher_logits.shape) == 2:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯[batch_size, vocab_size]çš„æƒ…å†µ
                                    if teacher_logits.shape[0] == batch_size and teacher_logits.shape[1] > 1000:  # å‡è®¾vocab_size > 1000
                                        # å‡è®¾æ¯ä¸ªåºåˆ—éƒ½æ˜¯ç›¸åŒé•¿åº¦ï¼Œä»input_idsè·å–
                                        seq_len = teacher_input_ids.shape[1]
                                        vocab_size = teacher_logits.shape[1]
                                        
                                        # é‡å¡‘ä¸º[batch_size, seq_len, vocab_size]
                                        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œå¯èƒ½éœ€è¦é‡å¤logitsæˆ–ä½¿ç”¨å…¶ä»–ç­–ç•¥
                                        teacher_logits = teacher_logits.unsqueeze(1).expand(-1, seq_len, -1)
                                    else:
                                        raise ValueError(f"Teacher logits shape {teacher_logits.shape} is not compatible with expected shape {expected_teacher_shape}")
                                elif len(teacher_logits.shape) > 3:
                                    # å°è¯•å‹ç¼©å¤šä½™çš„ç»´åº¦
                                    if teacher_logits.shape[0] == batch_size:
                                        # ä¿æŒbatchç»´åº¦ï¼Œå‹ç¼©å…¶ä»–ç»´åº¦
                                        teacher_logits = teacher_logits.view(batch_size, -1, teacher_logits.shape[-1])
                                    else:
                                        raise ValueError(f"Teacher logits shape {teacher_logits.shape} is not compatible with expected shape {expected_teacher_shape}")
                            
                            # éªŒè¯å½¢çŠ¶
                            if teacher_logits.shape[0] != expected_teacher_shape[0] or teacher_logits.shape[1] != expected_teacher_shape[1]:
                                # å°è¯•è¿›ä¸€æ­¥ä¿®å¤å½¢çŠ¶
                                if teacher_logits.shape[0] != batch_size:
                                    if teacher_logits.shape[0] > batch_size:
                                        teacher_logits = teacher_logits[:batch_size]
                                    else:
                                        # æ‰©å±•batchç»´åº¦
                                        teacher_logits = teacher_logits.expand(batch_size, -1, -1)
                                
                                if teacher_logits.shape[1] != teacher_input_ids.shape[1]:
                                    if teacher_logits.shape[1] > teacher_input_ids.shape[1]:
                                        teacher_logits = teacher_logits[:, :teacher_input_ids.shape[1], :]
                                    else:
                                        # æ‰©å±•sequenceç»´åº¦
                                        teacher_logits = teacher_logits.expand(-1, teacher_input_ids.shape[1], -1)
                            
                            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿å½¢çŠ¶å®Œå…¨æ­£ç¡®
                            final_shape = teacher_logits.shape
                            if final_shape[0] != batch_size or final_shape[1] != teacher_input_ids.shape[1]:
                                raise ValueError(f"Failed to fix teacher_logits shape. Final shape: {final_shape}")
                            
                            
                            # å°†æ•™å¸ˆlogitsæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
                            train_data["teacher_logits"] = teacher_logits
                            
                            
                        except Exception as e:
                            # å›é€€åˆ°å ä½ç¬¦ï¼ˆä¸æ¨èï¼Œä½†ç¡®ä¿ç¨‹åºèƒ½è¿è¡Œï¼‰
                            batch_size = train_data["input_ids"].shape[0]
                            seq_len = train_data["input_ids"].shape[1]
                            vocab_size = 32000  # å‡è®¾çš„è¯æ±‡è¡¨å¤§å°
                            placeholder_logits = torch.randn(batch_size, seq_len, vocab_size) * 0.1
                            train_data["teacher_logits"] = placeholder_logits
                    
                    # å‡†å¤‡å­¦ç”Ÿæ¨¡å‹è¿›è¡Œlogprobæ¨ç†
                    try:
                        student_policy.prepare_for_lp_inference()
                    except Exception as e:
                        raise
                    

                    try:
                        
                        # æ£€æŸ¥teacher_logitsçš„å½¢çŠ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if "teacher_logits" in train_data:
                            teacher_logits = train_data["teacher_logits"]
                            
                            # å¦‚æœteacher_logitsçš„å½¢çŠ¶ä¸æ­£ç¡®ï¼Œå¼ºåˆ¶ä¿®å¤
                            if len(teacher_logits.shape) != 3:
                                if len(teacher_logits.shape) == 2:
                                    # å¦‚æœæ˜¯[batch_size, vocab_size]ï¼Œé‡å¡‘ä¸º[batch_size, seq_len, vocab_size]
                                    batch_size = teacher_logits.shape[0]
                                    vocab_size = teacher_logits.shape[1]
                                    seq_len = train_data["input_ids"].shape[1]
                                    teacher_logits = teacher_logits.unsqueeze(1).expand(-1, seq_len, -1)
          
                                else:
                                    raise ValueError(f"teacher_logits has unexpected shape: {teacher_logits.shape}")
                            
                            # éªŒè¯ä¿®å¤åçš„å½¢çŠ¶
                            expected_shape = (train_data["input_ids"].shape[0], train_data["input_ids"].shape[1], -1)
                            if teacher_logits.shape[0] != expected_shape[0] or teacher_logits.shape[1] != expected_shape[1]:
                                raise ValueError(f"Failed to fix teacher_logits shape")
                            
                            # æ›´æ–°train_dataä¸­çš„teacher_logits
                            train_data["teacher_logits"] = teacher_logits
 
                        
                        # å‡†å¤‡è¾“å…¥æ•°æ®
                        input_ids = train_data["input_ids"].to("cuda")
                        attention_mask = torch.ones_like(input_ids, dtype=torch.long)
                        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0).expand(input_ids.shape[0], -1)
                        
                        # ç›´æ¥è°ƒç”¨å­¦ç”Ÿæ¨¡å‹
                        with torch.no_grad():
                            student_policy.prepare_for_lp_inference()
                            
                            num_shards = len(student_policy.worker_group.workers)

                            
                            # ç¡®ä¿batch sizeæ˜¯shardsçš„å€æ•°
                            current_batch_size = input_ids.shape[0]
                            if current_batch_size % num_shards != 0:
                                # è°ƒæ•´batch sizeåˆ°æœ€è¿‘çš„shardså€æ•°
                                adjusted_batch_size = ((current_batch_size // num_shards) + 1) * num_shards

                                
                                # æ‰©å±•æ•°æ®åˆ°è°ƒæ•´åçš„batch size
                                if adjusted_batch_size > current_batch_size:
                                    # é‡å¤æœ€åä¸€ä¸ªæ ·æœ¬æ¥å¡«å……
                                    padding_size = adjusted_batch_size - current_batch_size
                                    input_ids = torch.cat([input_ids, input_ids[-1:].repeat(padding_size, 1)], dim=0)
                                    attention_mask = torch.cat([attention_mask, attention_mask[-1:].repeat(padding_size, 1)], dim=0)
                                    position_ids = torch.cat([position_ids, position_ids[-1:].repeat(padding_size, 1)], dim=0)
                            
                            # åˆ›å»ºæ­£ç¡®çš„è®­ç»ƒæ•°æ®æ ¼å¼
                            train_data_for_logprobs_dict = {
                                "input_ids": input_ids,
                                "input_lengths": torch.tensor([input_ids.shape[1]] * input_ids.shape[0]),
                                "token_mask": torch.ones(input_ids.shape[0], input_ids.shape[1]),
                                "sample_mask": torch.ones(input_ids.shape[0]),
                            }
                            
                            # éªŒè¯æ‰€æœ‰å­—æ®µéƒ½æ˜¯å¼ é‡
                            for key, value in train_data_for_logprobs_dict.items():
                                if not torch.is_tensor(value):
                                    print(f"  âŒ Critical error: {key} is not a tensor: {type(value)}")
                                    raise ValueError(f"Field {key} must be a tensor, got {type(value)}")
                            
                           
                    except Exception as e:
                        raise
               
                    # è®¡ç®—è’¸é¦æŸå¤±
                    print("  âœ“ Computing distillation loss...")
                    try:
                        # ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—è’¸é¦æŸå¤± - ä¼ é€’æ‰€æœ‰å¿…è¦çš„å‚æ•°
                        # å°†è’¸é¦å‚æ•°æ·»åŠ åˆ°train_dataä¸­ï¼Œä¾›æŸå¤±å‡½æ•°ä½¿ç”¨
                        # æ³¨æ„ï¼šè¿™äº›æ˜¯æ ‡é‡å€¼ï¼Œä¸æ˜¯å¼ é‡ï¼Œæ‰€ä»¥ä¸ä¼šä¼ é€’ç»™worker
                        train_data["kl_type"] = kl_type
                        train_data["mixed_kl_weight"] = mixed_kl_weight
                        
                        # ç¡®ä¿åªåœ¨response tokensä¸Šè®¡ç®—KLæ•£åº¦
                        if "token_mask" in train_data:
                            token_mask = train_data["token_mask"]
                            total_tokens = token_mask.numel()
                            response_tokens = token_mask.sum().item()
                            prompt_tokens = total_tokens - response_tokens

                        else:
                            # å¦‚æœæ²¡æœ‰token_maskï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ï¼ˆå…¨1ï¼Œä½†è¿™ä¸æ˜¯ç†æƒ³æƒ…å†µï¼‰
                            token_mask = torch.ones_like(train_data["input_ids"], dtype=torch.bool)
                            print(f"  âš ï¸ Warning: No token_mask found, using all tokens for loss calculation")
                        
                    except Exception as e:
                        print(f"  âŒ Failed to compute distillation loss: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                
                # 5. è®­ç»ƒå­¦ç”Ÿæ¨¡å‹

                # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                all_batch_sizes = [train_data[key].shape[0] for key in train_data.keys() if torch.is_tensor(train_data[key])]
                if len(set(all_batch_sizes)) != 1:
                    raise ValueError(f"Batch dimensions must be consistent, got: {all_batch_sizes}")
                
                

                distillation_safe_data = {}
                
                for key, value in train_data.items():
                    if key in ["teacher_logits"]:
                        distillation_safe_data[key] = value
                        if len(value.shape) == 3:
                            batch_size, seq_len, vocab_size = value.shape
                            flattened_logits = value.view(batch_size * seq_len, vocab_size)
                            
                            # åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„keyï¼Œworkerä¸ä¼šæ£€æŸ¥
                            safe_key = f"distillation_{key}_flattened"
                            distillation_safe_data[safe_key] = flattened_logits
                            
                            # å­˜å‚¨åŸå§‹å½¢çŠ¶ä¿¡æ¯
                            distillation_safe_data[f"{safe_key}_shape"] = torch.tensor([batch_size, seq_len, vocab_size])
                        else:
                            distillation_safe_data[key] = value
                    else:
                        # å¯¹äºå…¶ä»–å­—æ®µï¼Œç›´æ¥å¤åˆ¶
                        distillation_safe_data[key] = value
                

                
                
                
                
                with timer.time("training_prep"):

                    student_policy.prepare_for_training()  
                    STUDENT_GENERATION_STALE = True  # *** MARK AS STALE AFTER TRAINING ***
                
                # åªä¿ç•™workeréœ€è¦çš„æ ‡å‡†å¼ é‡å­—æ®µ
                worker_required_fields = ["input_ids", "input_lengths", "token_mask", "sample_mask", "teacher_logits"]
                clean_worker_data = {}
                
                for field in worker_required_fields:
                    if field in train_data:
                        if torch.is_tensor(train_data[field]):
                            clean_worker_data[field] = train_data[field]
                        else:
                            continue
                    else:
                        continue
                
                # éªŒè¯æ¸…ç†åçš„æ•°æ®
                if len(clean_worker_data) != len(worker_required_fields):
                    raise ValueError("Missing required fields for worker")
                
                # åˆ›å»ºå¹²å‡€çš„BatchedDataDictç”¨äºworker
                worker_train_data = BatchedDataDict[DistillationLossDataDict](clean_worker_data)
           
                with timer.time("policy_training"):
                    try:
                        # ä½¿ç”¨æ¸…ç†åçš„æ•°æ®ä¼ é€’ç»™worker
                        train_results = student_policy.train(worker_train_data, loss_fn)
                    except Exception as e:
                        raise
                # é‡‡ç”¨ä¸å…¶ä»–ç®—æ³•ä¸€è‡´çš„æ–¹å¼ï¼Œé¿å…é‡å¤è®°å½•train/loss
                loss_list = train_results["all_mb_metrics"]["loss"]
                loss = sum(loss_list) / len(loss_list)
                
                # æ„å»ºè®­ç»ƒæŒ‡æ ‡
                metrics = {
                    "loss": loss,  # ä¸»è¦è®­ç»ƒæŸå¤±
                    "grad_norm": train_results["grad_norm"].numpy() if hasattr(train_results["grad_norm"], "numpy") else train_results["grad_norm"],
                }
                
                # æ·»åŠ å…¶ä»–å¾®æ‰¹æ¬¡æŒ‡æ ‡ï¼ˆä½†ä¸åŒ…å«lossï¼Œé¿å…é‡å¤ï¼‰
                # æ­£ç¡®å¤„ç†æ•°æ®ç±»å‹ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•°å€¼ç±»å‹
                all_mb_metrics = train_results["all_mb_metrics"].copy()
                if "loss" in all_mb_metrics:
                    del all_mb_metrics["loss"]  # é¿å…é‡å¤è®°å½•loss
                
                # å®‰å…¨åœ°æ·»åŠ å¾®æ‰¹æ¬¡æŒ‡æ ‡ï¼Œç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                for k, v in all_mb_metrics.items():
                    if isinstance(v, (list, tuple)):
                        # å¦‚æœæ˜¯list/tupleï¼Œè®¡ç®—å¹³å‡å€¼
                        if len(v) > 0:
                            if isinstance(v[0], (int, float)):
                                metrics[k] = sum(v) / len(v)
                            elif hasattr(v[0], 'numpy'):
                                metrics[k] = sum(x.numpy() for x in v) / len(v)
                            else:
                                # è·³è¿‡æ— æ³•å¤„ç†çš„ç±»å‹
                                continue
                        else:
                            # ç©ºlistï¼Œè·³è¿‡
                            continue
                    elif isinstance(v, (int, float)):
                        # ç›´æ¥ä½¿ç”¨æ•°å€¼
                        metrics[k] = v
                    elif hasattr(v, 'numpy'):
                        # è½¬æ¢ä¸ºnumpy
                        metrics[k] = v.numpy()
                    elif hasattr(v, 'item'):
                        # è½¬æ¢ä¸ºPythonæ ‡é‡
                        metrics[k] = v.item()
                    else:
                        # è·³è¿‡æ— æ³•å¤„ç†çš„ç±»å‹
                        continue
                
                # è®°å½•ç”Ÿæˆé•¿åº¦ç›¸å…³æŒ‡æ ‡
                if "input_ids" in train_data:
                    input_lengths = (train_data["input_ids"] != 0).sum(dim=1)
                    metrics.update({
                        "avg_input_length": input_lengths.float().mean().item(),
                        "max_input_length": input_lengths.max().item(),
                        "min_input_length": input_lengths.min().item(),
                        "input_length_std": input_lengths.float().std().item(),
                    })
                
                # è®°å½•å½“å‰æœ€ä½³éªŒè¯lossï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if "val_loss" in distillation_save_state and distillation_save_state["val_loss"] is not None:
                    current_best_val_loss = distillation_save_state["val_loss"]
                    metrics["best_val_loss"] = current_best_val_loss
                
                # è®°å½•è’¸é¦å‚æ•°
                metrics.update({
                    "kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                    "mixed_kl_weight": mixed_kl_weight,
                })
                
                # ä½¿ç”¨prefix="train"è®°å½•æ‰€æœ‰æŒ‡æ ‡ï¼Œé¿å…é‡å¤
                if logger is not None:
                    logger.log_metrics(metrics, step, prefix="train")
                    
                    # æ‰“å°è®­ç»ƒlossä¿¡æ¯
                    print(f"  âœ…âœ…âœ… [Training] Step {step}: Loss = {loss:.6f}")
    
                step += 1
                distillation_save_state["step"] = step
                # ä½¿ç”¨é…ç½®ä¸­çš„å€¼
                distillation_save_state["consumed_samples"] += distillation_config.get("num_prompts_per_step", 1)

                
                # 7. ä¿å­˜æ£€æŸ¥ç‚¹
                if step % distillation_config["save_steps"] == 0:
                    try:
                        checkpoint_path = checkpointer.init_tmp_checkpoint(
                            step, distillation_save_state, master_config
                        )
                        student_policy.save_checkpoint(
                            weights_path=os.path.join(checkpoint_path, "policy", "weights"),
                            optimizer_path=os.path.join(checkpoint_path, "policy", "optimizer"),
                            tokenizer_path=os.path.join(checkpoint_path, "policy", "tokenizer"),
                        )
                        # ä¿å­˜æ•°æ®åŠ è½½å™¨çŠ¶æ€
                        torch.save(
                            train_dataloader.state_dict(),
                            os.path.join(checkpoint_path, "train_dataloader.pt"),
                        )
                        checkpointer.finalize_checkpoint(checkpoint_path)
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                
                if step % distillation_config["eval_steps"] == 0 and val_dataloader is not None:
                    try:
                        if NEED_REFIT and STUDENT_GENERATION_STALE:
                            # ä¼ é€’ç”Ÿæˆé…ç½®å‚æ•°
                            generation_config = {
                                'temperature': temperature,
                                'decoding_method': decoding_method,
                                'max_length': max_length,
                            }
                            refit_student_generation(
                                student_policy, student_generation, colocated_inference, generation_config=generation_config, master_config=master_config
                            )
                            STUDENT_GENERATION_STALE = False
                        else:
                            if student_generation is not None:
                                student_generation.prepare_for_generation()
                        
                        val_metrics = validate(
                            student_generation,
                            val_dataloader,
                            tokenizer,
                            step + 1,
                            master_config,
                        )
                        
                        # è®°å½•éªŒè¯æŒ‡æ ‡
                        if val_metrics:
                            # è®°å½•éªŒè¯loss - åªè®°å½•åˆ°eval/å‘½åç©ºé—´
                            if "val_loss" in val_metrics:
                                logger.log_metrics({"eval/loss": val_metrics["val_loss"]}, step + 1)
                                distillation_save_state["val_loss"] = val_metrics["val_loss"]
                                print(f"  âœ…âœ…âœ… [Validation] Step {step + 1}: Val Loss = {val_metrics['val_loss']:.6f}")
                            
                            # è®°å½•å…¶ä»–éªŒè¯æŒ‡æ ‡ - åªè®°å½•åˆ°eval/å‘½åç©ºé—´
                            for k, v in val_metrics.items():
                                if k != "val_loss" and isinstance(v, (int, float)):
                                    logger.log_metrics({f"eval/{k}": v}, step + 1)
                            
                            # è®°å½•éªŒè¯æ—¶çš„ç”Ÿæˆé•¿åº¦ä¿¡æ¯ - åªè®°å½•åˆ°eval/å‘½åç©ºé—´
                            if "val_avg_sequence_length" in val_metrics:
                                logger.log_metrics({
                                    "eval/avg_sequence_length": val_metrics["val_avg_sequence_length"],
                                    "eval/max_sequence_length": val_metrics.get("val_max_sequence_length", 0),
                                    "eval/min_sequence_length": val_metrics.get("val_min_sequence_length", 0),
                                }, step + 1)
                            
                            # è®°å½•éªŒè¯æ—¶çš„è’¸é¦å‚æ•° - åªè®°å½•åˆ°eval/å‘½åç©ºé—´
                            logger.log_metrics({
                                "eval/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                                "eval/mixed_kl_weight": mixed_kl_weight,
                            }, step + 1)
                        
                        if student_generation is not None:
                            student_generation.finish_generation()
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                
                # 9. æ—¥å¿—è®°å½•
                if step % distillation_config["logging_steps"] == 0:
                    try:
                        logger.log_metrics({
                            "step": step,
                            "consumed_samples": distillation_save_state["consumed_samples"],
                        })
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
    
    except Exception as e:
        import traceback
        traceback.print_exc()
