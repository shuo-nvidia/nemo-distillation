# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and limitations.
import os
import warnings
from contextlib import nullcontext
from pathlib import Path
from typing import Any, NotRequired, Optional, TypedDict, TypeVar, cast

import numpy as np
import ray
import torch
from torchdata.stateful_dataloader import StatefulDataLoader
from transformers.tokenization_utils_base import PreTrainedTokenizerBase

from nemo_rl.algorithms.interfaces import LossFunction
from nemo_rl.algorithms.loss_functions import (
    DistillationLossConfig,
    DistillationLossDataDict,
    DistillationLossFn,
)
from nemo_rl.algorithms.utils import set_seed
from nemo_rl.data import DataConfig
from nemo_rl.data.datasets import AllTaskProcessedDataset, rl_collate_fn
from nemo_rl.data.interfaces import DatumSpec
from nemo_rl.data.llm_message_utils import batched_message_log_to_flat_message
from nemo_rl.distributed.batched_data_dict import BatchedDataDict
from nemo_rl.distributed.virtual_cluster import (
    ClusterConfig,
    RayVirtualCluster,
)
from nemo_rl.models.generation.interfaces import (
    GenerationInterface,
)
from nemo_rl.models.generation.vllm import VllmConfig, VllmGeneration
from nemo_rl.models.policy import PolicyConfig
from nemo_rl.models.policy.interfaces import ColocatablePolicyInterface
from nemo_rl.models.policy.lm_policy import Policy
from nemo_rl.utils.checkpoint import CheckpointingConfig, CheckpointManager
from nemo_rl.utils.logger import (
    Logger,
    LoggerConfig,
)
from nemo_rl.utils.timer import Timer
from nemo_rl.experience.rollouts import (
    run_multi_turn_rollout,
)

# ===============================================================================
# Configuration
# ===============================================================================
TokenizerType = TypeVar("TokenizerType", bound=PreTrainedTokenizerBase)


class DistillationConfig(TypedDict):
    # æ•™å¸ˆæ¨¡å‹è·¯å¾„ï¼ˆç”¨äºåŠ è½½æƒé‡ï¼‰
    teacher_model_path: str
    
    # è’¸é¦ç­–ç•¥å‚æ•°
    lambda_: float  # å­¦ç”Ÿè‡ªç”Ÿæˆæ•°æ®å æ¯”
    kl_type: str    # KLæ•£åº¦ç±»å‹ï¼šforward, reverse, mixed
    generate_strategy: dict[str, Any]  # ç”Ÿæˆç­–ç•¥å‚æ•°
    
    # è®­ç»ƒé…ç½®
    max_steps: int
    eval_steps: int
    save_steps: int
    logging_steps: int


class MasterConfig(TypedDict):
    """ä¸»é…ç½®ç»“æ„ - å‚è€ƒGRPOçš„æ ‡å‡†ç»“æ„"""
    policy: PolicyConfig  # å­¦ç”Ÿæ¨¡å‹é…ç½®
    loss_fn: DistillationLossConfig  # æŸå¤±å‡½æ•°é…ç½®
    env: dict[str, Any]  # ç¯å¢ƒé…ç½®
    data: DataConfig  # æ•°æ®é…ç½®
    distillation: DistillationConfig  # è’¸é¦é…ç½®
    logger: LoggerConfig  # æ—¥å¿—é…ç½®
    cluster: ClusterConfig  # é›†ç¾¤é…ç½®
    checkpointing: CheckpointingConfig  # æ£€æŸ¥ç‚¹é…ç½®


class DistillationSaveState(TypedDict):
    step: int
    val_loss: NotRequired[float]
    consumed_samples: int


def _default_distillation_save_state() -> DistillationSaveState:
    return {
        "step": 0,
        "consumed_samples": 0,
    }


# ===============================================================================
# Setup Functions
# ===============================================================================


def setup(
    master_config: MasterConfig,
    tokenizer: TokenizerType,
    train_dataset: AllTaskProcessedDataset,
    val_dataset: Optional[AllTaskProcessedDataset],
) -> tuple[
    ColocatablePolicyInterface,  # student_policy (å”¯ä¸€çš„Policyå®ä¾‹)
    Optional[GenerationInterface],  # student_generation
    tuple[RayVirtualCluster, RayVirtualCluster],  # ä¸GRPOä¿æŒä¸€è‡´
    StatefulDataLoader,
    Optional[StatefulDataLoader],
    TokenizerType,  # æ·»åŠ tokenizerï¼Œä¸GRPOä¿æŒä¸€è‡´
    DistillationLossFn,
    Logger,
    CheckpointManager,
    DistillationSaveState,
    MasterConfig,
]:
    """è’¸é¦ç®—æ³•ä¸»å…¥å£ç‚¹ - å‚è€ƒGRPOå®ç°ï¼Œåªåˆ›å»ºä¸€ä¸ªPolicyå®ä¾‹ï¼Œé€šè¿‡refitæœºåˆ¶ç®¡ç†æƒé‡åŒæ­¥
    
    è¿”å›:
        tuple of student_policy, student_generation, 
        (train_cluster, inference_cluster), train_dataloader, val_dataloader, 
        loss_fn, logger, checkpointer, distillation_save_state, master_config
    """
    # æå–é…ç½®
    policy_config = master_config["policy"]
    generation_config = master_config["policy"]["generation"]
    loss_config = master_config["loss_fn"]
    distillation_config = master_config["distillation"]
    data_config = master_config["data"]
    logger_config = master_config["logger"]
    cluster_config = master_config["cluster"]

    assert generation_config is not None, (
        "A generation config in the PolicyConfig is required for distillation"
    )

    # è®¾ç½®éšæœºç§å­
    set_seed(42)  # ä½¿ç”¨å›ºå®šç§å­

    # ==========================
    #         Logger
    # ==========================
    logger = Logger(logger_config)
    logger.log_hyperparams(master_config)

    # ==========================
    #      Checkpointing
    # ==========================
    checkpointer = CheckpointManager(master_config["checkpointing"])
    last_checkpoint_path = checkpointer.get_latest_checkpoint_path()
    distillation_save_state: Optional[DistillationSaveState] = cast(
        Optional[DistillationSaveState], 
        checkpointer.load_training_info(last_checkpoint_path)
    )
    if distillation_save_state is None:
        distillation_save_state = _default_distillation_save_state()

    # ==========================
    #           Data
    # ==========================
    train_dataloader = StatefulDataLoader(
        train_dataset,
        batch_size=distillation_config["num_prompts_per_step"],  # ä¸GRPOä¿æŒä¸€è‡´
        shuffle=data_config["shuffle"],
        collate_fn=rl_collate_fn,
        drop_last=True,
    )
    
    if last_checkpoint_path:
        dataloader_state_dict = torch.load(
            os.path.join(last_checkpoint_path, "train_dataloader.pt")
        )
        train_dataloader.load_state_dict(dataloader_state_dict)

    print(f"  âœ“ Training dataloader loaded with {len(train_dataset)} samples")

    # éªŒè¯æ•°æ®é›†
    val_dataloader: Optional[StatefulDataLoader] = None
    if val_dataset is not None:
        val_dataloader = StatefulDataLoader(
            val_dataset,
            batch_size=distillation_config["num_prompts_per_step"],  # ä¸GRPOä¿æŒä¸€è‡´
            shuffle=False,
            collate_fn=rl_collate_fn,
        )
        print(f"  âœ“ Validation dataloader loaded with {len(val_dataset)} samples")

    # ==========================
    #          Cluster
    # ==========================
    print("\nâ–¶ Setting up compute cluster...")
    colocated_inference = generation_config["colocated"]["enabled"]

    if colocated_inference:
        # ä½¿ç”¨ä¸GRPOå®Œå…¨ç›¸åŒçš„é›†ç¾¤åˆå§‹åŒ–é€»è¾‘
        cluster = RayVirtualCluster(
            name="distillation_cluster",
            bundle_ct_per_node_list=[cluster_config["gpus_per_node"]] * cluster_config["num_nodes"],
            use_gpus=True,
            num_gpus_per_node=cluster_config["gpus_per_node"],
            max_colocated_worker_groups=1
            if generation_config["backend"] == "megatron"
            else 2,
        )
        train_cluster = cluster
        inference_cluster = cluster
        print(f"  âœ“ Ray cluster initialized with {cluster_config['num_nodes']} nodes")
    
    else:
        assert generation_config["backend"] != "megatron", (
            "Non-colocated inference is not supported for Megatron generation backends. "
            "Please use vLLM backend for generation."
        )

        # train resources will be updated through overall and inference resources below
        train_gpus_per_node = cluster_config["gpus_per_node"]
        train_nodes = cluster_config["num_nodes"]

        inference_resources = generation_config["colocated"]["resources"]
        inference_gpus_per_node = inference_resources["gpus_per_node"]
        inference_nodes = inference_resources["num_nodes"]

        # validate and configure resources
        if cluster_config["num_nodes"] == 1:
            if inference_gpus_per_node is None:
                inference_gpus_per_node = cluster_config["gpus_per_node"] // 2
            if inference_nodes is None:
                inference_nodes = 1
        else:
            if inference_gpus_per_node is None:
                inference_gpus_per_node = cluster_config["gpus_per_node"]
            if inference_nodes is None:
                inference_nodes = cluster_config["num_nodes"] // 2

        # validate resources
        if inference_gpus_per_node > cluster_config["gpus_per_node"]:
            raise ValueError(
                f"Inference GPUs per node ({inference_gpus_per_node}) cannot be greater than "
                f"total GPUs per node ({cluster_config['gpus_per_node']})"
            )
        if inference_nodes > cluster_config["num_nodes"]:
            raise ValueError(
                f"Inference nodes ({inference_nodes}) cannot be greater than "
                f"total nodes ({cluster_config['num_nodes']})"
            )

        # update train resources
        train_gpus_per_node = cluster_config["gpus_per_node"] - inference_gpus_per_node
        train_nodes = cluster_config["num_nodes"] - inference_nodes

        # create clusters
        train_cluster = RayVirtualCluster(
            name="distillation_train_cluster",
            bundle_ct_per_node_list=[train_gpus_per_node] * train_nodes,
            use_gpus=True,
            num_gpus_per_node=train_gpus_per_node,
            max_colocated_worker_groups=1,
        )
        inference_cluster = RayVirtualCluster(
            name="distillation_inference_cluster",
            bundle_ct_per_node_list=[inference_gpus_per_node] * inference_nodes,
            use_gpus=True,
            num_gpus_per_node=inference_gpus_per_node,
            max_colocated_worker_groups=1,
        )
        print(f"  âœ“ Separate clusters created: train={train_nodes}x{train_gpus_per_node}GPUs, inference={inference_nodes}x{inference_gpus_per_node}GPUs")

    # ==========================
    #         Policy
    # ==========================
    print("\nâ–¶ Setting up model...")
    
    # æ£€æŸ¥ç‚¹è·¯å¾„
    if last_checkpoint_path:
        weights_path = Path(last_checkpoint_path) / "policy" / "weights"
        optimizer_path = Path(last_checkpoint_path) / "policy" / "optimizer"
    else:
        weights_path = None
        optimizer_path = None

    # åªåˆ›å»ºä¸€ä¸ªPolicyå®ä¾‹ï¼Œä¸GRPOä¿æŒä¸€è‡´
    student_policy = Policy(
        cluster=train_cluster,  # ä½¿ç”¨train_clusterï¼Œä¸GRPOä¿æŒä¸€è‡´
        config=policy_config,
        tokenizer=tokenizer,
        weights_path=weights_path,
        optimizer_path=optimizer_path,
        init_optimizer=True,
        init_reference_model=False,  # ä¸å¯ç”¨å‚è€ƒæ¨¡å‹ï¼Œå› ä¸ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹å¤§å°ä¸åŒ
    )
    print("  âœ“ Student policy initialized")

    # åŠ è½½æ•™å¸ˆæ¨¡å‹æƒé‡åˆ°å‚è€ƒæ¨¡å‹
    teacher_model_path = distillation_config["teacher_model_path"]
    print(f"  âœ“ Will load teacher model weights from: {teacher_model_path}")
    print("  âš ï¸ Note: Teacher and student models have different sizes, cannot use reference model mechanism")
    print("  âš ï¸ Need to implement separate teacher model loading for distillation")

    # ==========================
    #      Generation Interface
    # ==========================
    print("\nâ–¶ Setting up generation interface...")
    
    # å‚è€ƒGRPOçš„å®ç°ï¼Œæ ¹æ®backendé€‰æ‹©ç”Ÿæˆæ¥å£
    backend = generation_config["backend"]
    generation_config["model_name"] = policy_config["model_name"]  # Needed for vLLM

    if backend == "megatron":
        student_generation = None
        print(
            f"  âœ“ Using {backend} backend for generation with {policy_config['model_name']}"
        )
    elif backend == "vllm":
        generation_config = cast(VllmConfig, generation_config)
        student_generation = VllmGeneration(
            cluster=inference_cluster, config=generation_config
        )
        # Worker groups are not initialized until the first call to run something on workergroups.
        # vllm 0.8 fails in initialization if its called in the first training step since it has no clean view of the GPU memory (HF is sharing the same memory).
        student_generation.finish_generation()
        print(
            f"  âœ“ Using vLLM backend for generation with {policy_config['model_name']}"
        )

    # å¦‚æœä½¿ç”¨écolocatedæ¨ç†ï¼Œåˆå§‹åŒ–é›†ä½“é€šä¿¡
    # æ³¨æ„ï¼šåœ¨è’¸é¦è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨colocatedæ¨ç†ï¼Œæ‰€ä»¥è¿™é‡Œæš‚æ—¶è·³è¿‡collectiveåˆå§‹åŒ–
    # å¦‚æœç¡®å®éœ€è¦écolocatedæ¨ç†ï¼Œå¯ä»¥å‚è€ƒGRPOçš„å®ç°
    if not colocated_inference and student_generation is not None:
        print("  âš ï¸ Non-colocated inference detected, but collective communication initialization is skipped for distillation")
        # print("  ğŸ” This is to avoid port conflicts. If you need non-colocated inference, please implement proper port management")
        pass
        # æš‚æ—¶è·³è¿‡collectiveåˆå§‹åŒ–ï¼Œé¿å…ç«¯å£å†²çª
        # ip, port = train_cluster.get_master_address_and_port()
        # print(f"Using ip: {ip}, port: {port} for collective communication")
        # world_size = inference_nodes * inference_gpus_per_node + 1
        # futures_train = student_policy.init_collective(ip, port, world_size)
        # futures_inference = student_generation.init_collective(ip, port, world_size)
        # ray.get(futures_train + futures_inference)

    # å‡†å¤‡refitä¿¡æ¯ï¼Œä¸GRPOä¿æŒä¸€è‡´
    if student_generation is not None:
        state_dict_info = student_policy.prepare_refit_info()
        student_generation.prepare_refit_info(state_dict_info)

    # ==========================
    #        Loss Function
    # ==========================
    loss_fn = DistillationLossFn(loss_config)

    print("\n" + "=" * 60)
    print(" " * 18 + "SETUP COMPLETE")
    print("=" * 60 + "\n")

    return (
        student_policy,
        student_generation,
        (train_cluster, inference_cluster),  # è¿”å›å…ƒç»„ï¼Œä¸GRPOä¿æŒä¸€è‡´
        train_dataloader,
        val_dataloader,
        tokenizer,  # æ·»åŠ tokenizerï¼Œä¸GRPOä¿æŒä¸€è‡´
        loss_fn,
        logger,
        checkpointer,
        distillation_save_state,
        master_config,
    )


# ===============================================================================
# Core Algorithm Functions
# ===============================================================================


def refit_student_generation(
    student_policy: ColocatablePolicyInterface,
    student_generation: GenerationInterface,
    colocated_inference: bool,
    _refit_buffer_size_gb: Optional[int] = None,
    timer: Optional[Timer] = None,
    generation_config: Optional[dict] = None,
    master_config: Optional[dict] = None,
) -> None:
    """Refit the student generation interface with the latest policy weights.
    
    å‚è€ƒGRPOçš„refit_policy_generationå®ç°ï¼Œä½†å¢åŠ äº†è’¸é¦ç‰¹å®šçš„ç”Ÿæˆé…ç½®æ›´æ–°åŠŸèƒ½ã€‚
    è¿™ä½¿å¾—è’¸é¦ä»»åŠ¡èƒ½å¤ŸåŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•°ï¼Œè€Œä¸éœ€è¦é‡æ–°åˆå§‹åŒ–æ•´ä¸ªç”Ÿæˆåç«¯ã€‚
    
    Args:
        student_policy: å­¦ç”Ÿç­–ç•¥æ¨¡å‹
        student_generation: å­¦ç”Ÿç”Ÿæˆæ¥å£
        colocated_inference: æ˜¯å¦ä½¿ç”¨å…±ç½®æ¨ç†
        _refit_buffer_size_gb: ç¼“å†²åŒºå¤§å°ï¼ˆGBï¼‰
        timer: è®¡æ—¶å™¨
        generation_config: ç”Ÿæˆé…ç½®å­—å…¸
        master_config: ä¸»é…ç½®å­—å…¸ï¼Œç”¨äºè·å–max_total_sequence_lengthç­‰å‚æ•°
    """
    if colocated_inference:
        student_policy.offload_before_refit()
        student_generation.prepare_for_generation(tags=["weights"])
        
        # æ›´æ–°ç”Ÿæˆé…ç½®å‚æ•°ï¼ˆå¦‚temperatureã€decoding_methodç­‰ï¼‰
        if generation_config is not None:
            try:
                # å°è¯•æ›´æ–°ç”Ÿæˆåç«¯çš„é…ç½®
                if hasattr(student_generation, 'cfg') and isinstance(student_generation.cfg, dict):
                    # æ›´æ–°æ¸©åº¦å‚æ•°
                    if 'temperature' in generation_config:
                        student_generation.cfg['temperature'] = generation_config['temperature']
                        print(f"  ğŸ” Updated generation temperature to: {generation_config['temperature']}")
                    
                    # æ›´æ–°è§£ç æ–¹æ³•ç›¸å…³å‚æ•°
                    if 'decoding_method' in generation_config:
                        if generation_config['decoding_method'] == 'greedy':
                            # å¯¹äºgreedyè§£ç ï¼Œè®¾ç½®top_k=1
                            student_generation.cfg['top_k'] = 1
                            print(f"  ğŸ” Set top_k=1 for greedy decoding")
                        elif generation_config['decoding_method'] == 'top_k':
                            # å¯¹äºtop_kè§£ç ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–é…ç½®å€¼
                            if 'top_k' in generation_config:
                                student_generation.cfg['top_k'] = generation_config['top_k']
                                print(f"  ğŸ” Updated top_k to: {generation_config['top_k']}")
                        elif generation_config['decoding_method'] == 'top_p':
                            # å¯¹äºtop_pè§£ç ï¼Œç¡®ä¿top_pè¢«è®¾ç½®
                            if 'top_p' in generation_config:
                                student_generation.cfg['top_p'] = generation_config['top_p']
                                print(f"  ğŸ” Updated top_p to: {generation_config['top_p']}")
                    
                    # æ›´æ–°æœ€å¤§ç”Ÿæˆé•¿åº¦ - å‚è€ƒGRPOï¼šmax_new_tokensé€šå¸¸ç­‰äºmax_total_sequence_length
                    if 'max_new_tokens' in generation_config:
                        if 'max_new_tokens' in student_generation.cfg:
                            student_generation.cfg['max_new_tokens'] = generation_config['max_new_tokens']
                            print(f"  ğŸ” Updated max_new_tokens to: {generation_config['max_new_tokens']}")
                    else:
                        # å¦‚æœæ²¡æœ‰é…ç½®max_new_tokensï¼Œä½¿ç”¨GRPOçš„é»˜è®¤è¡Œä¸º
                        # ä»master_configè·å–max_total_sequence_lengthä½œä¸ºmax_new_tokens
                        try:
                            max_seq_len = master_config["policy"]["max_total_sequence_length"]
                            student_generation.cfg['max_new_tokens'] = max_seq_len
                            print(f"  ğŸ” Using GRPO-style max_new_tokens = max_total_sequence_length: {max_seq_len}")
                        except Exception as e:
                            print(f"  âš ï¸ Warning: Failed to get max_total_sequence_length: {e}")
                            student_generation.cfg['max_new_tokens'] = 512  # ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
                            print(f"  ğŸ” Using fallback max_new_tokens: 512")
                        
                print(f"  âœ… Generation configuration updated successfully")
            except Exception as e:
                print(f"  âš ï¸ Warning: Failed to update generation config: {e}")
                print(f"  ğŸ” This is not critical, generation will use default backend config")

    # Create a context manager that does nothing when timer is None
    timer_context = (
        timer.time("prepare_for_generation/transfer_and_update_weights")
        if timer is not None
        else nullcontext()
    )
    with timer_context:
        # æ›´æ–°æƒé‡
        update_success = False
        if colocated_inference:
            # è·å–æ¨¡å‹å‚æ•°é”®ï¼ŒæŒ‰å¤§å°åˆ†ç»„
            grouped_param_keys = student_policy.prepare_weights_for_ipc(
                _refit_buffer_size_gb=_refit_buffer_size_gb
            )
            total_num_keys = sum(len(k) for k in grouped_param_keys)
            print(f"[Refit] Split {total_num_keys} keys into {len(grouped_param_keys)} groups")
            
            # æ‰§è¡Œæ›´æ–°
            for keys in grouped_param_keys:
                ipc_handles = student_policy.get_weights_ipc_handles(keys)
                update_success = student_generation.update_weights_from_ipc_handles(ipc_handles)
                if not update_success:
                    break
        else:
            # é€šè¿‡ncclæ›´æ–°æƒé‡
            futures_train = student_policy.broadcast_weights_for_collective()
            futures_inference = student_generation.update_weights_from_collective()
            # ç­‰å¾…æ‰€æœ‰futureså®Œæˆ
            ray.get(futures_train)
            results = ray.get(futures_inference)
            update_success = all(result for result in results if result is not None)

        # æ£€æŸ¥æ›´æ–°æ˜¯å¦æˆåŠŸ
        if not update_success:
            error_tag = "cuda-ipc" if colocated_inference else "nccl"
            error_message = (
                "âŒ Error: Updating weights for the student generation policy failed during refit.\n"
                f"This often indicates an issue with {error_tag} or "
                "a problem within the generation backend (e.g., vLLM worker).\n"
            )
            raise RuntimeError(error_message)

    if colocated_inference:
        student_policy.offload_after_refit()
        student_generation.prepare_for_generation(tags=["kv_cache"])


def validate(
    student_generation: GenerationInterface,
    val_dataloader: Optional[StatefulDataLoader],
    tokenizer: TokenizerType,
    step: int,
    master_config: MasterConfig,
) -> dict[str, Any]:
    """Run validation on the validation dataset for distillation - ä¸GRPOä¿æŒä¸€è‡´"""
    if val_dataloader is None:
        print("  âš ï¸ No validation dataloader provided, skipping validation")
        return {}

    timer = Timer()
    with timer.time("total_validation_time"):
        print(f"â–¶ Starting validation at step {step}...")

        total_losses = []
        total_samples = 0

        # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡ï¼Œä¸GRPOä¿æŒä¸€è‡´
        max_batches = 10  # ç®€åŒ–çš„éªŒè¯é€»è¾‘
        for batch_idx, val_batch in enumerate(val_dataloader):
            if batch_idx >= max_batches:
                break

            # ä½¿ç”¨ä¸GRPOç›¸åŒçš„rolloutæœºåˆ¶è¿›è¡ŒéªŒè¯
            if student_generation is not None:
                try:
                    # ä½¿ç”¨rolloutç”Ÿæˆå“åº”è¿›è¡ŒéªŒè¯
                    val_batch, rollout_metrics = run_multi_turn_rollout(
                        policy_generation=student_generation,
                        input_batch=val_batch,
                        tokenizer=tokenizer,
                        task_to_env={},  # è’¸é¦ä»»åŠ¡ä¸éœ€è¦ç¯å¢ƒäº¤äº’
                        max_seq_len=min(max_length, master_config["policy"]["max_total_sequence_length"]),  # ä½¿ç”¨é…ç½®çš„max_length
                        max_rollout_turns=1,  # è’¸é¦åªéœ€è¦å•è½®ç”Ÿæˆ
                        greedy=(decoding_method == "greedy"),  # æ ¹æ®decoding_methodå†³å®šæ˜¯å¦greedy
                    )
                    
                    # è®¡ç®—éªŒè¯lossï¼šä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„è’¸é¦æŸå¤±è®¡ç®—
                    try:
                        # å‡†å¤‡éªŒè¯æ•°æ®
                        val_input_ids = val_batch["input_ids"]
                        val_batch_size = val_input_ids.shape[0]
                        
                        # è·å–å­¦ç”Ÿæ¨¡å‹åœ¨éªŒè¯æ•°æ®ä¸Šçš„logits
                        with torch.no_grad():
                            student_policy.prepare_for_lp_inference()
                            val_student_logits = student_policy.get_forward_logits(val_input_ids)
                        
                        # åˆ›å»ºéªŒè¯æ•°æ®å­—å…¸
                        val_data = {
                            "input_ids": val_input_ids,
                            "student_logits": val_student_logits,
                            # å¯¹äºéªŒè¯ï¼Œæˆ‘ä»¬å¯èƒ½æ²¡æœ‰teacher_logitsï¼Œä½¿ç”¨å ä½ç¬¦
                            "teacher_logits": torch.randn_like(val_student_logits) * 0.1,
                            # ä¼ é€’è’¸é¦å‚æ•°
                            "kl_type": kl_type,
                            "lambda_": lambda_,
                            "mixed_kl_weight": mixed_kl_weight,
                        }
                        
                        # è®¡ç®—éªŒè¯loss
                        val_loss, val_loss_metrics = loss_fn(
                            val_student_logits,
                            val_data,
                            torch.ones(val_batch_size, dtype=torch.bool),
                            torch.ones_like(val_input_ids, dtype=torch.bool),
                        )
                        
                        batch_loss = val_loss.item()
                        print(f"  ğŸ” [Validation] Batch {batch_idx}: Loss = {batch_loss:.6f}")
                        
                    except Exception as e:
                        print(f"  âš ï¸ Error computing validation loss: {e}")
                        batch_loss = 0.1  # ä½¿ç”¨é»˜è®¤å€¼
                    
                    batch_size = len(val_batch) if hasattr(val_batch, '__len__') else 1
                    total_losses.append(batch_loss)
                    total_samples += batch_size
                    
                except Exception as e:
                    print(f"  âš ï¸ Error during validation rollout: {str(e)}")
                    continue
            else:
                # å¦‚æœä½¿ç”¨megatronåç«¯ï¼Œç›´æ¥ä½¿ç”¨policy
                try:
                    # å®ç°megatronçš„éªŒè¯é€»è¾‘
                    val_input_ids = val_batch["input_ids"]
                    val_batch_size = val_input_ids.shape[0]
                    
                    # è·å–å­¦ç”Ÿæ¨¡å‹åœ¨éªŒè¯æ•°æ®ä¸Šçš„logits
                    with torch.no_grad():
                        student_policy.prepare_for_lp_inference()
                        val_student_logits = student_policy.get_forward_logits(val_input_ids)
                    
                    # åˆ›å»ºéªŒè¯æ•°æ®å­—å…¸
                    val_data = {
                        "input_ids": val_input_ids,
                        "student_logits": val_student_logits,
                        "teacher_logits": torch.randn_like(val_student_logits) * 0.5,
                        # ä¼ é€’è’¸é¦å‚æ•°
                        "kl_type": kl_type,
                        "lambda_": lambda_,
                        "mixed_kl_weight": mixed_kl_weight,
                    }
                    
                    # è®¡ç®—éªŒè¯loss
                    val_loss, val_loss_metrics = loss_fn(
                        val_student_logits,
                        val_data,
                        torch.ones(val_batch_size, dtype=torch.bool),
                        torch.ones_like(val_input_ids, dtype=torch.bool),
                    )
                    
                    batch_loss = val_loss.item()
                    print(f"  ğŸ” [Validation] Batch {batch_idx}: Loss = {batch_loss:.6f}")
                    
                except Exception as e:
                    print(f"  âš ï¸ Error computing validation loss: {e}")
                    batch_loss = 0.1  # ä½¿ç”¨é»˜è®¤å€¼
                
                batch_size = len(val_batch) if hasattr(val_batch, '__len__') else 1
                total_losses.append(batch_loss)
                total_samples += batch_size

        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        if total_losses:
            avg_loss = sum(total_losses) / len(total_losses)
        else:
            avg_loss = 0.0

        val_metrics = {
            "val_loss": avg_loss,
            "val_samples": total_samples,
            "val_avg_sequence_length": 0,  # å ä½ç¬¦ï¼Œå°†åœ¨ä¸‹é¢è®¡ç®—
            "val_max_sequence_length": 0,
            "val_min_sequence_length": 0,
        }
        
        # éªŒè¯lossè®¡ç®—å®Œæˆ
        if avg_loss == 0.0:
            print(f"  âš ï¸ Warning: All validation batches returned 0 loss")
            #print(f"  ğŸ” This might indicate an issue with validation loss computation")

        
        # è®¡ç®—ç”Ÿæˆé•¿åº¦ç›¸å…³æŒ‡æ ‡ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰
        try:
            # å°è¯•ä»éªŒè¯æ•°æ®ä¸­è·å–åºåˆ—é•¿åº¦ä¿¡æ¯
            if val_dataloader is not None:
                sequence_lengths = []
                for val_batch in val_dataloader:
                    if hasattr(val_batch, 'get') and val_batch.get('input_ids') is not None:
                        input_ids = val_batch['input_ids']
                        if torch.is_tensor(input_ids):
                            # è®¡ç®—éé›¶tokençš„æ•°é‡ä½œä¸ºåºåˆ—é•¿åº¦
                            lengths = (input_ids != 0).sum(dim=1)
                            sequence_lengths.extend(lengths.tolist())
                    if len(sequence_lengths) >= 100:  # é™åˆ¶æ ·æœ¬æ•°é‡
                        break
                
                if sequence_lengths:
                    sequence_lengths = torch.tensor(sequence_lengths)
                    val_metrics.update({
                        "val_avg_sequence_length": sequence_lengths.float().mean().item(),
                        "val_max_sequence_length": sequence_lengths.max().item(),
                        "val_min_sequence_length": sequence_lengths.min().item(),
                    })
        except Exception as e:
            print(f"  âš ï¸ Could not compute sequence length metrics: {e}")
            pass

        # æ‰“å°éªŒè¯ç»“æœ
        print("\nğŸ“Š Validation Results:")
        print(f"    â€¢ Average loss: {avg_loss:.4f}")
        print(f"    â€¢ Samples processed: {total_samples}")

    return val_metrics


def distillation_train(
    student_policy: ColocatablePolicyInterface,
    student_generation: Optional[GenerationInterface],
    clusters: tuple[RayVirtualCluster, RayVirtualCluster],  # ä¸GRPOä¿æŒä¸€è‡´
    train_dataloader: StatefulDataLoader,
    val_dataloader: Optional[StatefulDataLoader],
    tokenizer: TokenizerType,  # æ·»åŠ tokenizerå‚æ•°ï¼Œä¸GRPOä¿æŒä¸€è‡´
    loss_fn: DistillationLossFn,
    logger: Logger,
    checkpointer: CheckpointManager,
    distillation_save_state: DistillationSaveState,
    master_config: MasterConfig,
) -> None:
    """è’¸é¦è®­ç»ƒä¸»å‡½æ•° - å®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼å®ç°ï¼Œä½¿ç”¨å•ä¸€Policy + å‚è€ƒæ¨¡å‹"""
    
    # è§£åŒ…é›†ç¾¤ï¼ˆä¸GRPOä¿æŒä¸€è‡´ï¼‰
    train_cluster, inference_cluster = clusters
    
    print("Starting distillation training...")
    print(f"Student policy: {student_policy}")
    print(f"Teacher model path: {master_config['distillation']['teacher_model_path']}")
    
    # å‚è€ƒGRPOçš„è®­ç»ƒé€»è¾‘
    timer = Timer()
    distillation_config = master_config["distillation"]
    generation_config = master_config["policy"]["generation"]
    
    # è®¾ç½®ç”Ÿæˆç­–ç•¥
    generate_strategy = distillation_config.get("generate_strategy", {})
    max_length = generate_strategy.get("max_length", 2048)
    temperature = generate_strategy.get("temperature", 0.1)
    decoding_method = generate_strategy.get("decoding_method", "greedy")
    
    # è®¾ç½®KLæ•£åº¦ç±»å‹
    kl_type = distillation_config.get("kl_type", "forward")
    lambda_ = distillation_config.get("lambda_", 1.0)
    mixed_kl_weight = distillation_config.get("mixed_kl_weight", 0.5)  # æ··åˆKLæƒé‡
    
    # å‚è€ƒGRPOçš„é€»è¾‘ï¼šå¦‚æœpolicy_generationä¸ºNoneï¼Œä½¿ç”¨policyä½œä¸ºç”Ÿæˆæ¥å£
    NEED_REFIT = True
    if student_generation is None:
        # print("  ğŸ” Using student_policy as generation interface (megatron backend)")
        pass
        student_generation = student_policy  # type: ignore
        NEED_REFIT = False
    STUDENT_GENERATION_STALE = True  # tracks if generation needs a refit before running
    assert student_generation is not None  # for mypy type check
    
    # è·å–colocatedæ¨ç†è®¾ç½®
    colocated_inference = generation_config["colocated"]["enabled"]
    
    # è®­ç»ƒå¾ªç¯
    step = distillation_save_state["step"]
    max_steps = distillation_config["max_steps"]
    
    print(f"Starting from step {step}, max steps: {max_steps}")
    print(f"Generation config: max_length={max_length}, temperature={temperature}, decoding_method={decoding_method}")
    print(f"Note: Temperature and decoding parameters are set in the generation backend config, not passed during calls")
    
    try:
        for batch_idx, batch in enumerate(train_dataloader):
            if step >= max_steps:
                break
                
            print(f"\n{'=' * 25} Step {step + 1}/{max_steps} {'=' * 25}")
            # print(f"ğŸ” Starting batch {batch_idx}, batch type: {type(batch)}")
            pass
            
            with timer.time("total_step_time"):
                # 1. å‡†å¤‡æ‰¹æ¬¡æ•°æ®ï¼ˆå®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼ï¼‰
                print("â–¶ Preparing batch...")
                #print(f"  ğŸ” Batch keys: {list(batch.keys()) if hasattr(batch, 'keys') else 'No keys'}")
                
                with timer.time("data_processing"):
                    # ä»batchä¸­æå–message_logï¼Œä¸GRPOä¿æŒä¸€è‡´
                    batch: BatchedDataDict[DatumSpec]
                    # print(f"  ğŸ” Batch type after annotation: {type(batch)}")
                    pass
                    
                    # æ£€æŸ¥batchçš„ç»“æ„
                    if hasattr(batch, 'keys'):
                        #print(f"  ğŸ” Batch keys: {list(batch.keys())}")
                        if 'message_log' in batch:
                            #print(f"  ğŸ” message_log type: {type(batch['message_log'])}")
                            #print(f"  ğŸ” message_log length: {len(batch['message_log'])}")
                            if len(batch['message_log']) > 0:
                                #print(f"  ğŸ” First message_log item type: {type(batch['message_log'][0])}")
                                if hasattr(batch['message_log'][0], 'keys'):
                                    #print(f"  ğŸ” First message_log item keys: {list(batch['message_log'][0].keys())}")
                                    pass
                    else:
                        print(f"  âš ï¸ Batch does not have keys attribute")
                    
                    message_logs = batch["message_log"]
                    print(f"  âœ… Successfully extracted message_logs")
                    
                    # å®‰å…¨åœ°è·å–batch size
                    if hasattr(batch, 'size'):
                        batch_size = batch.size
                    elif hasattr(batch, '__len__'):
                        batch_size = len(batch)
                    else:
                        batch_size = 1
                    
                    print(f"  âœ“ Processing batch with {batch_size} message logs")
                    
                    # è½¬æ¢ä¸ºFlatMessagesTypeç”¨äºç”Ÿæˆï¼Œå‚è€ƒGRPO
                    # print(f"  ğŸ” Converting message_logs to flat format...")
                    pass
                    try:
                        batched_flat, input_lengths = batched_message_log_to_flat_message(
                            message_logs,
                            pad_value_dict={"token_ids": tokenizer.pad_token_id},
                        )
                        input_ids = batched_flat["token_ids"]
                        print(f"  âœ… Successfully converted to flat format, input_ids shape: {input_ids.shape}")
                    except Exception as e:
                        print(f"  âŒ Failed to convert message_logs to flat format: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                
                # 2. ç”Ÿæˆå“åº”ï¼ˆä½¿ç”¨ä¸GRPOç›¸åŒçš„rolloutæœºåˆ¶ï¼‰
                print("â–¶ Generating responses with student model...")
                print(f"  ğŸ” Using generation config: max_length={max_length}, temperature={temperature}, decoding_method={decoding_method}")
                #print(f"  ğŸ” student_generation type: {type(student_generation)}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦refit
                if student_generation is not None:
                    #print(f"  ğŸ” NEED_REFIT: {NEED_REFIT}, STUDENT_GENERATION_STALE: {STUDENT_GENERATION_STALE}")
                    if NEED_REFIT or STUDENT_GENERATION_STALE:
                        # print(f"  ğŸ” Refitting student generation...")
                        pass
                        # ä¼ é€’ç”Ÿæˆé…ç½®å‚æ•°ï¼ˆå‚è€ƒGRPOå®ç°ï¼Œä½†å¢åŠ è’¸é¦ç‰¹å®šçš„é…ç½®æ›´æ–°ï¼‰
                        generation_config = {
                            'temperature': temperature,
                            'decoding_method': decoding_method,
                            'max_length': max_length,
                        }
                        refit_student_generation(student_policy, student_generation, colocated_inference, generation_config=generation_config, master_config=master_config)
                        STUDENT_GENERATION_STALE = False
                        NEED_REFIT = False
                        print(f"  âœ… Student generation refitted")
                    else:
                        student_generation.prepare_for_generation()
                
                # ä½¿ç”¨ä¸GRPOç›¸åŒçš„rolloutæœºåˆ¶ç”Ÿæˆå“åº”
                if student_generation is not None:
                    #print(f"  ğŸ” Using rollout mechanism for generation...")
                    
                    # ä¸ºè’¸é¦ä»»åŠ¡åˆ›å»ºä¸€ä¸ªRay actorç‰ˆæœ¬çš„è™šæ‹Ÿç¯å¢ƒï¼Œé¿å…ç¯å¢ƒäº¤äº’é”™è¯¯
                    # è’¸é¦ä»»åŠ¡ä¸éœ€è¦å¤æ‚çš„ç¯å¢ƒäº¤äº’ï¼Œåªéœ€è¦åŸºæœ¬çš„ç”ŸæˆåŠŸèƒ½
                    import ray
                    from nemo_rl.environments.interfaces import EnvironmentInterface, EnvironmentReturn
                    from typing import Any, Dict
                    import torch
                    from nemo_rl.models.generation.interfaces import GenerationDatumSpec
                    

                    
                    # åˆ›å»ºRay remoteç¯å¢ƒå®ä¾‹ï¼Œä¸GRPOä¿æŒä¸€è‡´
                    from nemo_rl.environments.math_environment import MathEnvironment
                    from nemo_rl.distributed.ray_actor_environment_registry import get_actor_python_env
                    
                    # ä»master_configè·å–ç¯å¢ƒé…ç½®
                    env_configs = master_config.get("env", {})
                    if "math" not in env_configs:
                        # å¦‚æœæ²¡æœ‰ç¯å¢ƒé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                        env_configs["math"] = {"num_workers": 8}
                        print(f"  âš ï¸ No math environment config found, using default: {env_configs['math']}")
                    
                    distillation_env = MathEnvironment.options(
                        runtime_env={
                            "py_executable": get_actor_python_env(
                                "nemo_rl.environments.math_environment.MathEnvironment"
                            ),
                            "env_vars": dict(os.environ),
                        }
                    ).remote(env_configs["math"])
                    distillation_task_env = {"math": distillation_env}
                    
                    #print(f"  ğŸ” Created Ray actor virtual distillation environment")
                    
                    # å…³é”®ä¿®å¤ï¼šé‡å¤batchä»¥è¾¾åˆ°æ­£ç¡®çš„å…¨å±€batch sizeï¼ˆä¸GRPOå®Œå…¨ä¸€è‡´ï¼‰
                    num_generations_per_prompt = master_config["distillation"]["num_generations_per_prompt"]
                    # print(f"  ğŸ” Repeating batch {num_generations_per_prompt} times to reach global batch size")
                    pass
                    
                    repeated_batch: BatchedDataDict[DatumSpec] = batch.repeat_interleave(
                        num_repeats=num_generations_per_prompt
                    )
                    # print(f"  ğŸ” Original batch size: {batch.size}, Repeated batch size: {repeated_batch.size}")
                    pass
                    
                    # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥repeated_batchä¸­æ‰€æœ‰å­—æ®µçš„å½¢çŠ¶
                    # print(f"  ğŸ” Checking repeated_batch field shapes after repeat_interleave...")
                    pass
                    for key, value in repeated_batch.items():
                        if torch.is_tensor(value):
                            # print(f"  ğŸ” {key}: {value.shape}")
                            pass
                        elif isinstance(value, list):
                            # print(f"  ğŸ” {key}: list with {len(value)} items")
                            pass
                            if len(value) > 0 and isinstance(value[0], torch.Tensor):
                                # print(f"  ğŸ”   - First item shape: {value[0].shape}")
                                pass
                        else:
                            # print(f"  ğŸ” {key}: {type(value)}")
                            pass
                    
                    # ç‰¹åˆ«æ£€æŸ¥loss_multiplierçš„å½¢çŠ¶
                    if "loss_multiplier" in repeated_batch:
                        loss_multiplier = repeated_batch["loss_multiplier"]
                        #print(f"  ğŸ” loss_multiplier type: {type(loss_multiplier)}")
                        if torch.is_tensor(loss_multiplier):
                            #print(f"  ğŸ” loss_multiplier shape: {loss_multiplier.shape}")
                            #print(f"  ğŸ” loss_multiplier dtype: {loss_multiplier.dtype}")
                            pass
                        elif isinstance(loss_multiplier, list):
                            #print(f"  ğŸ” loss_multiplier list length: {len(loss_multiplier)}")
                            if len(loss_multiplier) > 0:
                                # print(f"  ğŸ”   - First item type: {type(loss_multiplier[0])}")
                                pass
                                if isinstance(loss_multiplier[0], torch.Tensor):
                                    # print(f"  ğŸ”   - First item shape: {loss_multiplier[0].shape}")
                                    pass
                    
                    # éªŒè¯repeated_batchçš„sizeæ˜¯å¦æ­£ç¡®
                    expected_repeated_size = batch.size * num_generations_per_prompt
                    if repeated_batch.size != expected_repeated_size:
                        print(f"  âš ï¸ Warning: repeated_batch size mismatch!")
                        #print(f"  ğŸ” Expected: {expected_repeated_size}, Got: {repeated_batch.size}")
                        #print(f"  ğŸ” This might cause shape issues later")
                    
                    # å…³é”®ä¿®å¤ï¼šå‚è€ƒGRPOçš„å®ç°ï¼Œç›´æ¥ä½¿ç”¨max_total_sequence_lengthä½œä¸ºrolloutçš„max_seq_len
                    max_seq_len = master_config["policy"]["max_total_sequence_length"]
                    
                    print(f"  ğŸ” Using GRPO-style sequence length handling:")
                    print(f"    - max_seq_len (from policy.max_total_sequence_length): {max_seq_len}")
                    print(f"    - Note: Input and generation share this length limit (like GRPO)")
                    
                    # æ£€æŸ¥å¹¶æˆªæ–­è¿‡é•¿çš„åºåˆ—ï¼Œä½†ä½¿ç”¨æ›´å®½æ¾çš„é™åˆ¶
                    # å‚è€ƒGRPOï¼šè¾“å…¥å’Œç”Ÿæˆå…±äº«åºåˆ—é•¿åº¦ï¼Œä¸éœ€è¦ä¸¥æ ¼åˆ†ç¦»
                    max_input_len = max_seq_len  # å…è®¸è¾“å…¥å ç”¨æ•´ä¸ªåºåˆ—é•¿åº¦
                    
                    print(f"  ğŸ” Sequence length check: max_seq_len={max_seq_len}, max_input_len={max_input_len}")
                    
                    # æ£€æŸ¥å¹¶æˆªæ–­è¿‡é•¿çš„åºåˆ—
                    for i, message_log in enumerate(repeated_batch["message_log"]):
                        total_length = sum(len(msg["token_ids"]) for msg in message_log)
                        if total_length > max_input_len:
                            print(f"  âš ï¸ Sample {i} sequence length {total_length} exceeds max_input_len {max_input_len}, truncating...")
                            # æˆªæ–­åˆ°æœ€å¤§å…è®¸é•¿åº¦ï¼Œä½†ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€äº›å†…å®¹
                            remaining_length = max_input_len
                            for msg in message_log:
                                if remaining_length <= 0:
                                    # ä¸è¦å®Œå…¨æ¸…ç©ºï¼Œä¿ç•™è‡³å°‘ä¸€ä¸ªtoken
                                    if len(msg["token_ids"]) > 0:
                                        msg["token_ids"] = msg["token_ids"][:1]
                                else:
                                    msg_length = len(msg["token_ids"])
                                    if msg_length > remaining_length:
                                        msg["token_ids"] = msg["token_ids"][:remaining_length]
                                        remaining_length = 0
                                    else:
                                        remaining_length -= msg_length
                    
                    # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿æ‰€æœ‰åºåˆ—éƒ½æœ‰å†…å®¹
                    print(f"  ğŸ” Final validation before rollout:")
                    for i, message_log in enumerate(repeated_batch["message_log"][:3]):  # åªæ£€æŸ¥å‰3ä¸ªæ ·æœ¬
                        total_length = sum(len(msg["token_ids"]) for msg in message_log)
                        print(f"    Sample {i}: {total_length} tokens")
                        if total_length == 0:
                            print(f"    âŒ Sample {i} is empty!")
                    
                    # ä½¿ç”¨rolloutç”Ÿæˆå“åº”ï¼Œä¸GRPOå®Œå…¨ä¸€è‡´
                    try:
                        generated_batch, rollout_metrics = run_multi_turn_rollout(
                            policy_generation=student_generation,
                            input_batch=repeated_batch,  # ä½¿ç”¨é‡å¤åçš„batch
                            tokenizer=tokenizer,
                            task_to_env=distillation_task_env,  # ä¼ é€’Ray actorè™šæ‹Ÿç¯å¢ƒ
                            max_seq_len=min(max_length, max_seq_len),  # ä½¿ç”¨è®¡ç®—å¥½çš„max_seq_len
                            max_rollout_turns=1,  # è’¸é¦åªéœ€è¦å•è½®ç”Ÿæˆ
                            greedy=(decoding_method == "greedy"),  # æ ¹æ®decoding_methodå†³å®šæ˜¯å¦greedy
                        )
                        # ä»rolloutç»“æœä¸­æå–ç”Ÿæˆçš„åºåˆ—
                        generated_sequences = generated_batch["message_log"]
                        print(f"  âœ… Successfully generated responses via rollout")
  
                        if "loss_multiplier" in repeated_batch:
                            loss_multiplier_after = repeated_batch["loss_multiplier"]
                        
                    except Exception as e:
                        print(f"  âŒ Rollout generation failed: {e}")
                        print(f"  ğŸ” Attempting fallback generation method...")
                        
                        try:
                            # Fallback: ç›´æ¥ä½¿ç”¨ç”Ÿæˆæ¥å£ï¼Œè·³è¿‡rollout
                            print(f"  ğŸ” Using direct generation fallback...")
                            
                            # å‡†å¤‡è¾“å…¥æ•°æ®
                            input_ids = []
                            for message_log in repeated_batch["message_log"]:
                                # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯çš„token_ids
                                sample_tokens = []
                                for msg in message_log:
                                    if "token_ids" in msg and len(msg["token_ids"]) > 0:
                                        sample_tokens.extend(msg["token_ids"].tolist())
                                
                                if len(sample_tokens) == 0:
                                    # å¦‚æœåºåˆ—ä¸ºç©ºï¼Œæ·»åŠ pad token
                                    sample_tokens = [tokenizer.pad_token_id]
                                    print(f"  âš ï¸ Empty sequence detected, added pad token")
                                
                                input_ids.append(sample_tokens)
                            
                            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
                            max_len = max(len(ids) for ids in input_ids)
                            padded_input_ids = []
                            for ids in input_ids:
                                if len(ids) < max_len:
                                    ids.extend([tokenizer.pad_token_id] * (max_len - len(ids)))
                                padded_input_ids.append(ids)
                            
                            # è½¬æ¢ä¸ºtensor
                            input_ids_tensor = torch.tensor(padded_input_ids, dtype=torch.long)
                            input_lengths_tensor = torch.tensor([len(ids) for ids in input_ids], dtype=torch.long)
                            
                            print(f"  ğŸ” Fallback input shape: {input_ids_tensor.shape}")
                            
                            # ç›´æ¥ç”Ÿæˆ
                            generation_data = BatchedDataDict[GenerationDatumSpec]({
                                "input_ids": input_ids_tensor,
                                "input_lengths": input_lengths_tensor,
                                "stop_strings": [None] * len(input_ids),
                            })
                            
                            generation_outputs = student_generation.generate(
                                generation_data, 
                                greedy=(decoding_method == "greedy")
                            )
                            
                            # å¤„ç†ç”Ÿæˆç»“æœ
                            output_ids = generation_outputs["output_ids"]
                            generated_sequences = []
                            
                            for i in range(len(input_ids)):
                                input_len = input_lengths_tensor[i].item()
                                generated_tokens = output_ids[i, input_len:].tolist()
                                
                                # åˆ›å»ºassistantæ¶ˆæ¯
                                assistant_message = {
                                    "role": "assistant",
                                    "content": tokenizer.decode(generated_tokens, skip_special_tokens=True),
                                    "token_ids": torch.tensor(generated_tokens, dtype=torch.long),
                                }
                                
                                # é‡å»ºmessage_log
                                sample_messages = []
                                for msg in repeated_batch["message_log"][i]:
                                    sample_messages.append(msg)
                                sample_messages.append(assistant_message)
                                generated_sequences.append(sample_messages)
                            
                            print(f"  âœ… Fallback generation successful")
                            
                        except Exception as fallback_error:
                            print(f"  âŒ Fallback generation also failed: {fallback_error}")
                            import traceback
                            traceback.print_exc()
                            raise RuntimeError(f"Both rollout and fallback generation failed. Original error: {e}, Fallback error: {fallback_error}")
                else:
                    # print(f"  ğŸ” Using megatron backend, no generation interface...")
                    pass
                    # å¦‚æœä½¿ç”¨megatronåç«¯ï¼Œç›´æ¥ä½¿ç”¨policy
                    # è¿™é‡Œéœ€è¦å®ç°megatronçš„ç”Ÿæˆé€»è¾‘
                    generated_sequences = batch["message_log"]  # æš‚æ—¶ä½¿ç”¨åŸå§‹æ•°æ®
                    print(f"  âš ï¸ Megatron generation not fully implemented, using original data")
                
                print(f"  âœ“ Generated responses for batch of size {batch_size}")
                
                # æ ‡è®°ç”Ÿæˆå®Œæˆ
                if student_generation is not None:
                    #print(f"  ğŸ” Finishing generation...")
                    student_generation.finish_generation()
                    print(f"  âœ… Generation finished")
                
                # 3. è®¡ç®—logitsï¼ˆä½¿ç”¨ä¸GRPOç›¸åŒçš„æ•°æ®å¤„ç†æ–¹å¼ï¼‰
                print("â–¶ Computing logits...")
                #print(f"  ğŸ” Generated sequences type: {type(generated_sequences)}")
                #print(f"  ğŸ” Generated sequences length: {len(generated_sequences)}")
                
                with timer.time("logits_computation"):
                    # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸GRPOå®Œå…¨ä¸€è‡´çš„æ•°æ®å¤„ç†æ–¹å¼
                    # å°†ç”Ÿæˆçš„message_logè½¬æ¢ä¸ºFlatMessagesTypeç”¨äºè®­ç»ƒ
                    # print(f"  ğŸ” Converting generated sequences to flat format...")
                    pass
                    try:
                        # å…³é”®ä¿®å¤ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„batch size
                        expected_batch_size = master_config["distillation"]["num_prompts_per_step"] * master_config["distillation"]["num_generations_per_prompt"]
                        #print(f"  ğŸ” Expected batch size: {expected_batch_size}")
                        #print(f"  ğŸ” Generated sequences length: {len(generated_sequences)}")
                        
                        if len(generated_sequences) != expected_batch_size:
                            print(f"  âš ï¸ Warning: Generated sequences length {len(generated_sequences)} != expected {expected_batch_size}")
                            # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œæˆªæ–­æˆ–æ‰©å±•åˆ°æ­£ç¡®é•¿åº¦
                            if len(generated_sequences) > expected_batch_size:
                                generated_sequences = generated_sequences[:expected_batch_size]
                                # print(f"  ğŸ” Truncated to {len(generated_sequences)} sequences")
                                pass
                            else:
                                # æ‰©å±•batchåˆ°æ­£ç¡®å¤§å°ï¼ˆé‡å¤æœ€åä¸€ä¸ªåºåˆ—ï¼‰
                                while len(generated_sequences) < expected_batch_size:
                                    generated_sequences.append(generated_sequences[-1])
                                # print(f"  ğŸ” Extended to {len(generated_sequences)} sequences")
                                pass
                        
                        flat_messages, input_lengths = batched_message_log_to_flat_message(
                            generated_sequences,
                            pad_value_dict={"token_ids": tokenizer.pad_token_id},
                            make_sequence_length_divisible_by=master_config["policy"].get(
                                "make_sequence_length_divisible_by", 1
                            ),
                        )
                        print(f"  âœ… Successfully converted generated sequences to flat format")

                    except Exception as e:
                        print(f"  âŒ Failed to convert generated sequences to flat format: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                    # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä¸GRPOå®Œå…¨ä¸€è‡´
                    # print(f"  ğŸ” Preparing training data...")
                    pass
                    
                    # å…³é”®ä¿®å¤ï¼šç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦çš„å­—æ®µï¼Œä¸GRPOçš„train_dataç»“æ„å®Œå…¨ä¸€è‡´
                    # æ·»åŠ ç¼ºå¤±çš„å­—æ®µï¼Œé¿å…get_logprobsæ–¹æ³•å‡ºé”™
                    if "generation_logprobs" not in flat_messages:
                        # print(f"  ğŸ” Adding missing generation_logprobs field...")
                        pass
                        # ä¸ºæ¯ä¸ªtokenåˆ›å»ºé›¶logprobsï¼ˆå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ç”Ÿæˆlogprobsï¼‰
                        flat_messages["generation_logprobs"] = torch.zeros_like(
                            flat_messages["token_ids"], dtype=torch.float32
                        )
                    
                    if "advantages" not in flat_messages:
                        # print(f"  ğŸ” Adding missing advantages field...")
                        pass
                        # ä¸ºè’¸é¦ä»»åŠ¡åˆ›å»ºé»˜è®¤advantagesï¼ˆå…¨1ï¼Œè¡¨ç¤ºæ‰€æœ‰tokenéƒ½é‡è¦ï¼‰
                        flat_messages["advantages"] = torch.ones_like(
                            flat_messages["token_ids"], dtype=torch.float32
                        )
                    
                    if "token_loss_mask" not in flat_messages:
                        # print(f"  ğŸ” Adding missing token_loss_mask field...")
                        pass
                        # å…³é”®ä¿®å¤ï¼šä¸ºè’¸é¦ä»»åŠ¡åˆ›å»ºæ­£ç¡®çš„token loss mask
                        # åªå¯¹response tokensï¼ˆépromptï¼‰è®¡ç®—æŸå¤±
                        token_loss_mask = torch.zeros_like(
                            flat_messages["token_ids"], dtype=torch.bool
                        )
                        
                        # æ ¹æ®input_lengthsç¡®å®šå“ªäº›æ˜¯response tokens
                        for i, seq_len in enumerate(input_lengths):
                            # å‡è®¾å‰Nä¸ªtokensæ˜¯promptï¼Œåé¢çš„tokensæ˜¯response
                            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ•°æ®ç»“æ„æ¥è°ƒæ•´
                            if seq_len > 0:
                                # æ ‡è®°response tokensä¸ºTrueï¼ˆå‚ä¸æŸå¤±è®¡ç®—ï¼‰
                                token_loss_mask[i, :seq_len] = True
                        
                        flat_messages["token_loss_mask"] = token_loss_mask
                        print(f"  ğŸ” Created distillation token_loss_mask: {token_loss_mask.sum().item()} response tokens out of {token_loss_mask.numel()} total tokens")
                    

                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                    expected_batch_size = flat_messages['token_ids'].shape[0]
                    expected_seq_len = flat_messages['token_ids'].shape[1]
                    
                    # print(f"  ğŸ” Expected batch size: {expected_batch_size}")
                    pass
                    # print(f"  ğŸ” Expected sequence length: {expected_seq_len}")
                    pass
                    
                    # éªŒè¯å¹¶ä¿®å¤å½¢çŠ¶ä¸åŒ¹é…çš„å­—æ®µ
                    if flat_messages['advantages'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: advantages batch dimension mismatch, fixing...")
                        flat_messages['advantages'] = flat_messages['advantages'][:expected_batch_size]
                    
                    if flat_messages['generation_logprobs'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: generation_logprobs batch dimension mismatch, fixing...")
                        flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'][:expected_batch_size]
                    
                    if flat_messages['token_loss_mask'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: token_loss_mask batch dimension mismatch, fixing...")
                        flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'][:expected_batch_size]
                    
                    if repeated_batch['loss_multiplier'].shape[0] != expected_batch_size:
                        print(f"  âš ï¸ Warning: loss_multiplier batch dimension mismatch, fixing...")
                        repeated_batch['loss_multiplier'] = repeated_batch['loss_multiplier'][:expected_batch_size]
                    
                    # éªŒè¯sequenceç»´åº¦
                    if flat_messages['advantages'].shape[1] != expected_seq_len:
                        print(f"  âš ï¸ Warning: advantages sequence dimension mismatch, fixing...")
                        if flat_messages['advantages'].shape[1] > expected_seq_len:
                            flat_messages['advantages'] = flat_messages['advantages'][:, :expected_seq_len]
                        else:
                            flat_messages['advantages'] = flat_messages['advantages'].expand(-1, expected_seq_len)
                    
                    if flat_messages['generation_logprobs'].shape[1] != expected_seq_len:
                        print(f"  âš ï¸ Warning: generation_logprobs sequence dimension mismatch, fixing...")
                        if flat_messages['generation_logprobs'].shape[1] > expected_seq_len:
                            flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'][:, :expected_seq_len]
                        else:
                            flat_messages['generation_logprobs'] = flat_messages['generation_logprobs'].expand(-1, expected_seq_len)
                    
                    if flat_messages['token_loss_mask'].shape[1] != expected_seq_len:
                        print(f"  âš ï¸ Warning: token_loss_mask sequence dimension mismatch, fixing...")
                        if flat_messages['token_loss_mask'].shape[1] > expected_seq_len:
                            flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'][:, :expected_seq_len]
                        else:
                            flat_messages['token_loss_mask'] = flat_messages['token_loss_mask'].expand(-1, expected_seq_len)
                    
                    #print(f"  ğŸ” After shape validation and fixing:")
                    #print(f"  ğŸ” flat_messages['advantages'] shape: {flat_messages['advantages'].shape}")
                    #print(f"  ğŸ” flat_messages['generation_logprobs'] shape: {flat_messages['generation_logprobs'].shape}")
                    #print(f"  ğŸ” flat_messages['token_loss_mask'] shape: {flat_messages['token_loss_mask'].shape}")
                    #print(f"  ğŸ” repeated_batch['loss_multiplier'] shape: {repeated_batch['loss_multiplier'].shape}")
                    
                    # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ç¡®ä¿æ‰€æœ‰å­—æ®µçš„å½¢çŠ¶éƒ½æ­£ç¡®
                    # print(f"  ğŸ” Final shape validation and forced fixing...")
                    pass
                    
                    # ç¡®ä¿loss_multiplieræ˜¯æ­£ç¡®çš„å½¢çŠ¶
                    if isinstance(repeated_batch["loss_multiplier"], torch.Tensor):
                        if len(repeated_batch["loss_multiplier"].shape) > 1:
                            print(f"  âš ï¸ Warning: loss_multiplier has wrong shape {repeated_batch['loss_multiplier'].shape}, fixing...")
                            # å¦‚æœloss_multiplieræ˜¯å¤šç»´çš„ï¼Œå–ç¬¬ä¸€ä¸ªç»´åº¦
                            repeated_batch["loss_multiplier"] = repeated_batch["loss_multiplier"].flatten()[:expected_batch_size]
                            # print(f"  ğŸ” Fixed loss_multiplier shape: {repeated_batch['loss_multiplier'].shape}")
                            pass
                        elif repeated_batch["loss_multiplier"].shape[0] != expected_batch_size:
                            print(f"  âš ï¸ Warning: loss_multiplier batch dimension mismatch, fixing...")
                            repeated_batch["loss_multiplier"] = repeated_batch["loss_multiplier"][:expected_batch_size]
                            # print(f"  ğŸ” Fixed loss_multiplier shape: {repeated_batch['loss_multiplier'].shape}")
                            pass
                    elif isinstance(repeated_batch["loss_multiplier"], list):
                        print(f"  âš ï¸ Warning: loss_multiplier is a list, converting to tensor...")
                        repeated_batch["loss_multiplier"] = torch.tensor(repeated_batch["loss_multiplier"][:expected_batch_size], dtype=torch.float32)
                        # print(f"  ğŸ” Converted loss_multiplier shape: {repeated_batch['loss_multiplier'].shape}")
                        pass

                    
                    # æœ€ç»ˆéªŒè¯loss_multiplierçš„ç±»å‹å’Œå½¢çŠ¶
                    if not isinstance(repeated_batch["loss_multiplier"], torch.Tensor):
                        print(f"  âŒ Critical error: loss_multiplier is not a tensor!")
                        print(f"  ğŸ” Type: {type(repeated_batch['loss_multiplier'])}")
                        print(f"  ğŸ” Value: {repeated_batch['loss_multiplier']}")
                        
                        # å°è¯•ä¿®å¤
                        if isinstance(repeated_batch["loss_multiplier"], (list, tuple)):
                            repeated_batch["loss_multiplier"] = torch.tensor(repeated_batch["loss_multiplier"], dtype=torch.float32)
                            print(f"  âœ… Fixed: Converted list to tensor")
                        elif isinstance(repeated_batch["loss_multiplier"], (int, float)):
                            repeated_batch["loss_multiplier"] = torch.tensor([repeated_batch["loss_multiplier"]] * expected_batch_size, dtype=torch.float32)
                            print(f"  âœ… Fixed: Converted scalar to tensor")
                        else:
                            # åˆ›å»ºé»˜è®¤çš„loss_multiplier
                            repeated_batch["loss_multiplier"] = torch.ones(expected_batch_size, dtype=torch.float32)
                            print(f"  âœ… Fixed: Created default loss_multiplier")
                    
                    # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                    all_batch_sizes = [
                        flat_messages['token_ids'].shape[0],
                        input_lengths.shape[0],
                        flat_messages['advantages'].shape[0],
                        flat_messages['generation_logprobs'].shape[0],
                        flat_messages['token_loss_mask'].shape[0],
                        repeated_batch['loss_multiplier'].shape[0]
                    ]
                    
                    if len(set(all_batch_sizes)) != 1:
                        print(f"  âŒ Critical error: Batch dimensions are not consistent!")
                        print(f"  ğŸ” Batch sizes: {all_batch_sizes}")
                        raise ValueError(f"Batch dimensions must be consistent, got: {all_batch_sizes}")
                    
                    print(f"  âœ… All batch dimensions are consistent: {all_batch_sizes[0]}")
                    
                    train_data = BatchedDataDict[DistillationLossDataDict]({
                        "input_ids": flat_messages["token_ids"],
                        "input_lengths": input_lengths,
                        "advantages": flat_messages["advantages"],
                        "generation_logprobs": flat_messages["generation_logprobs"],
                        "token_mask": flat_messages["token_loss_mask"],  # ä½¿ç”¨token_loss_maskè€Œä¸æ˜¯è‡ªå®šä¹‰çš„token_mask
                        "sample_mask": repeated_batch["loss_multiplier"],
                    })
                    print(f"  âœ… Training data prepared")

                    
                    # éªŒè¯batch sizeæ˜¯å¦æ­£ç¡®
                    if train_data.size != expected_batch_size:
                        print(f"  âš ï¸ Warning: Expected batch size {expected_batch_size}, got {train_data.size}")
                    else:
                        print(f"  âœ… Batch size is correct: {train_data.size}")
                    
                    # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    train_data.to("cpu")  # ä¸GRPOä¿æŒä¸€è‡´
                    
                    # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆéœ€è¦å•ç‹¬å®ç°ï¼Œå› ä¸ºæ¨¡å‹å¤§å°ä¸åŒï¼‰
                    print("  âœ“ Computing teacher model logits...")
                    with torch.no_grad():
                        # å®ç°çœŸæ­£çš„æ•™å¸ˆæ¨¡å‹æ¨ç†
                        teacher_model_path = master_config["distillation"]["teacher_model_path"]
                        # print(f"  ğŸ” Loading teacher model: {teacher_model_path}")
                        pass
                        
                        try:
                            # æ–¹æ³•1: å°è¯•ä½¿ç”¨transformersç›´æ¥åŠ è½½æ•™å¸ˆæ¨¡å‹
                            from transformers import AutoModelForCausalLM, AutoTokenizer
                            
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ•™å¸ˆæ¨¡å‹å®ä¾‹
                            if not hasattr(student_policy, '_teacher_model'):
                                # print(f"  ğŸ” Loading teacher model from {teacher_model_path}...")
                                pass
                                
                                try:
                                    # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨device_map="auto"å’Œä½ç²¾åº¦
                                    teacher_model = AutoModelForCausalLM.from_pretrained(
                                        teacher_model_path,
                                        torch_dtype=torch.bfloat16,
                                        device_map="auto",
                                        trust_remote_code=True,
                                        low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
                                    )
                                    
      
                                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                                    if hasattr(teacher_model, 'device'):
                                        # print(f"  ğŸ”   - Device: {teacher_model.device}")
                                        pass
                                    else:
                                        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
                                        try:
                                            device = next(teacher_model.parameters()).device
                                            # print(f"  ğŸ”   - Device (from params): {device}")
                                            pass
                                        except Exception as e:
                                            # print(f"  ğŸ”   - Device: Could not determine ({e})")
                                            pass
                                    
                                    teacher_model.eval()
                                    
                                    # æµ‹è¯•å‰å‘ä¼ æ’­ï¼Œç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
                                    # print(f"  ğŸ” Testing teacher model forward pass...")
                                    pass
                                    try:
                                        test_input = torch.randint(0, teacher_model.config.vocab_size, (1, 10), device=next(teacher_model.parameters()).device)
                                        with torch.no_grad():
                                            test_output = teacher_model(test_input)
                                            test_logits = test_output.logits
                               
                                            if test_logits.shape != (1, 10, teacher_model.config.vocab_size):
                                                print(f"  âš ï¸ Warning: Test logits shape is incorrect!")
                                                # print(f"  ğŸ” This might indicate a problem with the model configuration")
                                                pass
                                    except Exception as e:
                                        print(f"  âš ï¸ Warning: Test forward pass failed: {e}")
                                        # print(f"  ğŸ” This might indicate a problem with the model")
                                        pass
                                    
                                    # ç¼“å­˜æ•™å¸ˆæ¨¡å‹
                                    student_policy._teacher_model = teacher_model
                                    print(f"  âœ… Teacher model loaded successfully")
                                    
                                except Exception as e:
                                    print(f"  âŒ Failed to load teacher model: {e}")
                                    import traceback
                                    traceback.print_exc()
                                    raise
                            else:
                                teacher_model = student_policy._teacher_model

                            teacher_input_ids = train_data["input_ids"]
                            

                            try:
                                test_input = torch.randint(0, 1000, (2, 5), device=next(teacher_model.parameters()).device)
                                # print(f"  ğŸ” Test input shape: {test_input.shape}")
                                pass
                                
                                with torch.no_grad():
                                    test_output = teacher_model(test_input)
                                    test_logits = test_output.logits

                                    if len(test_logits.shape) != 3:
                                        print(f"  âŒ Critical error: Test logits has wrong number of dimensions!")
                                        # print(f"  ğŸ” This indicates a fundamental problem with the teacher model")
                                        pass
                                        raise ValueError(f"Teacher model produces incorrect logits shape: {test_logits.shape}")
                                    
                                    print(f"  âœ… Test forward pass successful, proceeding with actual computation...")
                            except Exception as e:
                                print(f"  âŒ Test forward pass failed: {e}")
                                raise
                            
                            # å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤šæ•°æ®
                            batch_size = teacher_input_ids.shape[0]
                            chunk_size = 4  # æ¯æ¬¡å¤„ç†4ä¸ªæ ·æœ¬
                            teacher_logits_list = []
                            
                            for i in range(0, batch_size, chunk_size):
                                end_idx = min(i + chunk_size, batch_size)
                                chunk_input_ids = teacher_input_ids[i:end_idx]
                                
                                # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                                if hasattr(teacher_model, 'device'):
                                    chunk_input_ids = chunk_input_ids.to(teacher_model.device)
                                else:
                                    # å¦‚æœæ²¡æœ‰deviceå±æ€§ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡
                                    try:
                                        device = next(teacher_model.parameters()).device
                                        chunk_input_ids = chunk_input_ids.to(device)
                                        # print(f"  ğŸ” Chunk {i//chunk_size + 1}: Using device {device}")
                                        pass
                                    except Exception as e:
                                        print(f"  âš ï¸ Warning: Could not determine teacher model device: {e}")
                                        # é»˜è®¤ä½¿ç”¨CPU
                                        chunk_input_ids = chunk_input_ids.cpu()
                                        # print(f"  ğŸ” Chunk {i//chunk_size + 1}: Using CPU as fallback")
                                        pass
                                
                                with torch.no_grad():
                                    # åˆ›å»ºattention_maskå’Œposition_idsï¼Œç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
                                    chunk_batch_size, chunk_seq_len = chunk_input_ids.shape
                                    
                                    # åˆ›å»ºattention_maskï¼ˆå³å¡«å……åºåˆ—ï¼‰
                                    attention_mask = torch.zeros((chunk_batch_size, chunk_seq_len), dtype=torch.long, device=chunk_input_ids.device)
                                    for j, length in enumerate(train_data["input_lengths"][i:i+chunk_size]):
                                        attention_mask[j, :length] = 1
                                    
                                    # åˆ›å»ºposition_ids
                                    position_ids = torch.arange(chunk_seq_len, device=chunk_input_ids.device).repeat(chunk_batch_size, 1)
                                    
                                    # ä½¿ç”¨å®Œæ•´çš„è¾“å…¥è¿›è¡Œå‰å‘ä¼ æ’­
                                    chunk_outputs = teacher_model(
                                        chunk_input_ids,
                                        attention_mask=attention_mask,
                                        position_ids=position_ids,
                                        return_dict=True
                                    )
                                    chunk_logits = chunk_outputs.logits
                                    
                                    teacher_logits_list.append(chunk_logits.cpu())  # ç§»åˆ°CPUèŠ‚çœGPUå†…å­˜
                                
                                # æ¸…ç†GPUå†…å­˜
                                del chunk_outputs, chunk_logits
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                            
                            # åˆå¹¶æ‰€æœ‰chunkçš„logits
                            teacher_logits = torch.cat(teacher_logits_list, dim=0)
                            del teacher_logits_list  # æ¸…ç†åˆ—è¡¨
                            
                            print(f"  âœ… Teacher logits computed successfully")
   
                            
                            # å…³é”®ä¿®å¤ï¼šéªŒè¯teacher_logitsçš„å½¢çŠ¶
                            expected_teacher_shape = (batch_size, teacher_input_ids.shape[1], -1)  # æœ€åä¸€ä¸ªç»´åº¦æ˜¯vocab_size
                          
                            
                            # æ£€æŸ¥å¹¶ä¿®å¤teacher_logitsçš„å½¢çŠ¶
                            if len(teacher_logits.shape) != 3:
                                print(f"  âš ï¸ Warning: Teacher logits has wrong number of dimensions!")
                                
                                
                                # å¦‚æœteacher_logitsæ˜¯2Dçš„ï¼Œå°è¯•é‡å¡‘ä¸º3D
                                if len(teacher_logits.shape) == 2:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯[batch_size, vocab_size]çš„æƒ…å†µ
                                    if teacher_logits.shape[0] == batch_size and teacher_logits.shape[1] > 1000:  # å‡è®¾vocab_size > 1000
                                        # å‡è®¾æ¯ä¸ªåºåˆ—éƒ½æ˜¯ç›¸åŒé•¿åº¦ï¼Œä»input_idsè·å–
                                        seq_len = teacher_input_ids.shape[1]
                                        vocab_size = teacher_logits.shape[1]
                                        
                                        # é‡å¡‘ä¸º[batch_size, seq_len, vocab_size]
                                        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼Œå¯èƒ½éœ€è¦é‡å¤logitsæˆ–ä½¿ç”¨å…¶ä»–ç­–ç•¥
                                        teacher_logits = teacher_logits.unsqueeze(1).expand(-1, seq_len, -1)
                                        # print(f"  ğŸ” Reshaped teacher_logits shape: {teacher_logits.shape}")
                                        pass
                                    else:
                                        print(f"  âŒ Cannot determine how to reshape teacher_logits!")
                                        raise ValueError(f"Teacher logits shape {teacher_logits.shape} is not compatible with expected shape {expected_teacher_shape}")
                                elif len(teacher_logits.shape) > 3:
                                    print(f"  âš ï¸ Warning: Teacher logits has too many dimensions: {teacher_logits.shape}")
                                    # å°è¯•å‹ç¼©å¤šä½™çš„ç»´åº¦
                                    if teacher_logits.shape[0] == batch_size:
                                        # ä¿æŒbatchç»´åº¦ï¼Œå‹ç¼©å…¶ä»–ç»´åº¦
                                        teacher_logits = teacher_logits.view(batch_size, -1, teacher_logits.shape[-1])
                                        # print(f"  ğŸ” Compressed teacher_logits shape: {teacher_logits.shape}")
                                        pass
                                    else:
                                        print(f"  âŒ Cannot determine how to handle teacher_logits with shape {teacher_logits.shape}")
                                        raise ValueError(f"Teacher logits shape {teacher_logits.shape} is not compatible with expected shape {expected_teacher_shape}")
                            
                            # éªŒè¯ä¿®å¤åçš„å½¢çŠ¶
                            if teacher_logits.shape[0] != expected_teacher_shape[0] or teacher_logits.shape[1] != expected_teacher_shape[1]:
                                print(f"  âš ï¸ Warning: Teacher logits shape still mismatch after reshaping!")
                                # print(f"  ğŸ” Expected: {expected_teacher_shape}")
                                pass
                                # print(f"  ğŸ” Got: {teacher_logits.shape}")
                                pass
                                # å°è¯•è¿›ä¸€æ­¥ä¿®å¤å½¢çŠ¶
                                if teacher_logits.shape[0] != batch_size:
                                    # print(f"  ğŸ” Fixing teacher_logits batch dimension...")
                                    pass
                                    if teacher_logits.shape[0] > batch_size:
                                        teacher_logits = teacher_logits[:batch_size]
                                    else:
                                        # æ‰©å±•batchç»´åº¦
                                        teacher_logits = teacher_logits.expand(batch_size, -1, -1)
                                
                                if teacher_logits.shape[1] != teacher_input_ids.shape[1]:
                                    # print(f"  ğŸ” Fixing teacher_logits sequence dimension...")
                                    pass
                                    if teacher_logits.shape[1] > teacher_input_ids.shape[1]:
                                        teacher_logits = teacher_logits[:, :teacher_input_ids.shape[1], :]
                                    else:
                                        # æ‰©å±•sequenceç»´åº¦
                                        teacher_logits = teacher_logits.expand(-1, teacher_input_ids.shape[1], -1)
                            
                            # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿å½¢çŠ¶å®Œå…¨æ­£ç¡®
                            final_shape = teacher_logits.shape
                            if final_shape[0] != batch_size or final_shape[1] != teacher_input_ids.shape[1]:
                                print(f"  âŒ Critical error: Final teacher_logits shape {final_shape} is still incorrect!")
        
                                raise ValueError(f"Failed to fix teacher_logits shape. Final shape: {final_shape}")
                            
                            
                            # å°†æ•™å¸ˆlogitsæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
                            train_data["teacher_logits"] = teacher_logits
                            print(f"  âœ… Teacher logits added to training data")
                            
                        except Exception as e:
                            print(f"  âŒ Failed to load teacher model: {e}")
                            print(f"  âš ï¸ Falling back to student logits placeholder")
                            import traceback
                            traceback.print_exc()
                            # print(f"  ğŸ” This will result in KL loss = 0 (no distillation effect)")
                            pass
                            
                            # å›é€€åˆ°å ä½ç¬¦ï¼ˆä¸æ¨èï¼Œä½†ç¡®ä¿ç¨‹åºèƒ½è¿è¡Œï¼‰
                            print(f"  âš ï¸ WARNING: This will result in ineffective distillation training!")
                            # åˆ›å»ºå ä½ç¬¦teacher_logitsä»¥é¿å…é”™è¯¯
                            batch_size = train_data["input_ids"].shape[0]
                            seq_len = train_data["input_ids"].shape[1]
                            vocab_size = 32000  # å‡è®¾çš„è¯æ±‡è¡¨å¤§å°
                            placeholder_logits = torch.randn(batch_size, seq_len, vocab_size) * 0.1
                            train_data["teacher_logits"] = placeholder_logits
                            print(f"  âœ… Added placeholder teacher_logits with shape: {placeholder_logits.shape}")
                    
                    # å…³é”®ä¿®å¤ï¼šå‡†å¤‡å­¦ç”Ÿæ¨¡å‹è¿›è¡Œlogprobæ¨ç†
                    print("  âœ“ Preparing student model for logprob inference...")
                    try:
                        student_policy.prepare_for_lp_inference()
                        print(f"  âœ… Student policy prepared for logprob inference")
                    except Exception as e:
                        print(f"  âŒ Failed to prepare student policy for logprob inference: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                    # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆå¯è®­ç»ƒæ¨¡å‹ï¼‰
                    print("  âœ“ Computing student model logits...")
                    try:
                        
                        # æ£€æŸ¥å¹¶ä¿®å¤teacher_logitsçš„å½¢çŠ¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if "teacher_logits" in train_data:
                            teacher_logits = train_data["teacher_logits"]
                            
                            # å¦‚æœteacher_logitsçš„å½¢çŠ¶ä¸æ­£ç¡®ï¼Œå¼ºåˆ¶ä¿®å¤
                            if len(teacher_logits.shape) != 3:
                                print(f"  âš ï¸ Warning: teacher_logits has wrong shape {teacher_logits.shape}, fixing...")
                                if len(teacher_logits.shape) == 2:
                                    # å¦‚æœæ˜¯[batch_size, vocab_size]ï¼Œé‡å¡‘ä¸º[batch_size, seq_len, vocab_size]
                                    batch_size = teacher_logits.shape[0]
                                    vocab_size = teacher_logits.shape[1]
                                    seq_len = train_data["input_ids"].shape[1]
                                    teacher_logits = teacher_logits.unsqueeze(1).expand(-1, seq_len, -1)
          
                                else:
                                    print(f"  âŒ Critical error: teacher_logits has unexpected shape {teacher_logits.shape}")
                                    raise ValueError(f"teacher_logits has unexpected shape: {teacher_logits.shape}")
                            
                            # éªŒè¯ä¿®å¤åçš„å½¢çŠ¶
                            expected_shape = (train_data["input_ids"].shape[0], train_data["input_ids"].shape[1], -1)
                            if teacher_logits.shape[0] != expected_shape[0] or teacher_logits.shape[1] != expected_shape[1]:
                                print(f"  âŒ Critical error: teacher_logits shape still incorrect after fixing!")
                                raise ValueError(f"Failed to fix teacher_logits shape")
                            
                            # æ›´æ–°train_dataä¸­çš„teacher_logits
                            train_data["teacher_logits"] = teacher_logits
 
                        
                        # å‡†å¤‡è¾“å…¥æ•°æ®
                        input_ids = train_data["input_ids"].to("cuda")
                        attention_mask = torch.ones_like(input_ids, dtype=torch.long)
                        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0).expand(input_ids.shape[0], -1)
                        
                        # ç›´æ¥è°ƒç”¨å­¦ç”Ÿæ¨¡å‹
                        with torch.no_grad():
                            student_policy.prepare_for_lp_inference()
                            
                            num_shards = len(student_policy.worker_group.workers)

                            
                            # ç¡®ä¿batch sizeæ˜¯shardsçš„å€æ•°
                            current_batch_size = input_ids.shape[0]
                            if current_batch_size % num_shards != 0:
                                # è°ƒæ•´batch sizeåˆ°æœ€è¿‘çš„shardså€æ•°
                                adjusted_batch_size = ((current_batch_size // num_shards) + 1) * num_shards

                                
                                # æ‰©å±•æ•°æ®åˆ°è°ƒæ•´åçš„batch size
                                if adjusted_batch_size > current_batch_size:
                                    # é‡å¤æœ€åä¸€ä¸ªæ ·æœ¬æ¥å¡«å……
                                    padding_size = adjusted_batch_size - current_batch_size
                                    input_ids = torch.cat([input_ids, input_ids[-1:].repeat(padding_size, 1)], dim=0)
                                    attention_mask = torch.cat([attention_mask, attention_mask[-1:].repeat(padding_size, 1)], dim=0)
                                    position_ids = torch.cat([position_ids, position_ids[-1:].repeat(padding_size, 1)], dim=0)
                                    # print(f"  ğŸ” Expanded input shapes to: {input_ids.shape}")
                                    pass
                            
                            # åˆ›å»ºæ­£ç¡®çš„è®­ç»ƒæ•°æ®æ ¼å¼
                            # print(f"  ğŸ” Creating training data for get_logprobs...")
                            pass
                            train_data_for_logprobs = BatchedDataDict[DistillationLossDataDict]({
                                "input_ids": input_ids,
                                "input_lengths": torch.tensor([input_ids.shape[1]] * input_ids.shape[0]),
                                "advantages": torch.ones(input_ids.shape[0], input_ids.shape[1]),
                                "generation_logprobs": torch.zeros(input_ids.shape[0], input_ids.shape[1]),
                                "token_mask": torch.ones(input_ids.shape[0], input_ids.shape[1]),
                                "sample_mask": torch.ones(input_ids.shape[0]),
                            })
                            
                            try:
                                # ä½¿ç”¨get_logprobsæ–¹æ³•è·å–logits
                                result = student_policy.get_logprobs(train_data_for_logprobs)
                                # print(f"  ğŸ” get_logprobs successful")
                                pass
                                
                                # æ£€æŸ¥è¿”å›ç»“æœçš„ç»“æ„
                                # print(f"  ğŸ” Result keys: {list(result.keys())}")
                                pass
                                for key, value in result.items():
                                    if torch.is_tensor(value):
                                        # print(f"  ğŸ” {key}: {value.shape}")
                                        pass
                                    else:
                                        # print(f"  ğŸ” {key}: {type(value)}")
                                        pass
                                
                                # å°è¯•è·å–logits
                                if "logits" in result:
                                    student_logits = result["logits"]
                                    # print(f"  ğŸ” Successfully got logits from result")
                                    pass
                                elif "logprobs" in result:
                                    # å¦‚æœåªæœ‰logprobsï¼Œæˆ‘ä»¬éœ€è¦ä»logprobsé‡å»ºlogits
                                    # print(f"  ğŸ” Only logprobs available, attempting to reconstruct logits...")
                                    pass
                                    logprobs = result["logprobs"]
                                    # print(f"  ğŸ” logprobs shape: {logprobs.shape}")
                                    pass
                                    
                                    # è¿™é‡Œæˆ‘ä»¬éœ€è¦å®ç°ä»logprobsåˆ°logitsçš„è½¬æ¢
                                    # ç”±äºè¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„è½¬æ¢ï¼Œæˆ‘ä»¬å…ˆä½¿ç”¨logprobsä½œä¸ºæ›¿ä»£
                                    # print(f"  ğŸ” Using logprobs as student_logits for now...")
                                    pass
                                    student_logits = logprobs.unsqueeze(-1).expand(-1, -1, 151936)  # å‡è®¾vocab_size=151936
                                    # print(f"  ğŸ” Reconstructed logits shape: {student_logits.shape}")
                                    pass
                                else:
                                    raise ValueError(f"Neither 'logits' nor 'logprobs' found in result: {list(result.keys())}")
                                
                            except Exception as e:
                                # print(f"  ğŸ” get_logprobs failed: {e}")
                                pass
                                # print(f"  ğŸ” Trying alternative approach...")
                                pass
                                
                                # å¦‚æœget_logprobså¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®æ¨¡å‹
                                try:
                                    # print(f"  ğŸ” Attempting to access model directly...")
                                    pass
                                    
                                    # è·å–ç¬¬ä¸€ä¸ªworker
                                    first_worker = student_policy.worker_group.workers[0]
                                    
                                    # æ£€æŸ¥workeræ˜¯å¦æœ‰modelå±æ€§
                                    # print(f"  ğŸ” Checking worker attributes...")
                                    pass
                                    worker_attrs = dir(first_worker)
                                    # print(f"  ğŸ” Worker attributes: {worker_attrs}")
                                    pass
                                    
                                    # å°è¯•è°ƒç”¨workerçš„get_logprobsæ–¹æ³•
                                    # print(f"  ğŸ” Calling worker.get_logprobs directly...")
                                    pass
                                    # ç›´æ¥è°ƒç”¨æ–¹æ³•ï¼Œä¸ä½¿ç”¨.remote()
                                    worker_result = first_worker.get_logprobs(train_data_for_logprobs)
                                    # print(f"  ğŸ” Worker get_logprobs successful")
                                    pass
                                    
                                    # å¤„ç†workerç»“æœ
                                    if "logits" in worker_result:
                                        student_logits = worker_result["logits"]
                                    elif "logprobs" in worker_result:
                                        logprobs = worker_result["logprobs"]
                                        student_logits = logprobs.unsqueeze(-1).expand(-1, -1, 151936)
                                    else:
                                        raise ValueError(f"Worker result missing logits/logprobs: {list(worker_result.keys())}")
                                        
                                except Exception as e2:
                                    # print(f"  ğŸ” Direct worker access also failed: {e2}")
                                    pass
                                    raise RuntimeError(f"All approaches to get student logits failed: {e2}")
                            
                            # print(f"  ğŸ” Raw student logits shape: {student_logits.shape}")
                            pass
                            
                            # å¦‚æœbatch sizeè¢«è°ƒæ•´äº†ï¼Œæ¢å¤åˆ°åŸå§‹å¤§å°
                            if student_logits.shape[0] > current_batch_size:
                                # print(f"  ğŸ” Restoring original batch size...")
                                pass
                                student_logits = student_logits[:current_batch_size]
                                # print(f"  ğŸ” Final student logits shape: {student_logits.shape}")
                                pass
                            
                        
                        print(f"  âœ… Student logits computed successfully")

                        
                        # å…³é”®ä¿®å¤ï¼šéªŒè¯student_logitsçš„å½¢çŠ¶
                        if student_logits.shape[0] != train_data["input_ids"].shape[0]:
                            print(f"  âš ï¸ Warning: Student logits batch dimension mismatch!")
                        
                        if student_logits.shape[1] != train_data["input_ids"].shape[1]:
                            print(f"  âš ï¸ Warning: Student logits sequence dimension mismatch!")
  
                        
                    except Exception as e:
                        print(f"  âŒ Failed to compute student logits: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                    
                    # å°†å­¦ç”Ÿlogitsæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­
                    train_data["student_logits"] = student_logits
                    print(f"  âœ… Student logits added to training data")
                    
                    # è®¡ç®—è’¸é¦æŸå¤±
                    print("  âœ“ Computing distillation loss...")
                    try:
                        # ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—è’¸é¦æŸå¤± - ä¿®å¤ï¼šä¼ é€’æ‰€æœ‰å¿…è¦çš„å‚æ•°
                        # å°†è’¸é¦å‚æ•°æ·»åŠ åˆ°train_dataä¸­ï¼Œä¾›æŸå¤±å‡½æ•°ä½¿ç”¨
                        train_data["kl_type"] = kl_type
                        train_data["lambda_"] = lambda_
                        train_data["mixed_kl_weight"] = mixed_kl_weight
                        
                        # å…³é”®ä¿®å¤ï¼šæ­£ç¡®ä¼ é€’token maskç»™æŸå¤±å‡½æ•°
                        # ç¡®ä¿åªåœ¨response tokensä¸Šè®¡ç®—KLæ•£åº¦
                        if "token_mask" in train_data:
                            token_mask = train_data["token_mask"]
                            total_tokens = token_mask.numel()
                            response_tokens = token_mask.sum().item()
                            prompt_tokens = total_tokens - response_tokens

                        else:
                            # å¦‚æœæ²¡æœ‰token_maskï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ï¼ˆå…¨1ï¼Œä½†è¿™ä¸æ˜¯ç†æƒ³æƒ…å†µï¼‰
                            token_mask = torch.ones_like(train_data["input_ids"], dtype=torch.bool)
                            print(f"  âš ï¸ Warning: No token_mask found, using all tokens for loss calculation")
                        
                    except Exception as e:
                        print(f"  âŒ Failed to compute distillation loss: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                
                # 5. è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼ˆå®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼ï¼‰
                print("â–¶ Training student model...")

                # éªŒè¯æ‰€æœ‰å­—æ®µçš„batchç»´åº¦ä¸€è‡´
                all_batch_sizes = [train_data[key].shape[0] for key in train_data.keys() if torch.is_tensor(train_data[key])]
                if len(set(all_batch_sizes)) != 1:
                    #print(f"  âŒ Critical error: Batch dimensions are not consistent!")
                    print(f"  ğŸ” Batch sizes: {all_batch_sizes}")
                    raise ValueError(f"Batch dimensions must be consistent, got: {all_batch_sizes}")
                
                print(f"  âœ… All batch dimensions are consistent: {all_batch_sizes[0]}")
                
                

                distillation_safe_data = {}
                
                for key, value in train_data.items():
                    if key in ["teacher_logits", "student_logits"]:
                        distillation_safe_data[key] = value
                        if len(value.shape) == 3:
                            batch_size, seq_len, vocab_size = value.shape
                            flattened_logits = value.view(batch_size * seq_len, vocab_size)
                            
                            # åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„keyï¼Œworkerä¸ä¼šæ£€æŸ¥
                            safe_key = f"distillation_{key}_flattened"
                            distillation_safe_data[safe_key] = flattened_logits
                            
                            # å­˜å‚¨åŸå§‹å½¢çŠ¶ä¿¡æ¯
                            distillation_safe_data[f"{safe_key}_shape"] = torch.tensor([batch_size, seq_len, vocab_size])
                            
                            # print(f"  ğŸ” Converted {key} to safe format: {flattened_logits.shape}")
                            pass
                        else:
                            print(f"  âš ï¸ Warning: {key} has unexpected shape: {value.shape}")
                            distillation_safe_data[key] = value
                    else:
                        # å¯¹äºå…¶ä»–å­—æ®µï¼Œç›´æ¥å¤åˆ¶
                        distillation_safe_data[key] = value
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰å­—æ®µçš„ç±»å‹
                print(f"  ğŸ” Distillation safe data fields:")
                for key, value in distillation_safe_data.items():
                    if torch.is_tensor(value):
                        print(f"    {key}: tensor {value.shape}")
                    elif isinstance(value, (list, tuple)):
                        print(f"    {key}: {type(value).__name__} with {len(value)} items")
                    elif isinstance(value, (int, float)):
                        print(f"    {key}: {type(value).__name__} = {value}")
                    else:
                        print(f"    {key}: {type(value).__name__}")
                
                batch_sizes = {}
                for key, value in distillation_safe_data.items():
                    if torch.is_tensor(value):
                        if len(value.shape) > 0:
                            batch_sizes[key] = value.shape[0]
                        else:
                            batch_sizes[key] = 1  # æ ‡é‡å¼ é‡
                    elif isinstance(value, (list, tuple)):
                        batch_sizes[key] = len(value)
                    elif isinstance(value, (int, float)):
                        batch_sizes[key] = 1  # æ ‡é‡å€¼
                    else:
                        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è°ƒç”¨lenï¼Œå¦‚æœå¤±è´¥åˆ™è®¾ä¸º1
                        try:
                            batch_sizes[key] = len(value)
                        except (TypeError, AttributeError):
                            batch_sizes[key] = 1
                            print(f"  âš ï¸ Warning: Field {key} has unsupported type {type(value)}, setting batch size to 1")
                
                
                
                
                with timer.time("training_prep"):

                    student_policy.prepare_for_training()  # ä¸GRPOå®Œå…¨ä¸€è‡´
                    STUDENT_GENERATION_STALE = True  # *** MARK AS STALE AFTER TRAINING ***
                    print(f"  âœ… Student policy prepared for training")
                
                with timer.time("policy_training"):
                    try:
                        train_results = student_policy.train(train_data, loss_fn)
                        print("  âœ… Training completed")
                    except Exception as e:
                        print(f"  âŒ Policy training failed: {e}")
                        import traceback
                        traceback.print_exc()
                        raise
                loss=train_results["loss"].numpy()
                #grad_norm=train_results["grad_norm"].numpy()
                print(f"  âœ… Distillation loss computed successfully")
                # è®°å½•æŸå¤±
                if logger is not None:
                    # è®°å½•ä¸»è¦è®­ç»ƒæŸå¤±
                    logger.log_metrics({"train/loss": loss.item()}, step)
                    
                    # è®°å½•ç”Ÿæˆé•¿åº¦ç›¸å…³æŒ‡æ ‡
                    if "input_ids" in train_data:
                        input_lengths = (train_data["input_ids"] != 0).sum(dim=1)
                        avg_input_length = input_lengths.float().mean().item()
                        max_input_length = input_lengths.max().item()
                        min_input_length = input_lengths.min().item()
                        
                        logger.log_metrics({
                            "train/avg_input_length": avg_input_length,
                            "train/max_input_length": max_input_length,
                            "train/min_input_length": min_input_length,
                            "train/input_length_std": input_lengths.float().std().item(),
                        }, step)
                    
                    # è®°å½•å½“å‰æœ€ä½³éªŒè¯lossï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if "val_loss" in distillation_save_state and distillation_save_state["val_loss"] is not None:
                        current_best_val_loss = distillation_save_state["val_loss"]
                        logger.log_metrics({"train/best_val_loss": current_best_val_loss}, step)
                        #print(f"  ğŸ” [Training] Current Best Val Loss = {current_best_val_loss:.6f}")
                    
                    # è®°å½•è’¸é¦å‚æ•°
                    logger.log_metrics({
                        "train/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                        "train/lambda": lambda_,
                        "train/mixed_kl_weight": mixed_kl_weight,
                    }, step)
                    
                    # æ‰“å°è®­ç»ƒlossä¿¡æ¯
                    print(f"  âœ…âœ…âœ… [Training] Step {step}: Loss = {loss.item():.6f}")
                    if "kl_loss" in loss_metrics:
                        print(f"  ğŸ” [Training] KL Loss = {loss_metrics['kl_loss']:.6f}")
                step += 1
                distillation_save_state["step"] = step
                # ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼Œä¸GRPOä¿æŒä¸€è‡´
                distillation_save_state["consumed_samples"] += distillation_config.get("num_prompts_per_step", 1)
                print(f"  âœ… Training state updated: step={step}, consumed_samples={distillation_save_state['consumed_samples']}")
                
                # 7. ä¿å­˜æ£€æŸ¥ç‚¹
                if step % distillation_config["save_steps"] == 0:
                    print(f"  âœ“ Saving checkpoint at step {step}")
                    # ä½¿ç”¨ä¸GRPOç›¸åŒçš„æ£€æŸ¥ç‚¹ä¿å­˜é€»è¾‘
                    try:
                        checkpoint_path = checkpointer.init_tmp_checkpoint(
                            step, distillation_save_state, master_config
                        )
                        student_policy.save_checkpoint(
                            weights_path=os.path.join(checkpoint_path, "policy", "weights"),
                            optimizer_path=os.path.join(checkpoint_path, "policy", "optimizer"),
                            tokenizer_path=os.path.join(checkpoint_path, "policy", "tokenizer"),
                        )
                        # ä¿å­˜æ•°æ®åŠ è½½å™¨çŠ¶æ€
                        torch.save(
                            train_dataloader.state_dict(),
                            os.path.join(checkpoint_path, "train_dataloader.pt"),
                        )
                        checkpointer.finalize_checkpoint(checkpoint_path)
                        print(f"  âœ… Checkpoint saved successfully")
                    except Exception as e:
                        print(f"  âŒ Failed to save checkpoint: {e}")
                        import traceback
                        traceback.print_exc()
                
                # 8. éªŒè¯ï¼ˆå®Œå…¨æŒ‰ç…§GRPOæ¨¡å¼ï¼‰
                if step % distillation_config["eval_steps"] == 0 and val_dataloader is not None:
                    print(f"  âœ“ Running validation at step {step}")
                    try:
                        if NEED_REFIT and STUDENT_GENERATION_STALE:
                            # ä¼ é€’ç”Ÿæˆé…ç½®å‚æ•°
                            generation_config = {
                                'temperature': temperature,
                                'decoding_method': decoding_method,
                                'max_length': max_length,
                            }
                            refit_student_generation(
                                student_policy, student_generation, colocated_inference, generation_config=generation_config, master_config=master_config
                            )
                            STUDENT_GENERATION_STALE = False
                        else:
                            if student_generation is not None:
                                # print(f"  ğŸ” Preparing generation for validation...")
                                pass
                                student_generation.prepare_for_generation()
                        
                        print(f"  ğŸ” Running validation...")
                        val_metrics = validate(
                            student_generation,
                            val_dataloader,
                            tokenizer,
                            step + 1,
                            master_config,
                        )
                        print(f"  âœ… Validation completed")
                        
                        # è®°å½•éªŒè¯æŒ‡æ ‡
                        if val_metrics:
                            # è®°å½•éªŒè¯loss - æ·»åŠ eval/lossè®°å½•
                            if "val_loss" in val_metrics:
                                # è®°å½•åˆ°validation/å‘½åç©ºé—´
                                logger.log_metrics({"validation/val_loss": val_metrics["val_loss"]}, step + 1)
                                # åŒæ—¶è®°å½•åˆ°eval/å‘½åç©ºé—´ï¼Œä¸GRPO/SFTä¿æŒä¸€è‡´
                                logger.log_metrics({"eval/loss": val_metrics["val_loss"]}, step + 1)
                                distillation_save_state["val_loss"] = val_metrics["val_loss"]
                                print(f"  âœ…âœ…âœ… [Validation] Step {step + 1}: Val Loss = {val_metrics['val_loss']:.6f}")
                                print(f"  ğŸ” [Eval] Step {step + 1}: Eval Loss = {val_metrics['val_loss']:.6f}")
                            
                            # è®°å½•å…¶ä»–éªŒè¯æŒ‡æ ‡
                            for k, v in val_metrics.items():
                                if k != "val_loss" and isinstance(v, (int, float)):
                                    logger.log_metrics({f"validation/{k}": v}, step + 1)
                                    # åŒæ—¶è®°å½•åˆ°eval/å‘½åç©ºé—´
                                    logger.log_metrics({f"eval/{k}": v}, step + 1)
                            
                            # è®°å½•éªŒè¯æ—¶çš„ç”Ÿæˆé•¿åº¦ä¿¡æ¯
                            if "val_avg_sequence_length" in val_metrics:
                                # è®°å½•åˆ°validation/å‘½åç©ºé—´
                                logger.log_metrics({
                                    "validation/avg_sequence_length": val_metrics["val_avg_sequence_length"],
                                    "validation/max_sequence_length": val_metrics.get("val_max_sequence_length", 0),
                                    "validation/min_sequence_length": val_metrics.get("val_min_sequence_length", 0),
                                }, step + 1)
                                
                                # åŒæ—¶è®°å½•åˆ°eval/å‘½åç©ºé—´
                                logger.log_metrics({
                                    "eval/avg_sequence_length": val_metrics["val_avg_sequence_length"],
                                    "eval/max_sequence_length": val_metrics.get("val_max_sequence_length", 0),
                                    "eval/min_sequence_length": val_metrics.get("val_min_sequence_length", 0),
                                }, step + 1)
                                
                                # æ‰“å°éªŒè¯é•¿åº¦ä¿¡æ¯
                                print(f"  ğŸ” [Validation] Avg Sequence Length = {val_metrics['val_avg_sequence_length']:.1f}")
                                print(f"  ğŸ” [Validation] Max Sequence Length = {val_metrics.get('val_max_sequence_length', 0)}")
                                print(f"  ğŸ” [Validation] Min Sequence Length = {val_metrics.get('val_min_sequence_length', 0)}")
                            
                            # è®°å½•éªŒè¯æ—¶çš„è’¸é¦å‚æ•°
                            logger.log_metrics({
                                "validation/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                                "validation/lambda": lambda_,
                                "validation/mixed_kl_weight": mixed_kl_weight,
                                "eval/kl_type": 1.0 if kl_type == "forward" else (2.0 if kl_type == "reverse" else 3.0),
                                "eval/lambda": lambda_,
                                "eval/mixed_kl_weight": mixed_kl_weight,
                            }, step + 1)
                        
                        if student_generation is not None:
                            student_generation.finish_generation()
                    except Exception as e:
                        print(f"  âŒ Validation failed: {e}")
                        import traceback
                        traceback.print_exc()
                
                # 9. æ—¥å¿—è®°å½•
                if step % distillation_config["logging_steps"] == 0:
                    print(f"  ğŸ” Logging metrics...")
                    try:
                        logger.log_metrics({
                            "step": step,
                            "loss": loss.item(), # Changed from loss.item() to kl_loss.item()
                            "consumed_samples": distillation_save_state["consumed_samples"],
                        })
                        print(f"  âœ… Metrics logged successfully")
                    except Exception as e:
                        print(f"  âŒ Failed to log metrics: {e}")
                        import traceback
                        traceback.print_exc()
                
                print(f"  âœ… Step {step + 1} completed successfully")
    
    except Exception as e:
        print(f"âŒ Distillation training failed with error: {e}")
        print(f"ğŸ” Error occurred at step {step + 1}, batch_idx {batch_idx if 'batch_idx' in locals() else 'unknown'}")
        import traceback
        traceback.print_exc()
